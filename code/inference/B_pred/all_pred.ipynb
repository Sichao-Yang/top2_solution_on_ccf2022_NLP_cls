{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4abc0f8c-d7ef-451d-af8f-7a65a2a1da13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.1+cu111\n",
      "1.10.1+cu111\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from functools import partial\n",
    "import scipy as sp\n",
    "import joblib\n",
    "import gc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import re\n",
    "import math\n",
    "import json\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from layers.losses import LabelSmoothingCrossEntropy, FocalLoss, LabelSM_Focal, DiceLoss, LabelSmoothingCrossEntropyWeight\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from layers.adversarial import FGM, PGD, Lookahead\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset)\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from NEZHA.modeling_nezha import NeZhaModel\n",
    "from NEZHA.configuration_nezha import NeZhaConfig\n",
    "# from transformers import NezhaModel\n",
    "# from transformers import NezhaConfig\n",
    "from models import BertLastFourEmbeddingsPooler, BertLastTwoCls, BertLastFourCls\n",
    "\n",
    "from transformers import *\n",
    "import logging\n",
    "print(torch.__version__)\n",
    "import warnings\n",
    "import transformers\n",
    "transformers.logging.set_verbosity_error()\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9439e30e-cf45-49b2-a69d-803b970559ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [[1, 0, 0],\n",
    "#  [0, 1, 0]]\n",
    "\n",
    "# [[L00, L10, L20],\n",
    "#  [L01, L11, L21]]\n",
    "\n",
    "# [[L00, LO1],\n",
    "#  [L10, L11],\n",
    "#  [L20, L21]]\n",
    "\n",
    "# --> []\n",
    "# [2, 3] * [3, 2]\n",
    "# [2, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4530039-4cde-4338-a37d-35e79e31ce9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class args(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # # bert_base_UER chinese_macbert_base chinese_roberta_wwm_ext bert_large_UER chinese_macbert_large roberta-36\n",
    "#         self.model_checkpoint =  '/root/autodl-tmp/pretrained_models/NEZHA-base-wwm'\n",
    "        \n",
    "        self.model_checkpoint =  '/root/autodl-tmp/CCF-小样本/Nezha_pytorch/nezha_model'\n",
    "        self.model_type = 'nezha'\n",
    "#         self.model_checkpoint =  '/root/autodl-tmp/CCF-小样本/Nezha_pytorch/pretrain/outputs'\n",
    "        self.device = 'cuda'\n",
    "        self.max_length = 412\n",
    "        self.bert_dim = 768\n",
    "        \n",
    "        self.num_reinit_layers = 1\n",
    "        self.reinit_pooler = False\n",
    "        self.layerwise_learning_rate_decay = 1\n",
    "        \n",
    "        self.bert_dim = 768\n",
    "        \n",
    "args = args()\n",
    "tokenizer = BertTokenizer.from_pretrained(args.model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b95ff14-5084-49b3-b3ec-873fbf55696d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "with open('./data/testB.json', 'r')as f:\n",
    "    test_data = f.readlines()\n",
    "    test_data = [eval(i.strip())for i in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e74284b-b92a-4afc-a510-dbd33525eaa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20890"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_all = []\n",
    "for idx, i in enumerate(test_data):\n",
    "    i['data_idx']=idx\n",
    "    test_data_all.append(i)\n",
    "len(test_data_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b90e3f7e-539c-4fe1-8c35-65e1c52da601",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20890it [01:01, 341.56it/s]\n"
     ]
    }
   ],
   "source": [
    "max_len = args.max_length - 4\n",
    "\n",
    "test_dict ={'input_ids': [], 'token_type_ids': [], 'attention_mask': [], 'input_lengths':[], 'data_idx': []}\n",
    "\n",
    "for idx, test_data_ in tqdm(enumerate(test_data_all)):\n",
    "    title = test_data_['title']\n",
    "    assignee = test_data_['assignee']\n",
    "    abstract = test_data_['abstract']\n",
    "    \n",
    "    title_ids_ori = tokenizer.encode(title, add_special_tokens=False)\n",
    "    assignee_ids_ori = tokenizer.encode(assignee, add_special_tokens=False)\n",
    "    abstract_ids_ori = tokenizer.encode(abstract, add_special_tokens=False)\n",
    "    \n",
    "    title_len = len(title_ids_ori)\n",
    "    assignee_len = len(assignee_ids_ori)\n",
    "    abstract_len = len(abstract_ids_ori)\n",
    "    \n",
    "    abstract_cutlen = max_len - title_len - assignee_len\n",
    "    \n",
    "    if abstract_cutlen >= abstract_len:\n",
    "        token_ids = [101] + title_ids_ori + [102] + assignee_ids_ori + [102] + abstract_ids_ori + [102]\n",
    "    else:\n",
    "        mid_len = int(abstract_cutlen / 2)\n",
    "        token_ids = [101] + title_ids_ori + [102] + assignee_ids_ori + [102] + abstract_ids_ori[: mid_len] + abstract_ids_ori[-mid_len: ] + [102]\n",
    "    token_type_ids = [0] * len(token_ids)\n",
    "    attention_mask = [1] * len(token_ids)\n",
    "    input_lengths = len(token_ids)\n",
    "    \n",
    "    test_dict['input_ids'].append(token_ids)\n",
    "    test_dict['token_type_ids'].append(token_type_ids)\n",
    "    test_dict['attention_mask'].append(attention_mask)\n",
    "    test_dict['input_lengths'].append(input_lengths)\n",
    "    test_dict['data_idx'].append(idx)\n",
    "\n",
    "        \n",
    "test_dict = {k: np.array(test_dict[k]) for k in test_dict}\n",
    "# In[6]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a71de376-8dd2-4d16-9824-0228368b0e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WBDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, tokenizer, batch_first=True, test=False):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.pad = tokenizer.pad_token_id\n",
    "        self.batch_first = batch_first\n",
    "        self.test = test\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data['input_ids'])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        instance = {}\n",
    "        instance['input_ids'] = self.data['input_ids'][index]\n",
    "        instance['token_type_ids'] = self.data['token_type_ids'][index]\n",
    "        instance['attention_mask'] = self.data['attention_mask'][index]\n",
    "        instance['input_lengths'] = self.data['input_lengths'][index]\n",
    "        if not self.test:\n",
    "            instance['labels'] = self.data['labels'][index]\n",
    "        else:\n",
    "            instance['data_idx'] = self.data['data_idx'][index]\n",
    "        \n",
    "        return instance\n",
    "\n",
    "    def collate(self, batch):\n",
    "        \n",
    "        input_ids = pad_sequence(\n",
    "            [torch.tensor(instance[\"input_ids\"], dtype=torch.long) for instance in batch],\n",
    "            batch_first=self.batch_first, padding_value=self.pad)\n",
    "        \n",
    "        token_type_ids = pad_sequence(\n",
    "            [torch.tensor(instance[\"token_type_ids\"], dtype=torch.long) for instance in batch],\n",
    "            batch_first=self.batch_first, padding_value=self.pad)\n",
    "        \n",
    "        attention_mask = pad_sequence(\n",
    "            [torch.tensor(instance[\"attention_mask\"], dtype=torch.long) for instance in batch],\n",
    "            batch_first=self.batch_first, padding_value=self.pad)\n",
    "        \n",
    "        input_lengths = torch.tensor([torch.tensor(instance[\"input_lengths\"], dtype=torch.int) for instance in batch])\n",
    "        \n",
    "        if not self.test:\n",
    "            labels = torch.tensor([torch.tensor(instance[\"labels\"], dtype=torch.long) for instance in batch])\n",
    "\n",
    "            return input_ids, token_type_ids, attention_mask, input_lengths, labels\n",
    "        else:\n",
    "            data_idx = torch.tensor([torch.tensor(instance[\"data_idx\"], dtype=torch.long) for instance in batch])\n",
    "            \n",
    "            return input_ids, token_type_ids, attention_mask, input_lengths, data_idx\n",
    "        \n",
    "test_batch_size = 64\n",
    "test_dataset =  WBDataset(test_dict, tokenizer, test=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=test_batch_size, collate_fn=test_dataset.collate, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75b9e9fd-59bc-451a-beff-025c5894cf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_model(model, test_loader, test_count):\n",
    "    \n",
    "    test_preds_fold = np.zeros((test_count, 36)) ####\n",
    "    \n",
    "    model.eval()\n",
    "    tk0 = tqdm(test_loader, total=len(test_loader))\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tk0):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            ids, segids, mask, lens, y_truth = batch\n",
    "\n",
    "            y_pred = model(\n",
    "                input_ids = ids, \n",
    "                input_mask = mask, \n",
    "                input_segids = segids,\n",
    "                input_lengths = lens,\n",
    "            )[0] \n",
    "\n",
    "            y_pred = torch.softmax(y_pred, dim=-1).cpu().detach().numpy()\n",
    "            test_bidx= i * test_batch_size\n",
    "            test_eidx= (i + 1) * test_batch_size\n",
    "            test_preds_fold[test_bidx:test_eidx] = y_pred\n",
    "            \n",
    "    return test_preds_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c29c189f-d659-46ba-a15c-a75bc0a1c3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "file_names = []\n",
    "file_path = r'./best_models/b_pseudo/'\n",
    "#提取文件中的所有文件生成一个列表\n",
    "folders = os.listdir(file_path)\n",
    "for file in folders:\n",
    "    if '.bin' in file:\n",
    "        file_names.append(file_path + file)\n",
    "        file_names.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a9493f7-561e-4979-9354-a8184d3f71e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 327/327 [01:45<00:00,  3.09it/s]\n",
      "100%|██████████| 327/327 [01:46<00:00,  3.07it/s]\n",
      "100%|██████████| 327/327 [01:45<00:00,  3.10it/s]\n",
      "100%|██████████| 327/327 [01:45<00:00,  3.09it/s]\n",
      "100%|██████████| 327/327 [01:45<00:00,  3.10it/s]\n",
      "100%|██████████| 327/327 [01:45<00:00,  3.09it/s]\n",
      "100%|██████████| 327/327 [01:46<00:00,  3.08it/s]\n",
      "100%|██████████| 327/327 [01:47<00:00,  3.04it/s]\n",
      "100%|██████████| 327/327 [01:47<00:00,  3.04it/s]\n",
      "100%|██████████| 327/327 [01:47<00:00,  3.04it/s]\n"
     ]
    }
   ],
   "source": [
    "b_test_preds = {}\n",
    "\n",
    "\n",
    "for file_name in file_names[:]:\n",
    "    if 'LastFourCls' in file_name:\n",
    "        model = modelBertLastFourCls\n",
    "    elif 'LastFourEmbeddingsPooler' in file_name:\n",
    "        model = modelBertLastFourEmbeddingsPooler\n",
    "    elif 'LastTwoCls' in file_name:\n",
    "        model = modelBertLastTwoCls\n",
    "    else:\n",
    "        print(fine_name)\n",
    "    \n",
    "    model.load_state_dict(torch.load(file_name))\n",
    "    test_pred = test_model(model, test_loader, len(test_data_all))\n",
    "    b_test_preds[file_name] = test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8470167c-52d2-4509-8086-6a12e429fe97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./best_models/curbert_6415_1103/nazhe_base-pseudo_labels_6410-BertLastFourEmbeddingsPooler-ls-fgm-seed2020-gkf2020-layerwise1-wd0.01-warmp0.-n_splits5-grad_acc1-num_epochs10-3e-05-32_fold_0.bin',\n",
       " './best_models/curbert_6415_1103/nazhe_base-pseudo_labels_6410-BertLastFourEmbeddingsPooler-ls-fgm-seed2020-gkf2020-layerwise1-wd0.01-warmp0.-n_splits5-grad_acc1-num_epochs10-3e-05-32_fold_1.bin',\n",
       " './best_models/curbert_6415_1103/nazhe_base-pseudo_labels_6410-BertLastFourEmbeddingsPooler-ls-fgm-seed2020-gkf2020-layerwise1-wd0.01-warmp0.-n_splits5-grad_acc1-num_epochs10-3e-05-32_fold_2.bin',\n",
       " './best_models/curbert_6415_1103/nazhe_base-pseudo_labels_6410-BertLastFourEmbeddingsPooler-ls-fgm-seed2020-gkf2020-layerwise1-wd0.01-warmp0.-n_splits5-grad_acc1-num_epochs10-3e-05-32_fold_3.bin',\n",
       " './best_models/curbert_6415_1103/nazhe_base-pseudo_labels_6410-BertLastFourEmbeddingsPooler-ls-fgm-seed2020-gkf2020-layerwise1-wd0.01-warmp0.-n_splits5-grad_acc1-num_epochs10-3e-05-32_fold_4.bin']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "file_names = []\n",
    "file_path = r'./best_models/curbert_6415_1103/'\n",
    "#提取文件中的所有文件生成一个列表\n",
    "folders = os.listdir(file_path)\n",
    "for file in folders:\n",
    "    if '.bin' in file:\n",
    "        file_names.append(file_path + file)\n",
    "        file_names.sort()\n",
    "file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6129622-2eba-4f6d-a8c6-23a173b31b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 327/327 [01:46<00:00,  3.08it/s]\n",
      "100%|██████████| 327/327 [01:47<00:00,  3.06it/s]\n",
      "100%|██████████| 327/327 [01:47<00:00,  3.05it/s]\n",
      "100%|██████████| 327/327 [01:47<00:00,  3.04it/s]\n",
      "100%|██████████| 327/327 [01:47<00:00,  3.05it/s]\n"
     ]
    }
   ],
   "source": [
    "for file_name in file_names[:]:\n",
    "    if 'LastFourCls' in file_name:\n",
    "        model = modelBertLastFourCls\n",
    "    elif 'LastFourEmbeddingsPooler' in file_name:\n",
    "        model = modelBertLastFourEmbeddingsPooler\n",
    "    elif 'LastTwoCls' in file_name:\n",
    "        model = modelBertLastTwoCls\n",
    "    else:\n",
    "        print(fine_name)\n",
    "    \n",
    "    model.load_state_dict(torch.load(file_name))\n",
    "    test_pred = test_model(model, test_loader, len(test_data_all))\n",
    "    b_test_preds[file_name] = test_pred\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69673322-01c5-4326-8d15-cee3ed853771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(b_test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02f85e1f-6d44-4621-a555-f216f65f3301",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/results_dict.pk', 'wb') as f:\n",
    "    joblib.dump(b_test_preds, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc907578-6cf4-4a96-8930-64b1ea16c372",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "modelBertLastFourEmbeddingsPooler = BertLastFourEmbeddingsPooler(args).to(device)\n",
    "modelBertLastTwoCls = BertLastTwoCls(args).to(device)\n",
    "modelBertLastFourCls = BertLastFourCls(args).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aaf56114-e35c-40aa-a32c-ccb8b2d3ecb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 327/327 [01:46<00:00,  3.08it/s]\n",
      "100%|██████████| 327/327 [01:46<00:00,  3.06it/s]\n",
      "100%|██████████| 327/327 [01:45<00:00,  3.09it/s]\n",
      "100%|██████████| 327/327 [01:47<00:00,  3.05it/s]\n",
      "100%|██████████| 327/327 [01:47<00:00,  3.05it/s]\n",
      "100%|██████████| 327/327 [01:46<00:00,  3.06it/s]\n",
      "100%|██████████| 327/327 [01:46<00:00,  3.07it/s]\n",
      "100%|██████████| 327/327 [01:46<00:00,  3.07it/s]\n",
      "100%|██████████| 327/327 [01:46<00:00,  3.07it/s]\n",
      "100%|██████████| 327/327 [01:46<00:00,  3.06it/s]\n",
      "100%|██████████| 327/327 [01:47<00:00,  3.05it/s]\n",
      "100%|██████████| 327/327 [01:47<00:00,  3.05it/s]\n",
      "100%|██████████| 327/327 [01:46<00:00,  3.06it/s]\n",
      "100%|██████████| 327/327 [01:47<00:00,  3.05it/s]\n",
      "100%|██████████| 327/327 [01:46<00:00,  3.06it/s]\n",
      "100%|██████████| 327/327 [01:47<00:00,  3.05it/s]\n",
      "100%|██████████| 327/327 [01:46<00:00,  3.06it/s]\n",
      "100%|██████████| 327/327 [01:47<00:00,  3.05it/s]\n",
      "100%|██████████| 327/327 [01:47<00:00,  3.05it/s]\n",
      "100%|██████████| 327/327 [01:46<00:00,  3.06it/s]\n",
      "100%|██████████| 327/327 [01:47<00:00,  3.05it/s]\n",
      "100%|██████████| 327/327 [01:46<00:00,  3.06it/s]\n",
      "100%|██████████| 327/327 [01:47<00:00,  3.05it/s]\n",
      "100%|██████████| 327/327 [01:47<00:00,  3.05it/s]\n",
      "100%|██████████| 327/327 [01:46<00:00,  3.06it/s]\n",
      "100%|██████████| 327/327 [01:46<00:00,  3.07it/s]\n",
      "100%|██████████| 327/327 [01:46<00:00,  3.06it/s]\n",
      "100%|██████████| 327/327 [01:46<00:00,  3.06it/s]\n",
      "100%|██████████| 327/327 [01:46<00:00,  3.06it/s]\n",
      "100%|██████████| 327/327 [01:46<00:00,  3.06it/s]\n",
      "100%|██████████| 327/327 [01:46<00:00,  3.06it/s]\n",
      "100%|██████████| 327/327 [01:46<00:00,  3.08it/s]\n",
      "100%|██████████| 327/327 [01:46<00:00,  3.06it/s]\n",
      "100%|██████████| 327/327 [01:46<00:00,  3.06it/s]\n",
      "100%|██████████| 327/327 [01:46<00:00,  3.06it/s]\n",
      "100%|██████████| 327/327 [01:46<00:00,  3.07it/s]\n",
      "100%|██████████| 327/327 [01:46<00:00,  3.07it/s]\n",
      "100%|██████████| 327/327 [01:46<00:00,  3.06it/s]\n",
      "100%|██████████| 327/327 [01:46<00:00,  3.07it/s]\n",
      "100%|██████████| 327/327 [01:46<00:00,  3.07it/s]\n",
      "100%|██████████| 327/327 [01:46<00:00,  3.07it/s]\n",
      "100%|██████████| 327/327 [01:46<00:00,  3.07it/s]\n",
      "100%|██████████| 327/327 [01:46<00:00,  3.07it/s]\n",
      "100%|██████████| 327/327 [01:46<00:00,  3.07it/s]\n",
      "100%|██████████| 327/327 [01:46<00:00,  3.07it/s]\n"
     ]
    }
   ],
   "source": [
    "test_preds = {}\n",
    "\n",
    "\n",
    "for file_name in file_names[:]:\n",
    "    if 'LastFourCls' in file_name:\n",
    "        model = modelBertLastFourCls\n",
    "    elif 'LastFourEmbeddingsPooler' in file_name:\n",
    "        model = modelBertLastFourEmbeddingsPooler\n",
    "    elif 'LastTwoCls' in file_name:\n",
    "        model = modelBertLastTwoCls\n",
    "    else:\n",
    "        print(fine_name)\n",
    "    \n",
    "    model.load_state_dict(torch.load(file_name))\n",
    "    test_pred = test_model(model, test_loader, len(test_data_all))\n",
    "    test_preds[file_name] = test_pred\n",
    "    \n",
    "#     test_preds.append(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9197f849-e2c3-4cd9-98de-cc78da626a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/B_results_dict.pk', 'wb') as f:\n",
    "    joblib.dump(test_preds, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90d56d6-b741-4f34-88ab-5582decc2d77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6324e40-9de2-4f81-a821-a135c47598cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2a0bc9f6-823f-40f3-bd45-a925c344d366",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = pd.read_csv('./data/submit_example_B.csv')\n",
    "\n",
    "final_preds = np.zeros(test_preds[0].shape)\n",
    "for idx in range(len(test_preds)):\n",
    "    test_preds_fold = test_preds[idx]\n",
    "    final_preds += test_preds_fold  / len(test_preds)\n",
    "    \n",
    "final_reses = np.argmax(final_preds, axis=1)\n",
    "\n",
    "final_sub = []\n",
    "final_sub_ = []\n",
    "for i in range(len(df_pred)):\n",
    "    final_res = final_reses[i]\n",
    "    final_sub_.append(str(final_res))\n",
    "    \n",
    "    \n",
    "final_reses_vote = {}\n",
    "for idx in range(len(test_preds)):\n",
    "    test_preds_fold = test_preds[idx]\n",
    "    \n",
    "    for i in range(test_preds_fold.shape[0]):\n",
    "        if i not in final_reses_vote:\n",
    "            final_reses_vote[i] = {i:0 for i in range(36)}\n",
    "        sample_res = np.argmax(test_preds_fold[i], axis=0)\n",
    "        final_reses_vote[i][sample_res] += 1\n",
    " \n",
    "\n",
    "final_sub_vote_ = []\n",
    "for idx, sample_id in enumerate(final_reses_vote):\n",
    "    \n",
    "    \n",
    "    sample_label = -1\n",
    "    sample_label_count = final_reses_vote[sample_id]\n",
    "    sample_label_sort =  sorted(sample_label_count.items(), key=lambda d:d[1], reverse=True)\n",
    "    \n",
    "    if sample_label_sort[0][1] > sample_label_sort[1][1]:\n",
    "        sample_label = str(sample_label_sort[0][0])\n",
    "    else:\n",
    "        sample_label = np.argmax(final_preds[sample_id], axis=0)\n",
    "        \n",
    "    final_sub_vote_.append(str(sample_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "030d70fb-7205-40bc-adf2-d0ec9410d9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pseudo label_id\n",
    "\n",
    "pseudo_data = test_data.copy()\n",
    "for label, pseudo_data_ in zip(final_sub_vote_, pseudo_data):\n",
    "    pseudo_data_['label_id'] = label\n",
    "    \n",
    "with open('./data/pseudo_B_1108_v1.json', 'w') as f:\n",
    "    json.dump(pseudo_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c48ed165-0aad-44a9-952d-85fc07102362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '487fdc38d8b39d35ba18206b835e57e3',\n",
       "  'title': '建立灵活以太网路径的方法和网络设备',\n",
       "  'assignee': '华为技术有限公司',\n",
       "  'abstract': '本申请提供了一种建立FlexE路径的方法和网络设备，能够降低节点的控制面的管理复杂度和信令开销。该方法包括：第一转发节点接收第一源节点发送的第一路径建立请求消息，第一路径建立请求消息用于建立第一源节点到第一目的节点之间的第一FlexE路径，第一FlexE路径用于传输第一业务流；第一转发节点根据第一路径建立请求消息，建立第一转发节点至第二转发节点之间的第二FlexE路径，并在建立第二FlexE路径后，删除已建立的第一转发节点到第二转发节点之间的第三FlexE路径；第一转发节点建立并保存第一FlexE路径和第二FlexE路径的对应关系，使得将第一业务流和第二业务流映射至第二FlexE路径的时隙上。',\n",
       "  'data_idx': 0,\n",
       "  'label_id': '0'},\n",
       " {'id': '32e267e4a7bc6bd000fcd45616e448d7',\n",
       "  'title': '一种程序代码标记方法及装置',\n",
       "  'assignee': '阿里巴巴集团控股有限公司',\n",
       "  'abstract': '本申请公开了一种程序代码标记方法及装置，该方法包括：顺序读取程序代码，当读取到程序代码中包含的切换标识符时，确定该切换标识符对应的标记规则，并采用该标记规则，对后续读取到的程序代码进行标记，直到再次读取到程序代码中包含的下一个切换标识符时，再次确定该下一个切换标识符对应的另一标记规则，并采用该另一标记规则，继续对读取到的程序代码进行标记，直到将全部的程序代码标记完为止。通过上述方法，即使程序代码由两种以上的编程语言进行编写的，但是可以根据切换标识符确定需要使用哪个标记规则，对后续读取到的程序代码进行准确的标记。',\n",
       "  'data_idx': 1,\n",
       "  'label_id': '4'},\n",
       " {'id': '9b808e8060a5e84886e4fb1bf94699d4',\n",
       "  'title': '一种新型环境保护净化器',\n",
       "  'assignee': '李菊红',\n",
       "  'abstract': '本实用新型适用于环境保护技术领域，提供了一种新型环境保护净化器，包括机体组件、驱动组件和过滤组件，所述机体组件包括外壳，所述外壳的外侧壁均匀开设有若干个进气孔；通过在外壳的内侧壁设置第一紫外线杀菌灯管，同时在隔板的一侧均匀固定第二紫外线杀菌灯管，在该净化器工作时，第一紫外线杀菌灯管和第二紫外线杀菌灯管通电同步工作，空气在被过滤过程中可以被第一紫外线杀菌灯管和第二紫外线杀菌灯管工作时产生的紫外线灯光进行照射，使得空气中的病毒和细菌被有效杀死，以满足医院病房的空气净化使用，避免传统的进化器只能对粉尘、PM2.5等空气杂质进行过滤，无法对病毒进行过滤处理，难以满足医院病房使用的问题。',\n",
       "  'data_idx': 2,\n",
       "  'label_id': '7'},\n",
       " {'id': '1e5366849744046a289799101f74494c',\n",
       "  'title': '一种具有屏蔽功能的密闭门铰链',\n",
       "  'assignee': '江苏龙腾门业有限公司',\n",
       "  'abstract': '本实用新型公开了一种具有屏蔽功能的密闭门铰链，包括门边框，门边框上部通过螺栓固定有两个上铰链，上铰链包括支撑块，支撑块中部水平设置有水平孔，水平孔内设置有铰链板，门边框上部设置有两组支撑组件，支撑组件包括两个支撑板，铰链板一端设置于两个支撑板中间，且通过紧固螺栓进行固定，支撑块纵向设置有贯穿孔，贯穿孔顶部和底部均形成有台阶孔，台阶孔内设置有调整轴承，两个调整轴承之间安装有调整轴，调整轴贯穿铰链板设置，支撑块上方通过螺栓设置有第一盖板，支撑块下方通过螺栓设置有第二盖板，门边框下方通过螺栓设置有承重臂，承重臂下部通过螺栓固定有承重轴，承重轴下方设置有下铰链，下铰链通过螺栓固定于门边框。',\n",
       "  'data_idx': 3,\n",
       "  'label_id': '2'},\n",
       " {'id': '7827bbef8b8090da1e47beb46713d086',\n",
       "  'title': '透明高温蒸煮膜用聚丙烯组合物及其制备方法',\n",
       "  'assignee': '中国石油化工股份有限公司',\n",
       "  'abstract': '本发明涉及一种透明高温蒸煮膜用聚丙烯组合物及其制备方法，由如下重量份数的原料制成：抗冲共聚聚丙烯树脂、抗氧剂、吸酸剂、抗静电剂、成核剂、防黏剂、爽滑剂。本发明得到的聚丙烯组合物，有较低的雾度，具有优异的透明性、良好的抗冲击性能及较高的耐热性能，同时表面光泽度高，使用该聚丙烯组合物可以加工成型得到高透明的耐高温蒸煮用的薄膜。36μm厚的流延薄膜雾度仅为0.52％，同时薄膜的拉伸强度、断裂拉伸应变以及耐撕裂性能均高于普通无规共聚聚丙烯制备的薄膜。该组合物制备的透明高温蒸煮膜主要应用于食品包装等领域。',\n",
       "  'data_idx': 4,\n",
       "  'label_id': '5'}]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pseudo_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b96b5cf1-58d8-47a2-a9c4-b673534e6e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/pseudo_labels_6410.json', 'r') as f:\n",
    "    pseudo_data_a = f.readlines()\n",
    "    pseudo_data_a = [eval(i.strip())for i in pseudo_data_a]\n",
    "    \n",
    "label_a = [i['label_id'] for i in pseudo_data_a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b6cee270-a193-413f-88c8-d6612204589d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_all(seed_value):\n",
    "    random.seed(seed_value) # Python\n",
    "    np.random.seed(seed_value) # cpu vars\n",
    "seed_all(20201227)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "138b7d21-2404-4683-a518-907306ea49d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "array = np.array(list(range(len(test_data))))\n",
    "labels = final_sub_vote_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "be4c3be6-2757-41ad-a121-c9d1b1266377",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_ = np.array(list(range(len(test_data))))\n",
    "labels = final_sub_vote_\n",
    "\n",
    "pseudo_idxes_B = []\n",
    "for i in range(20):\n",
    "    train_idxes, test_idxes, _, _ = train_test_split(np.expand_dims(array,axis=-1), labels, train_size=0.3, stratify=labels, random_state = 666)\n",
    "    train_idxes = train_idxes.squeeze()\n",
    "    pseudo_idxes_B.append(train_idxes)\n",
    "    \n",
    "    \n",
    "\n",
    "array_a = np.array(list(range(len(pseudo_data_a))))\n",
    "labels_a = label_a   \n",
    "    \n",
    "pseudo_idxes_A = []\n",
    "for i in range(20):\n",
    "    train_idxes, test_idxes, _, _ = train_test_split(np.expand_dims(array_a,axis=-1), labels_a, train_size=0.3, stratify=labels_a, random_state = 666)\n",
    "    train_idxes = train_idxes.squeeze()\n",
    "    pseudo_idxes_A.append(train_idxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "51b99ca3-107b-4f38-bb03-dcaf09ca6620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stratified_sample = train_test_split(np.expand_dims(array,axis=-1), labels, train_size=0.3, stratify=labels, random_state = 666)\n",
    "\n",
    "with open('./data/pseudo_idxes.pk', 'wb') as f:\n",
    "    joblib.dump({'pseudo_idxes_A': pseudo_idxes_A, 'pseudo_idxes_B': pseudo_idxes_B}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3aa0ec43-0ab5-40f8-9701-ac0cf5f53581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8462, 10830, 14703, ...,  6739,  8841,  8855])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_idxes.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "db5bb7e1-ac49-4336-a09b-e9747a493820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     1,     2, ..., 20887, 20888, 20889]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.expand_dims(array,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20092bb-678a-4231-a875-b4748d97979a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
