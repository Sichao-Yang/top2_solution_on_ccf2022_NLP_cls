# -*- coding: utf-8 -*-
"""pretrain.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zJJnT_Sg4A7i9ST1uv7FwooeY1qW8HLw
"""

import json
from torch.utils.data import DataLoader, Dataset
import torch
from transformers import AutoTokenizer, DataCollatorForLanguageModeling, DataCollatorWithPadding, Trainer, \
    AutoModelWithLMHead, TrainingArguments, AutoConfig,RobertaTokenizer,RobertaForMaskedLM, BertTokenizer
import pandas as pd
from pathlib import Path
from Nezha_pytorch.finetuning.NEZHA.modeling_nezha import NeZhaModel, NeZhaForMaskedLM
from Nezha_pytorch.finetuning.NEZHA.configuration_nezha import NeZhaConfig

pd.set_option('max_colwidth', 300)

# !wget https://s3.amazonaws.com/code-search-net/CodeSearchNet/v2/python.zip

# !unzip python.zip

import ast

def read_jsonfile(file_name):
    data = []
    with open(file_name, encoding='utf-8') as f:
        data = json.loads(f.read(), strict=False)
    return data

# pydf = pd.DataFrame(read_jsonfile("发明专利数据.json"))

# pydf = pydf[['pat_name', 'pat_applicant', 'pat_summary']]
# pydf = pydf.dropna()
# pydf['source'] = pydf['pat_name'] +  pydf['pat_applicant'] + pydf['pat_summary']


# df.to_excel('./data/pretrain_ab.xls')


pydf = pd.read_excel('./data/pretrain_ab.xls')
# tokenizer = AutoTokenizer.from_pretrained('hfl/chinese-roberta-wwm-ext')
model_checkpoint =  '/root/autodl-tmp/CCF-小样本/Nezha_pytorch/nezha_model'
tokenizer = BertTokenizer.from_pretrained(model_checkpoint)

# config = NeZhaConfig.from_pretrained(args.model_checkpoint, output_hidden_states=True)
model = NeZhaForMaskedLM.from_pretrained(model_checkpoint)

data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer, mlm=True, mlm_probability=0.15
)


class PDataset(Dataset):

    def __init__(self, df, tokenizer):
        super().__init__()
        self.df = df.reset_index(drop=True)
        # maxlen allowed by model config
        self.tokenizer = tokenizer

    def __getitem__(self, index):
        row = self.df.iloc[index]
        doc = row.source
       
        sep = self.tokenizer.sep_token_id
        cls = self.tokenizer.cls_token_id
        inputs = {}
        try:
            doc_id = tokenizer(doc, truncation=True, max_length=412)
            doc_id = data_collator([doc_id])
            inputs['input_ids'] = doc_id['input_ids'][0].tolist()
            inputs['labels'] = doc_id['labels'][0].tolist()
       
            if 'token_type_ids' in inputs:
                inputs['token_type_ids'] = [0] * len(inputs['input_ids'])
        except:
            print('*'*20)
            print(doc)
            print('*'*20)
          
        return inputs

    def __len__(self):
        return self.df.shape[0]


mask_id = tokenizer.mask_token_id
def data_collator_p(batch):
    max_length = max([len(i['input_ids']) for i in batch])
    input_id, token_type, labels = [], [], []
    for i in batch:
        input_id.append(i['input_ids'] + [mask_id]*(max_length-len(i['input_ids'])))
        #token_type.append(i['token_type_ids'] + [1] * (max_length - len(i['token_type_ids'])))
        labels.append(i['labels'] + [-100] * (max_length - len(i['labels'])))
    output={}
    output['input_ids'] = torch.as_tensor(input_id, dtype=torch.long)
    #output['token_type_ids'] = torch.as_tensor(token_type, dtype=torch.long)
    output['labels'] = torch.as_tensor(labels, dtype=torch.long)
    return output

training_args = TrainingArguments(
    output_dir='./pretrain_domain_code',
    overwrite_output_dir=True,
    num_train_epochs=4,
    per_device_train_batch_size=16,
    save_total_limit=1,
    save_strategy='epoch',
    learning_rate=5e-5,
    # fp16=True,
    gradient_accumulation_steps=4,
)
dataset = PDataset(pydf, tokenizer)

# model = AutoModelWithLMHead.from_pretrained('hfl/chinese-roberta-wwm-ext')
trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator_p,
    train_dataset=dataset,
)
trainer.train()
trainer.save_model('./pretrain_domain_code')
