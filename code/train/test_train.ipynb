{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b543a53-c707-4ec5-a80a-a05bd8856bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"..\") \n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from functools import partial\n",
    "import scipy as sp\n",
    "import joblib\n",
    "import gc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import re\n",
    "import math\n",
    "import json\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset)\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "from layers.losses import LabelSmoothingCrossEntropy, FocalLoss, LabelSM_Focal, DiceLoss, LabelSmoothingCrossEntropyWeight\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from layers.adversarial import FGM, PGD, Lookahead\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset)\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from NEZHA.modeling_nezha import NeZhaModel\n",
    "from NEZHA.configuration_nezha import NeZhaConfig\n",
    "# from transformers import NezhaModel\n",
    "# from transformers import NezhaConfig\n",
    "\n",
    "from transformers import *\n",
    "import logging\n",
    "import warnings\n",
    "import transformers\n",
    "transformers.logging.set_verbosity_error()\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from models import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb42c216-4693-45e4-9dee-a14f9608e45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 写日志\n",
    "logger = logging.getLogger('mylogger')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "timestamp = time.strftime(\"%Y.%m.%d_%H.%M.%S\", time.localtime())\n",
    "fh = logging.FileHandler('log_model.txt')\n",
    "fh.setLevel(logging.DEBUG)\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('[%(asctime)s][%(levelname)s] ## %(message)s')\n",
    "fh.setFormatter(formatter)\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(fh)\n",
    "logger.addHandler(ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2e76cd3-eb4c-4788-9ba2-cb78dff91002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机数种子\n",
    "def seed_all(seed_value):\n",
    "    random.seed(seed_value) # Python\n",
    "    np.random.seed(seed_value) # cpu vars\n",
    "    torch.manual_seed(seed_value) # cpu  vars\n",
    "    \n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value) # gpu vars\n",
    "        torch.backends.cudnn.deterministic = True  #needed\n",
    "        torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1909ddd9-8d43-4cef-a185-b624a2b304ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class args(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        # 预训练模型路径\n",
    "        self.model_checkpoint =  '../../user_data/nezha_model'\n",
    "        self.bert_model_checkpoint = '../../user_data/mac_bert_model'\n",
    "        # 模型有如下选择，具体详见 models.py\n",
    "        # BertLastTwoCls BertLastFourCls BertLastFourEmbeddingsPooler BertDynEmbeddings BertRNN\n",
    "        self.model_type = 'BertLastFourCls'\n",
    "        self.nezha = True\n",
    "        self.load_pretrained = True \n",
    "        self.loss_type = 'ce'\n",
    "        self.device = 'cuda'\n",
    "        self.max_length = 412\n",
    "        self.bert_dim = 768\n",
    "        self.test_batch_size = 64\n",
    "        \n",
    "        # 对抗训练\n",
    "        self.fgm = 0\n",
    "        self.pgd = 0\n",
    "        self.fp16 = 0\n",
    "        \n",
    "        # 伪标签训练设置\n",
    "        self.pseudo = 0\n",
    "        self.traindata_epoch = 2\n",
    "        \n",
    "        # reinit以及llrd\n",
    "        self.num_reinit_layers = 0\n",
    "        self.reinit_pooler = False\n",
    "        self.layerwise_learning_rate_decay = 1\n",
    "        \n",
    "args = args()\n",
    "tokenizer = BertTokenizer.from_pretrained(args.model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdab8a06-1dff-4b6d-968e-cfcca6024339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "with open('../../raw_data/train.json', 'r') as f:\n",
    "    train_data = f.readlines()\n",
    "    train_data = [eval(i.strip())for i in train_data]\n",
    "    \n",
    "with open('../../raw_data/train.json', 'r') as f:\n",
    "    test_a = f.readlines()\n",
    "    test_a = [eval(i.strip())for i in test_a]\n",
    "    \n",
    "with open('../../raw_data/testB.json', 'r') as f:\n",
    "    test_b = f.readlines()\n",
    "    test_b = [eval(i.strip())for i in test_b]\n",
    "    \n",
    "with open('../../raw_data/final_pseudo_b.json', 'r') as f:\n",
    "    pseudo_data_b = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4aa1ce86-7607-4b64-90f1-a21f2f2c7c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_traindata(train_data, tokenizer, max_len=412):\n",
    "    train_dict ={'input_ids': [], 'token_type_ids': [], 'attention_mask': [], 'input_lengths':[], 'labels': []}\n",
    "\n",
    "#     max_len = args.max_length - 4\n",
    "    for train_data_ in tqdm(train_data[: ]):\n",
    "        title = train_data_['title']\n",
    "        assignee = train_data_['assignee']\n",
    "        abstract = train_data_['abstract']\n",
    "        label = int(train_data_['label_id'])\n",
    "\n",
    "        title_ids_ori = tokenizer.encode(title, add_special_tokens=False)[:30]\n",
    "        assignee_ids_ori = tokenizer.encode(assignee, add_special_tokens=False)[-10:]\n",
    "        abstract_ids_ori = tokenizer.encode(abstract, add_special_tokens=False)\n",
    "\n",
    "        title_len = len(title_ids_ori)\n",
    "        assignee_len = len(assignee_ids_ori)\n",
    "        abstract_len = len(abstract_ids_ori)\n",
    "\n",
    "        abstract_cutlen = max_len - title_len - assignee_len\n",
    "\n",
    "        if abstract_cutlen >= abstract_len:\n",
    "            token_ids = [101] + title_ids_ori + [102] + assignee_ids_ori + [102] + abstract_ids_ori + [102]\n",
    "        else:\n",
    "            mid_len = int(abstract_cutlen / 2)\n",
    "            token_ids = [101] + title_ids_ori + [102] + assignee_ids_ori + [102] + abstract_ids_ori[: mid_len] + abstract_ids_ori[-mid_len: ] + [102]\n",
    "        token_type_ids = [0] * len(token_ids)\n",
    "        attention_mask = [1] * len(token_ids)\n",
    "        input_lengths = len(token_ids)\n",
    "\n",
    "        train_dict['input_ids'].append(token_ids)\n",
    "        train_dict['token_type_ids'].append(token_type_ids)\n",
    "        train_dict['attention_mask'].append(attention_mask)\n",
    "        train_dict['input_lengths'].append(input_lengths)\n",
    "        train_dict['labels'].append(label)\n",
    "\n",
    "    train_dict = {k: np.array(train_dict[k]) for k in train_dict}\n",
    "    return train_dict\n",
    "\n",
    "\n",
    "def create_testdata(test_data, tokenizer, max_len=412):\n",
    "    test_dict ={'input_ids': [], 'token_type_ids': [], 'attention_mask': [], 'input_lengths':[], 'data_idx': []}\n",
    "    test_data_all = []\n",
    "    for idx, i in enumerate(test_data):\n",
    "        i['data_idx']=idx\n",
    "        test_data_all.append(i)\n",
    "\n",
    "    for idx, test_data_ in tqdm(enumerate(test_data_all)):\n",
    "        title = test_data_['title']\n",
    "        assignee = test_data_['assignee']\n",
    "        abstract = test_data_['abstract']\n",
    "\n",
    "        title_ids_ori = tokenizer.encode(title, add_special_tokens=False)[:30]\n",
    "        assignee_ids_ori = tokenizer.encode(assignee, add_special_tokens=False)[-10:]\n",
    "        abstract_ids_ori = tokenizer.encode(abstract, add_special_tokens=False)\n",
    "\n",
    "        title_len = len(title_ids_ori)\n",
    "        assignee_len = len(assignee_ids_ori)\n",
    "        abstract_len = len(abstract_ids_ori)\n",
    "\n",
    "        abstract_cutlen = max_len - title_len - assignee_len\n",
    "\n",
    "        if abstract_cutlen >= abstract_len:\n",
    "            token_ids = [101] + title_ids_ori + [102] + assignee_ids_ori + [102] + abstract_ids_ori + [102]\n",
    "        else:\n",
    "            mid_len = int(abstract_cutlen / 2)\n",
    "            token_ids = [101] + title_ids_ori + [102] + assignee_ids_ori + [102] + abstract_ids_ori[: mid_len] + abstract_ids_ori[-mid_len: ] + [102]\n",
    "        token_type_ids = [0] * len(token_ids)\n",
    "        attention_mask = [1] * len(token_ids)\n",
    "        input_lengths = len(token_ids)\n",
    "\n",
    "        test_dict['input_ids'].append(token_ids)\n",
    "        test_dict['token_type_ids'].append(token_type_ids)\n",
    "        test_dict['attention_mask'].append(attention_mask)\n",
    "        test_dict['input_lengths'].append(input_lengths)\n",
    "        test_dict['data_idx'].append(idx)\n",
    "\n",
    "\n",
    "    test_dict = {k: np.array(test_dict[k]) for k in test_dict}\n",
    "    return test_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ad0f341-b5c2-4940-a262-d3c604df452e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WBDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, tokenizer, batch_first=True, test=False):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.pad = tokenizer.pad_token_id\n",
    "        self.batch_first = batch_first\n",
    "        self.test = test\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data['input_ids'])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        instance = {}\n",
    "        instance['input_ids'] = self.data['input_ids'][index]\n",
    "        instance['token_type_ids'] = self.data['token_type_ids'][index]\n",
    "        instance['attention_mask'] = self.data['attention_mask'][index]\n",
    "        instance['input_lengths'] = self.data['input_lengths'][index]\n",
    "        if not self.test:\n",
    "            instance['labels'] = self.data['labels'][index]\n",
    "        else:\n",
    "            instance['data_idx'] = self.data['data_idx'][index]\n",
    "        \n",
    "        return instance\n",
    "\n",
    "    def collate(self, batch):\n",
    "        \n",
    "        input_ids = pad_sequence(\n",
    "            [torch.tensor(instance[\"input_ids\"], dtype=torch.long) for instance in batch],\n",
    "            batch_first=self.batch_first, padding_value=self.pad)\n",
    "        \n",
    "        token_type_ids = pad_sequence(\n",
    "            [torch.tensor(instance[\"token_type_ids\"], dtype=torch.long) for instance in batch],\n",
    "            batch_first=self.batch_first, padding_value=self.pad)\n",
    "        \n",
    "        attention_mask = pad_sequence(\n",
    "            [torch.tensor(instance[\"attention_mask\"], dtype=torch.long) for instance in batch],\n",
    "            batch_first=self.batch_first, padding_value=self.pad)\n",
    "        \n",
    "        input_lengths = torch.tensor([torch.tensor(instance[\"input_lengths\"], dtype=torch.int) for instance in batch])\n",
    "        \n",
    "        if not self.test:\n",
    "            labels = torch.tensor([torch.tensor(instance[\"labels\"], dtype=torch.long) for instance in batch])\n",
    "\n",
    "            return input_ids, token_type_ids, attention_mask, input_lengths, labels\n",
    "        else:\n",
    "            data_idx = torch.tensor([torch.tensor(instance[\"data_idx\"], dtype=torch.long) for instance in batch])\n",
    "            \n",
    "            return input_ids, token_type_ids, attention_mask, input_lengths, data_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d121536-39ca-4831-9c62-a0d139b23b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedF1(object):\n",
    "    def __init__(self):\n",
    "        self.coef_ = []\n",
    "    def _kappa_loss(self, coef, X, y):\n",
    "        \"\"\"\n",
    "        y_hat = argmax(coef*X, axis=-1)\n",
    "        :param coef: (1D array) weights\n",
    "        :param X: (2D array)logits\n",
    "        :param y: (1D array) label\n",
    "        :return: -f1\n",
    "        \"\"\"\n",
    "        X_p = np.copy(X)\n",
    "        X_p = coef*X_p\n",
    "#         ll = accuracy_score(y, np.argmax(X_p, axis=-1))\n",
    "        ll = f1_score(y, np.argmax(X_p, axis=-1), average='macro')\n",
    "        return -ll\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        loss_partial = partial(self._kappa_loss, X=X, y=y)\n",
    "        initial_coef = [1. for _ in range(36)] ###########\n",
    "        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n",
    "        \n",
    "    def predict(self, X, y):\n",
    "        X_p = np.copy(X)\n",
    "        X_p = self.coef_['x'] * X_p\n",
    "#         return accuracy_score(y, np.argmax(X_p, axis=-1))\n",
    "        return f1_score(y, np.argmax(X_p, axis=-1), average='macro')\n",
    "    \n",
    "    def coefficients(self):\n",
    "        return self.coef_['x']\n",
    "    \n",
    "def metric(y_true, y_pred):\n",
    "    \n",
    "    if len(y_pred.shape) > 1:\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "    if len(y_true.shape) > 1:\n",
    "        y_true = np.argmax(y_true, axis=1)\n",
    "        \n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average= 'macro')\n",
    "    return acc, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de8f42f7-eca2-4ef4-b766-d8c274dad0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(data_loader, model, optimizer, device, scheduler):\n",
    "    logger.info('training.............')\n",
    "    model.train()\n",
    "    losses = []\n",
    "    train_f1 = []\n",
    "\n",
    "    if args.fp16==1:\n",
    "        from torch.cuda import amp\n",
    "        scaler = amp.GradScaler()\n",
    "    if args.fgm==1:\n",
    "        fgm = FGM(model)\n",
    "    \n",
    "    tk0 = tqdm(data_loader, total=len(data_loader))\n",
    "    for step, batch in enumerate(tk0):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        ids, segids, mask, lens, y_truth = batch\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids = ids, \n",
    "            input_mask = mask, \n",
    "            input_segids = segids,\n",
    "            input_lengths = lens,\n",
    "            input_labels = y_truth\n",
    "        ) \n",
    "        \n",
    "        loss, logits = outputs\n",
    "        loss = loss.mean() / grad_acc\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "        \n",
    "        if args.fp16==1:\n",
    "            scaler.scale(loss).backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            \n",
    "        if args.fgm==1:\n",
    "            fgm.attack()\n",
    "            loss_adv = model(\n",
    "                input_ids = ids, \n",
    "                input_mask = mask, \n",
    "                input_segids = segids,\n",
    "                input_lengths = lens,\n",
    "                input_labels = y_truth\n",
    "            )[0]\n",
    "            loss_adv = loss_adv.mean()\n",
    "            loss_adv.backward()\n",
    "            fgm.restore()\n",
    "            \n",
    "        if args.pgd==1:\n",
    "            pgd = PGD(model)\n",
    "            K = 3\n",
    "            pgd.backup_grad()\n",
    "            for t in range(K):\n",
    "                # 在embedding上添加对抗扰动, first attack时备份param.data\n",
    "                pgd.attack(is_first_attack=(t == 0))\n",
    "                if t != K - 1:\n",
    "                    model.zero_grad()\n",
    "                else:\n",
    "                    pgd.restore_grad()\n",
    "                loss_adv = model(\n",
    "                    input_ids = ids, \n",
    "                    input_mask = mask, \n",
    "                    input_segids = segids,\n",
    "                    input_lengths = lens,\n",
    "                    input_labels = y_truth\n",
    "                )[0]\n",
    "                loss_adv = loss_adv.mean()\n",
    "                loss_adv.backward()  # 反向传播，并在正常的grad基础上，累加对抗训练的梯度\n",
    "            pgd.restore()  # 恢复embedding参数\n",
    "            \n",
    "        y_prod_cpu = logits.cpu().detach().numpy()\n",
    "        y_truth_cpu = y_truth.cpu().detach().numpy()\n",
    "        \n",
    "        acc, f1 = metric(y_truth_cpu, y_prod_cpu)\n",
    "        train_f1.append(acc)\n",
    "        losses.append(loss.item())  \n",
    "        if step == len(data_loader)-1 or (step + 1) % grad_acc==0:\n",
    "            \n",
    "            if args.fp16==1:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                optimizer.step()\n",
    "                \n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            model.zero_grad()          \n",
    "            \n",
    "        tk0.set_postfix(loss=np.mean(losses), avg_f1=np.mean(train_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51ec77d3-65c9-4079-8ca7-66fc187bb24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(data_loader, model, val_count, device=torch.device(\"cpu\")):\n",
    "    logger.info('evaling.............')\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    eval_f1 = []\n",
    "    preds_tags = []\n",
    "    ture_labels = []\n",
    "    \n",
    "    valid_preds_fold = np.zeros((val_count, 36))\n",
    "    val_true = np.zeros((val_count))\n",
    "    with torch.no_grad():\n",
    "        tk0 = tqdm(data_loader, total=len(data_loader))\n",
    "        for step, batch in enumerate(tk0):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            ids, segids, mask, lens, y_truth = batch\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids = ids, \n",
    "                input_mask = mask, \n",
    "                input_segids = segids,\n",
    "                input_lengths = lens,\n",
    "                input_labels = y_truth\n",
    "            ) \n",
    "            \n",
    "            loss, logits = outputs\n",
    "            loss = loss.mean()\n",
    "            y_prod_cpu = logits.cpu().detach().numpy()\n",
    "            y_truth_cpu = y_truth.cpu().detach().numpy()\n",
    "            _, f1 = metric(y_truth_cpu, y_prod_cpu)\n",
    "            eval_f1.append(f1)        \n",
    "            losses.append(loss.item())                                 \n",
    "\n",
    "            val_bidx = step * batch_size\n",
    "            val_eidx = (step + 1) *batch_size\n",
    "            if val_eidx > val_count:\n",
    "                val_eidx = val_count\n",
    "            valid_preds_fold[val_bidx: val_eidx] = torch.softmax(logits, dim=-1).cpu().numpy()\n",
    "            val_true[val_bidx: val_eidx] = y_truth_cpu\n",
    "            tk0.set_postfix(loss=np.mean(losses), avg_f1=np.mean(eval_f1))\n",
    "\n",
    "        op = OptimizedF1()\n",
    "        op.fit(valid_preds_fold, val_true)\n",
    "        class_weights_fold = op.coefficients()\n",
    "        # print(val_true)\n",
    "        # print(np.argmax(valid_preds_fold, axis=1))\n",
    "        befor_f1, befor_f1 = metric(val_true, valid_preds_fold)\n",
    "        post_f1, post_f1 = metric(val_true, valid_preds_fold * class_weights_fold)\n",
    "            \n",
    "        logger.info(\"***** Report eval result *****\")\n",
    "        logger.info(\"befor_f1:{:.4f}, post_f1:{:.4f}\".format(befor_f1, post_f1))            \n",
    "\n",
    "        \n",
    "        return valid_preds_fold, class_weights_fold, befor_f1, post_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6737589-0100-4b85-af6f-87168c651165",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_model(model, test_loader, fold, test_count):\n",
    "    \n",
    "    test_preds_fold = np.zeros((test_count, 36)) ####\n",
    "    model.load_state_dict(torch.load( '{}_{}.bin'.format(bset_model_path,fold)))\n",
    "    \n",
    "    model.eval()\n",
    "    tk0 = tqdm(test_loader, total=len(test_loader))\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tk0):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            ids, segids, mask, lens, y_truth = batch\n",
    "\n",
    "            y_pred = model(\n",
    "                input_ids = ids, \n",
    "                input_mask = mask, \n",
    "                input_segids = segids,\n",
    "                input_lengths = lens,\n",
    "            )[0] \n",
    "            # print(y_pred)\n",
    "            y_pred = torch.softmax(y_pred, dim=-1).cpu().detach().numpy()\n",
    "            test_bidx= i * batch_size\n",
    "            test_eidx= (i + 1) * batch_size\n",
    "            test_preds_fold[test_bidx:test_eidx] = y_pred\n",
    "            \n",
    "    return test_preds_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3944d4a-fa76-4059-942a-bd139cdc663f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run(fold, all_data, train_idx, val_idx, batch_size, learning_rate,other_lr, test_inputs=None):\n",
    "\n",
    "\n",
    "    train_data = {k: all_data[k][train_idx] for k in train_dict}\n",
    "    valid_data = {k: all_data[k][val_idx] for k in train_dict}\n",
    "\n",
    "    train_dataset = WBDataset(train_data, tokenizer)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=train_dataset.collate, shuffle=True, num_workers=4)\n",
    "    \n",
    "#     pseudo_data = {k: np.concatenate([pseudo_dict_b[k], all_data[k][train_idx]]) for k in all_data}\n",
    "    if args.pseudo==1:\n",
    "        pseudo_data = {k: pseudo_dict[k] for k in pseudo_dict}\n",
    "        pseudo_dataset = WBDataset(pseudo_data, tokenizer)\n",
    "        pseudo_loader = DataLoader(pseudo_dataset, batch_size=batch_size, collate_fn=train_dataset.collate, shuffle=True, num_workers=4)\n",
    "    else:\n",
    "        val_count = len(val_idx)\n",
    "\n",
    "    valid_dataset = WBDataset(valid_data, tokenizer)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, collate_fn=train_dataset.collate, shuffle=True, num_workers=4)\n",
    "\n",
    "    if test_inputs is not None:\n",
    "        test_dataset =  WBDataset(test_inputs, tokenizer, test=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=test_dataset.collate, shuffle=False, num_workers=4)\n",
    "    \n",
    "    \n",
    "    device = torch.device(\"cuda\")\n",
    "    # load_pretrained loss_type\n",
    "    traindata_epoch = args.traindata_epoch\n",
    "#     BertLastTwoCls BertLastFourCls BertLastFourEmbeddingsPooler BertDynEmbeddings BertRNN\n",
    "    if args.model_type == 'BertLastTwoCls':\n",
    "        model = BertLastTwoCls(args, nezha=args.nezha, load_pretrained=args.load_pretrained, n_class=36).to(device)\n",
    "    elif args.model_type == 'BertLastFourCls':\n",
    "        model = BertLastFourCls(args, nezha=args.nezha, load_pretrained=args.load_pretrained, n_class=36).to(device)\n",
    "    elif args.model_type == 'BertDynEmbeddings':\n",
    "        model = BertDynEmbeddings(args, nezha=args.nezha, load_pretrained=args.load_pretrained, n_class=36).to(device)\n",
    "    elif args.model_type == 'BertRNN':\n",
    "        model = BertRNN(args, nezha=args.nezha, load_pretrained=args.load_pretrained, n_class=36).to(device)\n",
    "    else:\n",
    "        model = BertLastFourEmbeddingsPooler(args, nezha=args.nezha, load_pretrained=args.load_pretrained, n_class=36).to(device)\n",
    "            \n",
    "    if args.pseudo==1:\n",
    "        if traindata_epoch > 0:\n",
    "            num_train_steps = math.ceil(len(train_loader)/grad_acc * traindata_epoch + len(pseudo_loader)/grad_acc * (num_epochs-traindata_epoch))\n",
    "        else:\n",
    "            num_train_steps = math.ceil((len(pseudo_loader))/grad_acc * num_epochs)\n",
    "    else:\n",
    "        num_train_steps = math.ceil(len(train_loader)/grad_acc * num_epochs)\n",
    "    \n",
    "    logger.info('num_train_steps:{}'.format(num_train_steps))\n",
    "    \n",
    "    weight_decay = 0.0001\n",
    "    head_decay = 0.\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    if args.layerwise_learning_rate_decay == 1:\n",
    "        encoder_parameters = [(n, p) for n, p in model.named_parameters() if \"pretrained_model\"  in n]\n",
    "        decoder_parameters =  [(n, p) for n, p in model.named_parameters() if \"pretrained_model\" not in n]\n",
    "        \n",
    "        optimizer_parameters = [\n",
    "            {\"params\": [p for n, p in encoder_parameters if not any(nd in n for nd in no_decay)], \"weight_decay\": weight_decay, 'lr': learning_rate},\n",
    "            {\"params\": [p for n, p in encoder_parameters if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0, 'lr': learning_rate},\n",
    "            \n",
    "            \n",
    "            {\"params\": [p for n, p in decoder_parameters ], \"weight_decay\": head_decay, 'lr': other_lr},\n",
    "\n",
    "        ]\n",
    "    else:\n",
    "        encoder_parameters = [(n, p) for n, p in model.named_parameters() if \"pretrained_model\" in n and \"pooler\" not in n]\n",
    "        decoder_parameters = [(n, p) for n, p in model.named_parameters() if \"pretrained_model\" not in n]\n",
    "        pooler_parameters = [(n, p) for n, p in model.named_parameters() if  \"pooler\" in n]   + pooler_parameters \n",
    "        \n",
    "        optimizer_parameters = [\n",
    "            {\"params\": [p for n, p in decoder_parameters], \"weight_decay\": head_decay, 'lr': other_lr},\n",
    "        ]\n",
    "        \n",
    "        lr = learning_rate\n",
    "        layers = [model.pretrained_model.embeddings] + list(model.pretrained_model.encoder.layer)\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            lr *= args.layerwise_learning_rate_decay\n",
    "            optimizer_parameters += [\n",
    "                {\"params\": [p for n, p in layer.named_parameters() if not any(nd in n for nd in no_decay)], \"weight_decay\": weight_decay, \"lr\": lr,},\n",
    "                {\"params\": [p for n, p in layer.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0, \"lr\": lr,},\n",
    "            ]\n",
    "\n",
    "        \n",
    "    # get_cosine_schedule_with_warmup get_linear_schedule_with_warmup\n",
    "    optimizer = AdamW(optimizer_parameters)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,  \n",
    "        num_warmup_steps=int(num_train_steps * 0.0), \n",
    "        num_training_steps=num_train_steps\n",
    "    )\n",
    "\n",
    "    best_f1 = -99999\n",
    "    early_stop_count = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        \n",
    "        if args.pseudo==1 and epoch >= traindata_epoch:\n",
    "            train_loader = pseudo_loader\n",
    "            \n",
    "        model.train()\n",
    "        train_model(train_loader, model, optimizer, device, scheduler=scheduler)\n",
    "        \n",
    "        if args.pseudo==1:\n",
    "#             if epoch > num_epochs-2: break\n",
    "#             if epoch >= 3:\n",
    "#                 torch.save(model.state_dict(), '{}_{}.bin'.format(bset_model_path,fold))\n",
    "                \n",
    "            if epoch >= num_epochs - 2:\n",
    "                torch.save(model.state_dict(), '{}_{}_epoch{}.bin'.format(bset_model_path, fold, epoch))\n",
    "                \n",
    "        else:\n",
    "            if epoch >= 2:\n",
    "                valid_preds_fold, class_weights_fold, befor_f1, post_f1 = eval_model(valid_loader, model, val_count, device)\n",
    "\n",
    "                if befor_f1 > best_f1:\n",
    "                    early_stop_count = 0\n",
    "                    best_f1 = befor_f1\n",
    "                    torch.save(model.state_dict(), '{}_{}.bin'.format(bset_model_path,fold))\n",
    "                else:\n",
    "                    early_stop_count += 1\n",
    "                if early_stop_count > patience:\n",
    "                    logger.info(\"Early stopping\")\n",
    "                    torch.cuda.empty_cache()\n",
    "                    torch.cuda.empty_cache() \n",
    "                    break\n",
    "\n",
    "\n",
    "    if test_inputs is not None:\n",
    "        test_preds_fold = test_model(model, test_loader, fold, test_count=len(test_dict['input_ids']))\n",
    "\n",
    "        return test_preds_fold, class_weights_fold, best_f1\n",
    "    else:\n",
    "        return class_weights_fold, best_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90074292-8c1f-4c0a-93d5-0e246ad4ebae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 958/958 [00:02<00:00, 326.08it/s]\n",
      "100%|██████████| 20890/20890 [01:03<00:00, 327.56it/s]\n",
      "20890it [01:03, 326.80it/s]\n"
     ]
    }
   ],
   "source": [
    "# 数据集处理\n",
    "train_dict = create_traindata(train_data, tokenizer)\n",
    "pseudo_dict = create_traindata(pseudo_data_b, tokenizer)\n",
    "test_dict = create_testdata(test_b, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17855de-eabc-47b3-8b50-bedbf2ca193e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-11-12 20:07:39,122][INFO] ## ==========learning_rate:3e-05,batch_size:32=============\n",
      "[2022-11-12 20:07:39,125][INFO] ## ==========fold:0=============\n",
      "[2022-11-12 20:07:44,742][INFO] ## num_train_steps:120\n",
      "[2022-11-12 20:07:44,755][INFO] ## training.............\n",
      "100%|██████████| 24/24 [00:12<00:00,  1.98it/s, avg_f1=0.172, loss=3.21]\n",
      "[2022-11-12 20:07:56,897][INFO] ## training.............\n",
      "100%|██████████| 24/24 [00:11<00:00,  2.17it/s, avg_f1=0.354, loss=2.27]\n",
      "[2022-11-12 20:08:07,950][INFO] ## training.............\n",
      "100%|██████████| 24/24 [00:10<00:00,  2.22it/s, avg_f1=0.564, loss=1.49]\n",
      "[2022-11-12 20:08:18,744][INFO] ## evaling.............\n",
      "100%|██████████| 6/6 [00:01<00:00,  3.49it/s, avg_f1=0.402, loss=1.53]\n",
      "[2022-11-12 20:08:20,784][INFO] ## ***** Report eval result *****\n",
      "[2022-11-12 20:08:20,786][INFO] ## befor_f1:0.3664, post_f1:0.3760\n",
      "[2022-11-12 20:08:21,325][INFO] ## training.............\n",
      "100%|██████████| 24/24 [00:10<00:00,  2.20it/s, avg_f1=0.725, loss=1.02]\n",
      "[2022-11-12 20:08:32,258][INFO] ## evaling.............\n",
      "100%|██████████| 6/6 [00:01<00:00,  3.13it/s, avg_f1=0.478, loss=1.38]\n",
      "[2022-11-12 20:08:34,511][INFO] ## ***** Report eval result *****\n",
      "[2022-11-12 20:08:34,512][INFO] ## befor_f1:0.4691, post_f1:0.4785\n",
      "[2022-11-12 20:08:35,191][INFO] ## training.............\n",
      "100%|██████████| 24/24 [00:10<00:00,  2.21it/s, avg_f1=0.804, loss=0.78] \n",
      "[2022-11-12 20:08:46,042][INFO] ## evaling.............\n",
      "100%|██████████| 6/6 [00:01<00:00,  3.35it/s, avg_f1=0.496, loss=1.3] \n",
      "[2022-11-12 20:08:48,146][INFO] ## ***** Report eval result *****\n",
      "[2022-11-12 20:08:48,147][INFO] ## befor_f1:0.4918, post_f1:0.4953\n",
      " 11%|█         | 69/653 [00:11<01:34,  6.16it/s]"
     ]
    }
   ],
   "source": [
    "# 参数设置，多个则是类似网格搜索如下所示\n",
    "learning_rates = [3e-5]\n",
    "num_epochses = [5]\n",
    "batch_sizes = [32]\n",
    "other_lrs = [1e-4]\n",
    "patience = 2\n",
    "grad_acc = 1\n",
    "n_splits = 5\n",
    "device = torch.device('cuda')\n",
    "output_unique = 36\n",
    "search_model = 'layerwise1-wd0.0001-warmp0.0'\n",
    "seed_value = 666\n",
    "\n",
    "# 每一个fold随机数，不同可以使得模型初始化有所差异，提升融合效果\n",
    "seed_values = list(range(seed_value, seed_value+n_splits))\n",
    "# seed_values = [412, 927, 1227, 1992, 2020, 2010, 1990, 1966, 2022, 6666]\n",
    "test_preds_folds = []\n",
    "thresholds = []\n",
    "seed_all(seed_value)\n",
    "\n",
    "all_best_f1 = []\n",
    "for learning_rate, other_lr, batch_size, num_epochs in zip(learning_rates, other_lrs, batch_sizes, num_epochses):\n",
    "    \n",
    "    model_name = 'nezha_base-pseudo_labels_b-{}-{}-seed{}-gkf{}-{}-n_splits{}-grad_acc{}-num_epochs{}'.format(args.model_type, args.loss_type, seed_value, seed_value,\n",
    "                                                                                                              search_model,n_splits, grad_acc, num_epochs)\n",
    "    logger.info('==========learning_rate:{},batch_size:{}============='.format(learning_rate, batch_size))\n",
    "    current_config_f1 = 0\n",
    "    kf = StratifiedKFold(n_splits=n_splits ,shuffle=True, random_state=seed_value).split(\n",
    "        X=train_dict['labels'], y=train_dict['labels'])\n",
    "\n",
    "    bset_model_path = \"../../user_data/best_models/{}-{}-{}_fold\".format(model_name, learning_rate, batch_size)\n",
    "    n_count = 0\n",
    "\n",
    "    class_weights = []\n",
    "    split_idxes = []\n",
    "    for fold,(train_index,valid_index) in enumerate(kf):\n",
    "        logger.info('==========fold:{}============='.format(fold))\n",
    "        seed_all(seed_values[fold])\n",
    "        split_idxes.append((train_index,valid_index))\n",
    "        n = 0\n",
    "        all_data = train_dict\n",
    "        test_preds_fold, new_thresholds, best_f1= run(fold, all_data, train_index, valid_index, batch_size, learning_rate, other_lr, test_inputs=test_dict)\n",
    "\n",
    "        \n",
    "        all_best_f1.append(best_f1)\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.empty_cache()\n",
    "        thresholds.append(new_thresholds)\n",
    "        test_preds_folds.append(test_preds_fold)\n",
    "        \n",
    "    final_results = {'test_preds_folds':test_preds_folds, 'thresholds':thresholds, 'data_idx': test_dict['data_idx'],\n",
    "                    'all_best_f1': all_best_f1, 'seed_values': seed_values, 'seed_value': seed_value, 'split_idxes': split_idxes}\n",
    "\n",
    "    with open(\"../../user_data/results/{}-{}-{}.pk\".format(model_name, learning_rate, batch_size),'wb')as f:\n",
    "        joblib.dump(final_results,f)\n",
    "    \n",
    "\n",
    "    logger.info('learning_rate:{},batch_size:{}'.format(learning_rate, batch_size))\n",
    "    logger.info('=========================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1f3c96-3be7-484c-aaa1-7a59396fcdaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
