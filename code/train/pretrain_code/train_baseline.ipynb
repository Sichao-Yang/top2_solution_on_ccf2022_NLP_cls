{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10437,
     "status": "ok",
     "timestamp": 1664190737466,
     "user": {
      "displayName": "guang han",
      "userId": "09096391548724575519"
     },
     "user_tz": -480
    },
    "id": "rSkRdkIVilFw",
    "outputId": "d30c5e53-b6b9-41c6-edb2-9b4b874e6a5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.22.1-py3-none-any.whl (4.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.9 MB 2.1 MB/s \n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.6 MB 44.0 MB/s \n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Collecting huggingface-hub<1.0,>=0.9.0\n",
      "  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n",
      "\u001b[K     |████████████████████████████████| 120 kB 63.5 MB/s \n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.9.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.9.1 tokenizers-0.12.1 transformers-4.22.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1664190737467,
     "user": {
      "displayName": "guang han",
      "userId": "09096391548724575519"
     },
     "user_tz": -480
    },
    "id": "RKDWs-CEPAgD",
    "outputId": "4c1f4118-3c67-4369-e171-ca9344ef7f27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Sep 26 11:12:16 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   35C    P0    24W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wvtuRISuhk9b"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "referenced_widgets": [
      "a87377e8878f4d03a18a1a2b5cfe90e8"
     ]
    },
    "id": "PQqlhbkvhbJ3",
    "outputId": "6b3744fb-217e-45d1-fb24-a743930654e6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving 0 files to the new cache system\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a87377e8878f4d03a18a1a2b5cfe90e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import math\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup, DataCollatorWithPadding\n",
    "from transformers import BertTokenizer,AutoModel,AdamW,AutoConfig, AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import json\n",
    "import gc\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "tzQrEEythbKJ"
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    input_path = '/content/drive/MyDrive/ccf/'\n",
    "    model_path = 'hfl/chinese-roberta-wwm-ext-large' #  nghuyong/ernie-2.0-large-en studio-ousia/luke-large\n",
    "    scheduler = 'cosine'  # ['linear', 'cosine']\n",
    "    batch_scheduler = True\n",
    "    num_cycles = 0.5  # 1.5\n",
    "    num_warmup_steps = 0\n",
    "    max_input_length = 1024\n",
    "    epochs = 100  # 5\n",
    "    encoder_lr = 20e-6\n",
    "    decoder_lr = 20e-6\n",
    "    min_lr = 0.5e-6\n",
    "    eps = 1e-6\n",
    "    betas = (0.9, 0.999)\n",
    "    weight_decay = 0\n",
    "    num_fold = 5\n",
    "    batch_size = 4\n",
    "    seed = 1006\n",
    "    OUTPUT_DIR = '/content/drive/MyDrive/ccf/'\n",
    "    num_workers = 2\n",
    "    device='cuda'\n",
    "    print_freq = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "mykwcZyypr1O"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything(CFG.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MrBpJ61KhbJ7"
   },
   "source": [
    "## 1. Read Data & EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "69UrMW56hbJ9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_jsonfile(file_name):\n",
    "    data = []\n",
    "    with open(file_name) as f:\n",
    "        for i in f.readlines():\n",
    "            data.append(json.loads(i))\n",
    "    return data\n",
    "\n",
    "train = pd.DataFrame(read_jsonfile(CFG.input_path + \"/train.json\"))\n",
    "#add = pd.read_csv('/content/drive/MyDrive/ccf/additional_train.csv')\n",
    "#train = pd.concat([train, add])\n",
    "test = pd.DataFrame(read_jsonfile(CFG.input_path + \"/testA.json\"))\n",
    "train['label_id'] = train['label_id'].apply(lambda x :int(x))\n",
    "train.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1664091402343,
     "user": {
      "displayName": "guang han",
      "userId": "09096391548724575519"
     },
     "user_tz": -480
    },
    "id": "jVY6fFsZhbJ_",
    "outputId": "7103a313-c5e6-46ca-9faf-44430bfab8f2",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-b895e115-7917-4351-9786-dbe07efb8ad2\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>assignee</th>\n",
       "      <th>abstract</th>\n",
       "      <th>label_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>538f267d2e6fba48b1286fb7f1499fe7</td>\n",
       "      <td>一种信号的发送方法及基站、用户设备</td>\n",
       "      <td>华为技术有限公司</td>\n",
       "      <td>一种信号的发送方法及基站、用户设备。在一个子帧中为多个用户设备配置的参考信号的符号和数据的符...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>635a7d4b6358b6ff24a324bb871505db</td>\n",
       "      <td>一种5G通讯电缆故障监控系统</td>\n",
       "      <td>中铁二十二局集团电气化工程有限公司</td>\n",
       "      <td>本发明公开了一种5G通讯电缆故障监控系统，包括信号采样模块、补偿反馈模块，所述信号采样模块对...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>aaf98d6bfe1932cf1a262812ca59d1ba</td>\n",
       "      <td>一种测试方法及电子设备</td>\n",
       "      <td>腾讯科技(北京)有限公司</td>\n",
       "      <td>本发明提供了一种测试方法及电子设备，该方法包括：基于选取的测试任务确定目标测试用例，根据所述...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ad49c9ba6412452d9b25071af702f4fb</td>\n",
       "      <td>天线方位角调节装置</td>\n",
       "      <td>武汉虹信通信技术有限责任公司</td>\n",
       "      <td>一种天线方位角调节装置，包括对向的两个8字形支架(101)、动力输入电机(102)、主动齿轮...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>ffa2d7617b3eac3a1d7df01e5bb515a2</td>\n",
       "      <td>光纤老化预测方法及装置</td>\n",
       "      <td>新华三大数据技术有限公司</td>\n",
       "      <td>本申请提供一种光纤老化预测方法及装置，所述方法包括：获取待测光纤模块可接收的光信号的告警阈值...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>953</td>\n",
       "      <td>6af8c4c55c93ee38b8912db4576b3cfc</td>\n",
       "      <td>一种信息处理方法及装置</td>\n",
       "      <td>腾讯科技(深圳)有限公司</td>\n",
       "      <td>本发明公开了一种信息处理方法，所述方法包括：第一进程获取来自多个查询请求端的多个数据请求，所...</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>954</td>\n",
       "      <td>bc94427b0ae4c5a734ef7d32d6a1b9ea</td>\n",
       "      <td>一种适用于安防的广告机</td>\n",
       "      <td>靖江天元爱尔瑞电子科技有限公司</td>\n",
       "      <td>本实用新型公开了一种适用于安防的广告机，包括支撑架，支撑架的上端设置有显示屏，显示屏与壳体配...</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955</th>\n",
       "      <td>955</td>\n",
       "      <td>2b1d9b24b86b2e49f842bd2c93cb865c</td>\n",
       "      <td>一种广告投放控制方法及装置</td>\n",
       "      <td>阿里巴巴(中国)有限公司</td>\n",
       "      <td>本发明公开了一种广告投放控制方法及装置，以解决现有技术中基于地域定向的广告投放方式准确度较低...</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>956</th>\n",
       "      <td>956</td>\n",
       "      <td>674baad2739c09bc9cc759322a0085c7</td>\n",
       "      <td>一种广告数据推荐方法和系统</td>\n",
       "      <td>北京奇艺世纪科技有限公司</td>\n",
       "      <td>本发明公开了一种广告数据推荐方法和系统。所述方法包括：接收到用户对目标广告数据的浏览请求后，...</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>957</th>\n",
       "      <td>957</td>\n",
       "      <td>94b8d5a69a04bc931bb2d65ea95fc9b2</td>\n",
       "      <td>一种基于大数据的广告推送系统</td>\n",
       "      <td>浙江华坤道威数据科技有限公司</td>\n",
       "      <td>本发明公开了一种基于大数据的广告推送系统，包括用户信息采集模块、大数据采集模块、数据处理模块...</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>958 rows × 6 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b895e115-7917-4351-9786-dbe07efb8ad2')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-b895e115-7917-4351-9786-dbe07efb8ad2 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-b895e115-7917-4351-9786-dbe07efb8ad2');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "     index                                id              title  \\\n",
       "0        0  538f267d2e6fba48b1286fb7f1499fe7  一种信号的发送方法及基站、用户设备   \n",
       "1        1  635a7d4b6358b6ff24a324bb871505db     一种5G通讯电缆故障监控系统   \n",
       "2        2  aaf98d6bfe1932cf1a262812ca59d1ba        一种测试方法及电子设备   \n",
       "3        3  ad49c9ba6412452d9b25071af702f4fb          天线方位角调节装置   \n",
       "4        4  ffa2d7617b3eac3a1d7df01e5bb515a2        光纤老化预测方法及装置   \n",
       "..     ...                               ...                ...   \n",
       "953    953  6af8c4c55c93ee38b8912db4576b3cfc        一种信息处理方法及装置   \n",
       "954    954  bc94427b0ae4c5a734ef7d32d6a1b9ea        一种适用于安防的广告机   \n",
       "955    955  2b1d9b24b86b2e49f842bd2c93cb865c      一种广告投放控制方法及装置   \n",
       "956    956  674baad2739c09bc9cc759322a0085c7      一种广告数据推荐方法和系统   \n",
       "957    957  94b8d5a69a04bc931bb2d65ea95fc9b2     一种基于大数据的广告推送系统   \n",
       "\n",
       "              assignee                                           abstract  \\\n",
       "0             华为技术有限公司  一种信号的发送方法及基站、用户设备。在一个子帧中为多个用户设备配置的参考信号的符号和数据的符...   \n",
       "1    中铁二十二局集团电气化工程有限公司  本发明公开了一种5G通讯电缆故障监控系统，包括信号采样模块、补偿反馈模块，所述信号采样模块对...   \n",
       "2         腾讯科技(北京)有限公司  本发明提供了一种测试方法及电子设备，该方法包括：基于选取的测试任务确定目标测试用例，根据所述...   \n",
       "3       武汉虹信通信技术有限责任公司  一种天线方位角调节装置，包括对向的两个8字形支架(101)、动力输入电机(102)、主动齿轮...   \n",
       "4         新华三大数据技术有限公司  本申请提供一种光纤老化预测方法及装置，所述方法包括：获取待测光纤模块可接收的光信号的告警阈值...   \n",
       "..                 ...                                                ...   \n",
       "953       腾讯科技(深圳)有限公司  本发明公开了一种信息处理方法，所述方法包括：第一进程获取来自多个查询请求端的多个数据请求，所...   \n",
       "954    靖江天元爱尔瑞电子科技有限公司  本实用新型公开了一种适用于安防的广告机，包括支撑架，支撑架的上端设置有显示屏，显示屏与壳体配...   \n",
       "955       阿里巴巴(中国)有限公司  本发明公开了一种广告投放控制方法及装置，以解决现有技术中基于地域定向的广告投放方式准确度较低...   \n",
       "956       北京奇艺世纪科技有限公司  本发明公开了一种广告数据推荐方法和系统。所述方法包括：接收到用户对目标广告数据的浏览请求后，...   \n",
       "957     浙江华坤道威数据科技有限公司  本发明公开了一种基于大数据的广告推送系统，包括用户信息采集模块、大数据采集模块、数据处理模块...   \n",
       "\n",
       "     label_id  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "..        ...  \n",
       "953        35  \n",
       "954        35  \n",
       "955        35  \n",
       "956        35  \n",
       "957        35  \n",
       "\n",
       "[958 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.to_csv('train.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kcHSw4IShbJ_"
   },
   "source": [
    "### 1.1 Label Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1664091402344,
     "user": {
      "displayName": "guang han",
      "userId": "09096391548724575519"
     },
     "user_tz": -480
    },
    "id": "FBTH1j2ghbKA",
    "outputId": "e4010557-9ba5-47bb-f2ca-9a1afc7e0dda",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f23a9b53210>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train.label_id.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1664091402344,
     "user": {
      "displayName": "guang han",
      "userId": "09096391548724575519"
     },
     "user_tz": -480
    },
    "id": "oaF42PzgdTIi",
    "outputId": "fadb12f4-77d2-4bfb-bca4-4d82cf3886d0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    958.000000\n",
       "mean      11.140919\n",
       "std        9.325865\n",
       "min        0.000000\n",
       "25%        3.000000\n",
       "50%        8.000000\n",
       "75%       17.750000\n",
       "max       35.000000\n",
       "Name: label_id, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.label_id.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XOEhmIyyhbKB"
   },
   "source": [
    "### 1.2 Input Length Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "executionInfo": {
     "elapsed": 461,
     "status": "ok",
     "timestamp": 1664091402798,
     "user": {
      "displayName": "guang han",
      "userId": "09096391548724575519"
     },
     "user_tz": -480
    },
    "id": "xKobJHMZhbKB",
    "outputId": "56622d89-47f6-4062-c9cd-a3d3126ee533"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f23a8dbef90>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train['title_len'] = train['title'].apply(lambda x: len(x))\n",
    "train['title_len'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1664091402799,
     "user": {
      "displayName": "guang han",
      "userId": "09096391548724575519"
     },
     "user_tz": -480
    },
    "id": "jW4wfqaEeXDA",
    "outputId": "d2a0cf5b-ed82-4682-f0c5-b5b283a0bfe1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    958.000000\n",
       "mean      16.423800\n",
       "std        6.028142\n",
       "min        2.000000\n",
       "25%       12.000000\n",
       "50%       16.000000\n",
       "75%       21.000000\n",
       "max       37.000000\n",
       "Name: title_len, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['title_len'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1664091402800,
     "user": {
      "displayName": "guang han",
      "userId": "09096391548724575519"
     },
     "user_tz": -480
    },
    "id": "seYqTDIPhbKC",
    "outputId": "b9fc456a-0038-4305-9762-3b8d14d7f077"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f23a9966810>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train['assignee_len'] = train['assignee'].apply(lambda x: len(x))\n",
    "train['assignee_len'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1664091402801,
     "user": {
      "displayName": "guang han",
      "userId": "09096391548724575519"
     },
     "user_tz": -480
    },
    "id": "AsvY1v-zezA3",
    "outputId": "0a7f8b34-fa7b-407e-f116-6e5283e3b101"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    958.000000\n",
       "mean      10.494781\n",
       "std        4.019216\n",
       "min        2.000000\n",
       "25%        8.000000\n",
       "50%       11.000000\n",
       "75%       13.000000\n",
       "max       31.000000\n",
       "Name: assignee_len, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['assignee_len'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "executionInfo": {
     "elapsed": 502,
     "status": "ok",
     "timestamp": 1664091403295,
     "user": {
      "displayName": "guang han",
      "userId": "09096391548724575519"
     },
     "user_tz": -480
    },
    "id": "wVL5eQa0hbKD",
    "outputId": "fc7b062e-7934-4660-87ec-c7b1f8cc52a1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f23a9744490>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train['abstract_len'] = train['abstract'].apply(lambda x: len(x))\n",
    "train['abstract_len'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1664091403296,
     "user": {
      "displayName": "guang han",
      "userId": "09096391548724575519"
     },
     "user_tz": -480
    },
    "id": "DXAxxqp8fLfH",
    "outputId": "c300f6f1-85bf-4fc2-9ef7-b33c710254c9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    958.000000\n",
       "mean     254.092902\n",
       "std       52.885760\n",
       "min       59.000000\n",
       "25%      228.000000\n",
       "50%      271.000000\n",
       "75%      291.000000\n",
       "max      639.000000\n",
       "Name: abstract_len, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['abstract_len'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "executionInfo": {
     "elapsed": 541,
     "status": "ok",
     "timestamp": 1664091403833,
     "user": {
      "displayName": "guang han",
      "userId": "09096391548724575519"
     },
     "user_tz": -480
    },
    "id": "Ytd7IkCAhbKE",
    "outputId": "0eaf255d-40d2-466c-8fb3-aeb47d9fceec",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f23a95a0e10>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train['input_text'] = train['title'] + train['assignee'] + train['abstract']\n",
    "train['input_text_len'] = train['input_text'].apply(lambda x: len(x))\n",
    "train['input_text_len'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w8lG3vk5hbKE"
   },
   "source": [
    "结论：\n",
    "训练数据较小仅有900条数据，test数据〉2w条，典型小样本训练场景\n",
    "输入文本长度普遍在350个字以下，超过400的词很少\n",
    "label的分布也不是很均衡，可能存在较难样本学习不充分的情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1664091403833,
     "user": {
      "displayName": "guang han",
      "userId": "09096391548724575519"
     },
     "user_tz": -480
    },
    "id": "GFl40iz6hbKF",
    "outputId": "8db5b1ed-83e4-4f36-f4f4-0d0ea3675937",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:680: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  UserWarning,\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5)\n",
    "for fold, (_, val_) in enumerate(skf.split(X=train, y=train.label_id, groups=train.label_id)):\n",
    "    train.loc[val_, \"fold\"] = int(fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 815
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1664091403834,
     "user": {
      "displayName": "guang han",
      "userId": "09096391548724575519"
     },
     "user_tz": -480
    },
    "id": "EFZZ6X7VhbKF",
    "outputId": "416d634c-5b32-46c2-afea-6609106df853",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-e3ed15db-646d-48d2-ace6-a017f7a181c9\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>assignee</th>\n",
       "      <th>abstract</th>\n",
       "      <th>label_id</th>\n",
       "      <th>title_len</th>\n",
       "      <th>assignee_len</th>\n",
       "      <th>abstract_len</th>\n",
       "      <th>input_text</th>\n",
       "      <th>input_text_len</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>538f267d2e6fba48b1286fb7f1499fe7</td>\n",
       "      <td>一种信号的发送方法及基站、用户设备</td>\n",
       "      <td>华为技术有限公司</td>\n",
       "      <td>一种信号的发送方法及基站、用户设备。在一个子帧中为多个用户设备配置的参考信号的符号和数据的符...</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>265</td>\n",
       "      <td>一种信号的发送方法及基站、用户设备华为技术有限公司一种信号的发送方法及基站、用户设备。在一个...</td>\n",
       "      <td>290</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>635a7d4b6358b6ff24a324bb871505db</td>\n",
       "      <td>一种5G通讯电缆故障监控系统</td>\n",
       "      <td>中铁二十二局集团电气化工程有限公司</td>\n",
       "      <td>本发明公开了一种5G通讯电缆故障监控系统，包括信号采样模块、补偿反馈模块，所述信号采样模块对...</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>17</td>\n",
       "      <td>315</td>\n",
       "      <td>一种5G通讯电缆故障监控系统中铁二十二局集团电气化工程有限公司本发明公开了一种5G通讯电缆故...</td>\n",
       "      <td>346</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>aaf98d6bfe1932cf1a262812ca59d1ba</td>\n",
       "      <td>一种测试方法及电子设备</td>\n",
       "      <td>腾讯科技(北京)有限公司</td>\n",
       "      <td>本发明提供了一种测试方法及电子设备，该方法包括：基于选取的测试任务确定目标测试用例，根据所述...</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>180</td>\n",
       "      <td>一种测试方法及电子设备腾讯科技(北京)有限公司本发明提供了一种测试方法及电子设备，该方法包括...</td>\n",
       "      <td>203</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ad49c9ba6412452d9b25071af702f4fb</td>\n",
       "      <td>天线方位角调节装置</td>\n",
       "      <td>武汉虹信通信技术有限责任公司</td>\n",
       "      <td>一种天线方位角调节装置，包括对向的两个8字形支架(101)、动力输入电机(102)、主动齿轮...</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>300</td>\n",
       "      <td>天线方位角调节装置武汉虹信通信技术有限责任公司一种天线方位角调节装置，包括对向的两个8字形支...</td>\n",
       "      <td>323</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>ffa2d7617b3eac3a1d7df01e5bb515a2</td>\n",
       "      <td>光纤老化预测方法及装置</td>\n",
       "      <td>新华三大数据技术有限公司</td>\n",
       "      <td>本申请提供一种光纤老化预测方法及装置，所述方法包括：获取待测光纤模块可接收的光信号的告警阈值...</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>236</td>\n",
       "      <td>光纤老化预测方法及装置新华三大数据技术有限公司本申请提供一种光纤老化预测方法及装置，所述方法...</td>\n",
       "      <td>259</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>953</td>\n",
       "      <td>6af8c4c55c93ee38b8912db4576b3cfc</td>\n",
       "      <td>一种信息处理方法及装置</td>\n",
       "      <td>腾讯科技(深圳)有限公司</td>\n",
       "      <td>本发明公开了一种信息处理方法，所述方法包括：第一进程获取来自多个查询请求端的多个数据请求，所...</td>\n",
       "      <td>35</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>246</td>\n",
       "      <td>一种信息处理方法及装置腾讯科技(深圳)有限公司本发明公开了一种信息处理方法，所述方法包括：第...</td>\n",
       "      <td>269</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>954</td>\n",
       "      <td>bc94427b0ae4c5a734ef7d32d6a1b9ea</td>\n",
       "      <td>一种适用于安防的广告机</td>\n",
       "      <td>靖江天元爱尔瑞电子科技有限公司</td>\n",
       "      <td>本实用新型公开了一种适用于安防的广告机，包括支撑架，支撑架的上端设置有显示屏，显示屏与壳体配...</td>\n",
       "      <td>35</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>292</td>\n",
       "      <td>一种适用于安防的广告机靖江天元爱尔瑞电子科技有限公司本实用新型公开了一种适用于安防的广告机，...</td>\n",
       "      <td>318</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955</th>\n",
       "      <td>955</td>\n",
       "      <td>2b1d9b24b86b2e49f842bd2c93cb865c</td>\n",
       "      <td>一种广告投放控制方法及装置</td>\n",
       "      <td>阿里巴巴(中国)有限公司</td>\n",
       "      <td>本发明公开了一种广告投放控制方法及装置，以解决现有技术中基于地域定向的广告投放方式准确度较低...</td>\n",
       "      <td>35</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>216</td>\n",
       "      <td>一种广告投放控制方法及装置阿里巴巴(中国)有限公司本发明公开了一种广告投放控制方法及装置，以...</td>\n",
       "      <td>241</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>956</th>\n",
       "      <td>956</td>\n",
       "      <td>674baad2739c09bc9cc759322a0085c7</td>\n",
       "      <td>一种广告数据推荐方法和系统</td>\n",
       "      <td>北京奇艺世纪科技有限公司</td>\n",
       "      <td>本发明公开了一种广告数据推荐方法和系统。所述方法包括：接收到用户对目标广告数据的浏览请求后，...</td>\n",
       "      <td>35</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>233</td>\n",
       "      <td>一种广告数据推荐方法和系统北京奇艺世纪科技有限公司本发明公开了一种广告数据推荐方法和系统。所...</td>\n",
       "      <td>258</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>957</th>\n",
       "      <td>957</td>\n",
       "      <td>94b8d5a69a04bc931bb2d65ea95fc9b2</td>\n",
       "      <td>一种基于大数据的广告推送系统</td>\n",
       "      <td>浙江华坤道威数据科技有限公司</td>\n",
       "      <td>本发明公开了一种基于大数据的广告推送系统，包括用户信息采集模块、大数据采集模块、数据处理模块...</td>\n",
       "      <td>35</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>291</td>\n",
       "      <td>一种基于大数据的广告推送系统浙江华坤道威数据科技有限公司本发明公开了一种基于大数据的广告推送...</td>\n",
       "      <td>319</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>958 rows × 12 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e3ed15db-646d-48d2-ace6-a017f7a181c9')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-e3ed15db-646d-48d2-ace6-a017f7a181c9 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-e3ed15db-646d-48d2-ace6-a017f7a181c9');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "     index                                id              title  \\\n",
       "0        0  538f267d2e6fba48b1286fb7f1499fe7  一种信号的发送方法及基站、用户设备   \n",
       "1        1  635a7d4b6358b6ff24a324bb871505db     一种5G通讯电缆故障监控系统   \n",
       "2        2  aaf98d6bfe1932cf1a262812ca59d1ba        一种测试方法及电子设备   \n",
       "3        3  ad49c9ba6412452d9b25071af702f4fb          天线方位角调节装置   \n",
       "4        4  ffa2d7617b3eac3a1d7df01e5bb515a2        光纤老化预测方法及装置   \n",
       "..     ...                               ...                ...   \n",
       "953    953  6af8c4c55c93ee38b8912db4576b3cfc        一种信息处理方法及装置   \n",
       "954    954  bc94427b0ae4c5a734ef7d32d6a1b9ea        一种适用于安防的广告机   \n",
       "955    955  2b1d9b24b86b2e49f842bd2c93cb865c      一种广告投放控制方法及装置   \n",
       "956    956  674baad2739c09bc9cc759322a0085c7      一种广告数据推荐方法和系统   \n",
       "957    957  94b8d5a69a04bc931bb2d65ea95fc9b2     一种基于大数据的广告推送系统   \n",
       "\n",
       "              assignee                                           abstract  \\\n",
       "0             华为技术有限公司  一种信号的发送方法及基站、用户设备。在一个子帧中为多个用户设备配置的参考信号的符号和数据的符...   \n",
       "1    中铁二十二局集团电气化工程有限公司  本发明公开了一种5G通讯电缆故障监控系统，包括信号采样模块、补偿反馈模块，所述信号采样模块对...   \n",
       "2         腾讯科技(北京)有限公司  本发明提供了一种测试方法及电子设备，该方法包括：基于选取的测试任务确定目标测试用例，根据所述...   \n",
       "3       武汉虹信通信技术有限责任公司  一种天线方位角调节装置，包括对向的两个8字形支架(101)、动力输入电机(102)、主动齿轮...   \n",
       "4         新华三大数据技术有限公司  本申请提供一种光纤老化预测方法及装置，所述方法包括：获取待测光纤模块可接收的光信号的告警阈值...   \n",
       "..                 ...                                                ...   \n",
       "953       腾讯科技(深圳)有限公司  本发明公开了一种信息处理方法，所述方法包括：第一进程获取来自多个查询请求端的多个数据请求，所...   \n",
       "954    靖江天元爱尔瑞电子科技有限公司  本实用新型公开了一种适用于安防的广告机，包括支撑架，支撑架的上端设置有显示屏，显示屏与壳体配...   \n",
       "955       阿里巴巴(中国)有限公司  本发明公开了一种广告投放控制方法及装置，以解决现有技术中基于地域定向的广告投放方式准确度较低...   \n",
       "956       北京奇艺世纪科技有限公司  本发明公开了一种广告数据推荐方法和系统。所述方法包括：接收到用户对目标广告数据的浏览请求后，...   \n",
       "957     浙江华坤道威数据科技有限公司  本发明公开了一种基于大数据的广告推送系统，包括用户信息采集模块、大数据采集模块、数据处理模块...   \n",
       "\n",
       "     label_id  title_len  assignee_len  abstract_len  \\\n",
       "0           0         17             8           265   \n",
       "1           0         14            17           315   \n",
       "2           0         11            12           180   \n",
       "3           0          9            14           300   \n",
       "4           0         11            12           236   \n",
       "..        ...        ...           ...           ...   \n",
       "953        35         11            12           246   \n",
       "954        35         11            15           292   \n",
       "955        35         13            12           216   \n",
       "956        35         13            12           233   \n",
       "957        35         14            14           291   \n",
       "\n",
       "                                            input_text  input_text_len  fold  \n",
       "0    一种信号的发送方法及基站、用户设备华为技术有限公司一种信号的发送方法及基站、用户设备。在一个...             290   0.0  \n",
       "1    一种5G通讯电缆故障监控系统中铁二十二局集团电气化工程有限公司本发明公开了一种5G通讯电缆故...             346   0.0  \n",
       "2    一种测试方法及电子设备腾讯科技(北京)有限公司本发明提供了一种测试方法及电子设备，该方法包括...             203   0.0  \n",
       "3    天线方位角调节装置武汉虹信通信技术有限责任公司一种天线方位角调节装置，包括对向的两个8字形支...             323   0.0  \n",
       "4    光纤老化预测方法及装置新华三大数据技术有限公司本申请提供一种光纤老化预测方法及装置，所述方法...             259   0.0  \n",
       "..                                                 ...             ...   ...  \n",
       "953  一种信息处理方法及装置腾讯科技(深圳)有限公司本发明公开了一种信息处理方法，所述方法包括：第...             269   1.0  \n",
       "954  一种适用于安防的广告机靖江天元爱尔瑞电子科技有限公司本实用新型公开了一种适用于安防的广告机，...             318   2.0  \n",
       "955  一种广告投放控制方法及装置阿里巴巴(中国)有限公司本发明公开了一种广告投放控制方法及装置，以...             241   2.0  \n",
       "956  一种广告数据推荐方法和系统北京奇艺世纪科技有限公司本发明公开了一种广告数据推荐方法和系统。所...             258   3.0  \n",
       "957  一种基于大数据的广告推送系统浙江华坤道威数据科技有限公司本发明公开了一种基于大数据的广告推送...             319   4.0  \n",
       "\n",
       "[958 rows x 12 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xRIgdvPChbKG"
   },
   "source": [
    "## 2. Build model Input and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T2TmyJC_hbKG"
   },
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self,df,tokenizer):\n",
    "        self.title = df['title'].values\n",
    "        self.assignee = df['assignee'].values\n",
    "        self.abstract = df['abstract'].values\n",
    "        self.label = df['label_id'].values\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sep_token = tokenizer.sep_token\n",
    "    def __len__(self):\n",
    "        return len(self.title)\n",
    "    def __getitem__(self, item):\n",
    "        label = int(self.label[item])\n",
    "        title = self.title[item]\n",
    "        assignee = self.assignee[item]\n",
    "        abstract = self.abstract[item]\n",
    "        input_text =  title + self.sep_token + assignee + self.sep_token + abstract\n",
    "        inputs = self.tokenizer(input_text, truncation=True, max_length=400, padding='max_length')\n",
    "        return torch.as_tensor(inputs['input_ids'], dtype=torch.long), \\\n",
    "               torch.as_tensor(inputs['attention_mask'], dtype=torch.long), \\\n",
    "               torch.as_tensor(label, dtype=torch.long)\n",
    "\n",
    "class TrainPETDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.title = df['title'].values\n",
    "        self.assignee = df['assignee'].values\n",
    "        self.abstract = df['abstract'].values\n",
    "        self.label = df['label_id'].values\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sep_token = tokenizer.sep_token\n",
    "        self.mask_token = tokenizer.mask_token\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.title)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        label = '通信'  # self.label[item]\n",
    "        title = self.title[item]\n",
    "        assignee = self.assignee[item]\n",
    "        abstract = self.abstract[item]\n",
    "        input_text = '本发明设计' + self.mask_token+ self.mask_token + '领域，具体如下' + title + self.sep_token + assignee + self.sep_token + abstract\n",
    "        inputs = self.tokenizer(input_text, truncation=True, max_length=400, padding='max_length')\n",
    "        label = self.tokenizer(label, truncation=True, max_length=2, padding='max_length', add_special_tokens=False)[\n",
    "            'input_ids']\n",
    "        return torch.as_tensor(inputs['input_ids'] + [self.pad_token] * len(label), dtype=torch.long), \\\n",
    "               torch.as_tensor(inputs['attention_mask'] + [0] * len(label), dtype=torch.long), \\\n",
    "               torch.as_tensor([-100] * len(inputs['input_ids']) + label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BKCU5PeahbKH"
   },
   "source": [
    "## 3. Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15126,
     "status": "ok",
     "timestamp": 1664091418955,
     "user": {
      "displayName": "guang han",
      "userId": "09096391548724575519"
     },
     "user_tz": -480
    },
    "id": "0JTytv2RhbKH",
    "outputId": "79d46d7f-920d-4d5f-d95b-f85dff039ec6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext-large were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-roberta-wwm-ext-large and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, AlbertForSequenceClassification\n",
    "# model = AlbertForSequenceClassification.from_pretrained(CFG.model_path,num_labels=36)\n",
    "# tokenizer = BertTokenizer.from_pretrained(CFG.model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(CFG.model_path,num_labels=36,ignore_mismatched_sizes=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_path)\n",
    "\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput, AutoModelForMaskedLM\n",
    "\n",
    "class Custom_Bert_Simple(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.base = AutoModelForSequenceClassification.from_pretrained(CFG.model_path, num_labels=36)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        base_output = self.base(input_ids=input_ids,\n",
    "                                attention_mask=attention_mask,\n",
    "                                )\n",
    "\n",
    "        output = base_output[0]\n",
    "        loss = None\n",
    "        if labels:\n",
    "            loss = F.cross_entropy(output, labels)\n",
    "        return SequenceClassifierOutput(logits=output, loss=loss)\n",
    "\n",
    "class CCFModelPET(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Backbone\n",
    "        self.backbone = AutoModelForMaskedLM.from_pretrained(CFG.model_path)  # 加载预训练模型\n",
    "        self.config = AutoConfig.from_pretrained(CFG.model_path)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        output = self.backbone(input_ids=input_ids,\n",
    "                               attention_mask=attention_mask,\n",
    "                               ).logits # batch_size, seq_len, hidden_size\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = F.cross_entropy(output.view(-1, self.config.vocab_size), labels.view(-1))\n",
    "        return SequenceClassifierOutput(logits=output, loss=loss)\n",
    "\n",
    "class Custom_Bert_Mean(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        config = AutoConfig.from_pretrained(CFG.model_path)\n",
    "        config.output_hidden_states = True\n",
    "        self.base = AutoModel.from_pretrained(CFG.model_path, config=config)\n",
    "        dim = config.hidden_size\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.cls = nn.Linear(dim, 36)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        base_output = self.base(input_ids=input_ids,\n",
    "                                attention_mask=attention_mask,\n",
    "                                )\n",
    "\n",
    "        output = base_output.hidden_states[-1] # b, s ,h\n",
    "        output = self.cls(self.dropout(torch.mean(output, dim=1)))\n",
    "        loss = None\n",
    "        if labels:\n",
    "            loss = F.cross_entropy(output, labels)\n",
    "        return SequenceClassifierOutput(logits=output, loss=loss)\n",
    "\n",
    "class CCFModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Backbone\n",
    "        self.config = AutoConfig.from_pretrained(CFG.model_path)\n",
    "        self.config.max_position_embeddings=2000\n",
    "        self.backbone = AutoModel.from_pretrained(CFG.model_path, config=self.config)  # 加载预训练模型\n",
    "        \n",
    "        # Multidropout\n",
    "        self.dropout1 = nn.Dropout(0.1)  # dropout 0.1\n",
    "        self.dropout2 = nn.Dropout(0.2)  # dropout 0.2\n",
    "        self.dropout3 = nn.Dropout(0.3)  # dropout 0.3\n",
    "        self.dropout4 = nn.Dropout(0.4)  # dropout 0.4\n",
    "        self.dropout5 = nn.Dropout(0.5)  # dropout 0.5\n",
    "\n",
    "        # GRU\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=self.config.hidden_size,  # 输入大小\n",
    "            hidden_size=self.config.hidden_size // 2,  # 隐藏层大小\n",
    "            bidirectional=True,  # 双向\n",
    "            batch_first=True,  # batch在最前面的维度\n",
    "            dropout=0.1,  # dropout 0.1\n",
    "            num_layers=1  # 层数\n",
    "        )\n",
    "\n",
    "        # Head\n",
    "        self.head = nn.Linear(self.config.hidden_size, 36)  # 线性层\n",
    "\n",
    "        self._init_weights(self.head)  # 初始化权重\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        output_backbone = self.backbone(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state  # batch_size, seq_len, hidden_size\n",
    "        output_backbone = self.rnn(output_backbone)[0]  # GRU\n",
    "\n",
    "        output1 = self.head(self.dropout1(torch.mean(output_backbone, dim=1)))  # dropout 0.1 + 线性层\n",
    "        output2 = self.head(self.dropout2(torch.mean(output_backbone, dim=1)))  # dropout 0.2 + 线性层\n",
    "        output3 = self.head(self.dropout3(torch.mean(output_backbone, dim=1)))  # dropout 0.3 + 线性层\n",
    "        output4 = self.head(self.dropout4(torch.mean(output_backbone, dim=1)))  # dropout 0.4 + 线性层\n",
    "        output5 = self.head(self.dropout5(torch.mean(output_backbone, dim=1)))  # dropout 0.5 + 线性层\n",
    "\n",
    "        output = (output1 + output2 + output3 + output4 + output5) / 5  # 平均\n",
    "        loss = None\n",
    "        if labels:\n",
    "            loss = F.cross_entropy(output, labels)\n",
    "        return SequenceClassifierOutput(logits=output, loss=loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0ukyVMxhbKH"
   },
   "source": [
    "## 4.Build train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r4yGGljShbKI"
   },
   "outputs": [],
   "source": [
    "def get_score(preds, gts):\n",
    "    return f1_score(preds, gts, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o4Q6DGkohbKI"
   },
   "outputs": [],
   "source": [
    "def train_fn(train_loader, model, optimizer, epoch, scheduler, device):\n",
    "    model.train()\n",
    "    losses = AverageMeter()\n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        label = batch[2].to(device)\n",
    "        mask = batch[1].to(device)\n",
    "        input_ids = batch[0].to(device)\n",
    "        batch_size = label.size(0)\n",
    "        output = model(input_ids, mask, labels=label)\n",
    "        loss = output.loss\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 500)\n",
    "        optimizer.step()\n",
    "        global_step += 1\n",
    "        scheduler.step()\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(train_loader) - 1):\n",
    "            print('Epoch: [{0}][{1}/{2}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'Grad: {grad_norm:.4f}  '\n",
    "                  'LR: {lr:.8f}  '\n",
    "                  .format(epoch + 1, step, len(train_loader),\n",
    "                          remain=timeSince(start, float(step + 1) / len(train_loader)),\n",
    "                          loss=losses,\n",
    "                          grad_norm=grad_norm,\n",
    "                          lr=scheduler.get_lr()[0]))\n",
    "    return losses.avg\n",
    "\n",
    "def valid_fn(valid_loader, model, device):\n",
    "    losses = AverageMeter()\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    labels = []\n",
    "    start = end = time.time()\n",
    "    for step, batch in enumerate(valid_loader):\n",
    "        label = batch[2].to(device)\n",
    "        mask = batch[1].to(device)\n",
    "        input_ids = batch[0].to(device)\n",
    "        batch_size = label.size(0)\n",
    "        with torch.no_grad():\n",
    "            output = model(input_ids, mask, labels=label)\n",
    "        loss = output.loss\n",
    "        y_preds = output.logits.argmax(dim=-1)\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        preds.append(y_preds.to('cpu').numpy())\n",
    "        labels.append(label.to('cpu').numpy())\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(valid_loader) - 1):\n",
    "            print('EVAL: [{0}/{1}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  .format(step, len(valid_loader),\n",
    "                          loss=losses,\n",
    "                          remain=timeSince(start, float(step + 1) / len(valid_loader))))\n",
    "    predictions = np.concatenate(preds)\n",
    "    labels = np.concatenate(labels)\n",
    "    #print(predictions)\n",
    "    return losses.avg, predictions, labels\n",
    "\n",
    "\n",
    "def train_loop(fold, model, train_dataset, valid_dataset):\n",
    "    LOGGER.info(f\"========== training ==========\")\n",
    "\n",
    "    # ====================================================\n",
    "    # loader\n",
    "    # ====================================================\n",
    "\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=CFG.batch_size,\n",
    "                              shuffle=True,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n",
    "    valid_loader = DataLoader(valid_dataset,\n",
    "                              batch_size=CFG.batch_size * 2,\n",
    "                              shuffle=False,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "    # ====================================================\n",
    "    # model & optimizer\n",
    "    # ====================================================\n",
    "    #model = Custom_Bert_Simple()\n",
    "    #model = AutoModelForSequenceClassification.from_pretrained(CFG.model_path, num_labels=1)\n",
    "    model.to(CFG.device)\n",
    "\n",
    "    def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_parameters = [\n",
    "            {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "             'lr': encoder_lr, 'weight_decay': weight_decay},\n",
    "            {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "             'lr': encoder_lr, 'weight_decay': 0.0},\n",
    "        ]\n",
    "        return optimizer_parameters\n",
    "\n",
    "    optimizer_parameters = get_optimizer_params(model,\n",
    "                                                encoder_lr=CFG.encoder_lr,\n",
    "                                                decoder_lr=CFG.decoder_lr,\n",
    "                                                weight_decay=CFG.weight_decay)\n",
    "    optimizer = AdamW(optimizer_parameters, lr=CFG.encoder_lr, eps=CFG.eps, betas=CFG.betas)\n",
    "\n",
    "    # ====================================================\n",
    "    # scheduler\n",
    "    # ====================================================\n",
    "    def get_scheduler(cfg, optimizer, num_train_steps):\n",
    "        cfg.num_warmup_steps = cfg.num_warmup_steps * num_train_steps\n",
    "        if cfg.scheduler == 'linear':\n",
    "            scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps\n",
    "            )\n",
    "        elif cfg.scheduler == 'cosine':\n",
    "            scheduler = get_cosine_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps,\n",
    "                num_cycles=cfg.num_cycles\n",
    "            )\n",
    "        return scheduler\n",
    "\n",
    "    num_train_steps = int(len(train_dataset) / CFG.batch_size * CFG.epochs)\n",
    "    scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n",
    "\n",
    "    # ====================================================\n",
    "    # loop\n",
    "    # ====================================================\n",
    "    # criterion = torch.nn.CrossEntropyLoss(ignore_index=- 1)\n",
    "\n",
    "    # criterion = LabelSmoothingLoss()\n",
    "    best_score = 0.\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # train\n",
    "        avg_loss = train_fn(train_loader, model, optimizer, epoch, scheduler, CFG.device)\n",
    "\n",
    "        # eval\n",
    "        avg_val_loss, predictions, valid_labels = valid_fn(valid_loader, model, CFG.device)\n",
    "\n",
    "        # scoring\n",
    "        score = get_score(predictions, valid_labels)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        LOGGER.info(\n",
    "            f'Epoch {epoch + 1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        LOGGER.info(f'Epoch {epoch + 1} - Score: {score:.4f}')\n",
    "\n",
    "\n",
    "        if best_score < score:\n",
    "            best_score = score\n",
    "            best_predictions = predictions\n",
    "            LOGGER.info(f'Epoch {epoch + 1} - Save Best Score: {best_score:.4f} Model')\n",
    "            torch.save({'model': model.state_dict(),\n",
    "                        'predictions': predictions},\n",
    "                       CFG.OUTPUT_DIR + \"{}_best{}.pth\".format(CFG.model_path.replace('/', '_'),fold))\n",
    "\n",
    "\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    del scheduler, optimizer, model\n",
    "    return best_predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5oQ5XGEnhbKJ"
   },
   "source": [
    "## 5.Build Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DhZV-ZKshbKK"
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1664091418957,
     "user": {
      "displayName": "guang han",
      "userId": "09096391548724575519"
     },
     "user_tz": -480
    },
    "id": "mcxdTETChbKK",
    "outputId": "572c2ddf-d4c5-4cfb-b888-6e026e9f017d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===============lr_2e-05===============\n",
      "INFO:__main__:===============lr_2e-05===============\n",
      "===============seed_1006===============\n",
      "INFO:__main__:===============seed_1006===============\n",
      "===============total_epochs_100===============\n",
      "INFO:__main__:===============total_epochs_100===============\n",
      "===============num_warmup_steps_0===============\n",
      "INFO:__main__:===============num_warmup_steps_0===============\n"
     ]
    }
   ],
   "source": [
    "def get_logger(filename=CFG.OUTPUT_DIR+ 'train'):\n",
    "    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=f\"{filename}.log\")\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = get_logger()\n",
    "LOGGER.info('===============lr_{}==============='.format(CFG.encoder_lr))\n",
    "LOGGER.info('===============seed_{}==============='.format(CFG.seed))\n",
    "LOGGER.info('===============total_epochs_{}==============='.format(CFG.epochs))\n",
    "LOGGER.info('===============num_warmup_steps_{}==============='.format(CFG.num_warmup_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "4TRMLFrnhbKL",
    "outputId": "f5641892-5ff6-432b-bc39-55232d204b36"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== training ==========\n",
      "INFO:__main__:========== training ==========\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/191] Elapsed 0m 4s (remain 15m 15s) Loss: 3.5488(3.5488) Grad: 13.0020  LR: 0.00002000  \n",
      "Epoch: [1][100/191] Elapsed 0m 38s (remain 0m 34s) Loss: 2.5828(2.9586) Grad: 16.7794  LR: 0.00002000  \n",
      "Epoch: [1][190/191] Elapsed 1m 8s (remain 0m 0s) Loss: 1.9054(2.6613) Grad: 21.3975  LR: 0.00002000  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 2.6157(2.6157) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 2.6613  avg_val_loss: 2.0746  time: 74s\n",
      "INFO:__main__:Epoch 1 - avg_train_loss: 2.6613  avg_val_loss: 2.0746  time: 74s\n",
      "Epoch 1 - Score: 0.1869\n",
      "INFO:__main__:Epoch 1 - Score: 0.1869\n",
      "Epoch 1 - Save Best Score: 0.1869 Model\n",
      "INFO:__main__:Epoch 1 - Save Best Score: 0.1869 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 3.3931(2.0746) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/191] Elapsed 0m 0s (remain 1m 35s) Loss: 1.5163(1.5163) Grad: 13.7322  LR: 0.00002000  \n",
      "Epoch: [2][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 2.7374(1.4954) Grad: 19.6205  LR: 0.00001999  \n",
      "Epoch: [2][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 1.9643(1.4713) Grad: 16.3520  LR: 0.00001998  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 1.5080(1.5080) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 1.4713  avg_val_loss: 1.5826  time: 70s\n",
      "INFO:__main__:Epoch 2 - avg_train_loss: 1.4713  avg_val_loss: 1.5826  time: 70s\n",
      "Epoch 2 - Score: 0.4734\n",
      "INFO:__main__:Epoch 2 - Score: 0.4734\n",
      "Epoch 2 - Save Best Score: 0.4734 Model\n",
      "INFO:__main__:Epoch 2 - Save Best Score: 0.4734 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 2.1568(1.5826) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/191] Elapsed 0m 0s (remain 1m 35s) Loss: 0.3562(0.3562) Grad: 5.4441  LR: 0.00001998  \n",
      "Epoch: [3][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.1093(0.7673) Grad: 3.7203  LR: 0.00001997  \n",
      "Epoch: [3][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.8818(0.7756) Grad: 15.7862  LR: 0.00001996  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 1.7146(1.7146) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.7756  avg_val_loss: 1.5846  time: 70s\n",
      "INFO:__main__:Epoch 3 - avg_train_loss: 0.7756  avg_val_loss: 1.5846  time: 70s\n",
      "Epoch 3 - Score: 0.4847\n",
      "INFO:__main__:Epoch 3 - Score: 0.4847\n",
      "Epoch 3 - Save Best Score: 0.4847 Model\n",
      "INFO:__main__:Epoch 3 - Save Best Score: 0.4847 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 1.9844(1.5846) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/191] Elapsed 0m 0s (remain 1m 36s) Loss: 0.1299(0.1299) Grad: 4.1922  LR: 0.00001996  \n",
      "Epoch: [4][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.1837(0.4743) Grad: 5.0585  LR: 0.00001994  \n",
      "Epoch: [4][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.3904(0.4073) Grad: 8.3013  LR: 0.00001992  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 1.6991(1.6991) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 0.4073  avg_val_loss: 1.7411  time: 70s\n",
      "INFO:__main__:Epoch 4 - avg_train_loss: 0.4073  avg_val_loss: 1.7411  time: 70s\n",
      "Epoch 4 - Score: 0.4499\n",
      "INFO:__main__:Epoch 4 - Score: 0.4499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 1.6357(1.7411) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/191] Elapsed 0m 0s (remain 1m 29s) Loss: 0.2392(0.2392) Grad: 9.2585  LR: 0.00001992  \n",
      "Epoch: [5][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.1078(0.2762) Grad: 2.5687  LR: 0.00001990  \n",
      "Epoch: [5][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0600(0.2460) Grad: 1.9213  LR: 0.00001988  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 3.3118(3.3118) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - avg_train_loss: 0.2460  avg_val_loss: 1.6975  time: 70s\n",
      "INFO:__main__:Epoch 5 - avg_train_loss: 0.2460  avg_val_loss: 1.6975  time: 70s\n",
      "Epoch 5 - Score: 0.5155\n",
      "INFO:__main__:Epoch 5 - Score: 0.5155\n",
      "Epoch 5 - Save Best Score: 0.5155 Model\n",
      "INFO:__main__:Epoch 5 - Save Best Score: 0.5155 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 1.1406(1.6975) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][0/191] Elapsed 0m 0s (remain 1m 39s) Loss: 0.3123(0.3123) Grad: 48.6012  LR: 0.00001988  \n",
      "Epoch: [6][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0570(0.1449) Grad: 1.8789  LR: 0.00001985  \n",
      "Epoch: [6][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0719(0.1466) Grad: 5.3434  LR: 0.00001982  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 1.3702(1.3702) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - avg_train_loss: 0.1466  avg_val_loss: 2.2607  time: 70s\n",
      "INFO:__main__:Epoch 6 - avg_train_loss: 0.1466  avg_val_loss: 2.2607  time: 70s\n",
      "Epoch 6 - Score: 0.5094\n",
      "INFO:__main__:Epoch 6 - Score: 0.5094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.8203(2.2607) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [7][0/191] Elapsed 0m 0s (remain 1m 29s) Loss: 0.0367(0.0367) Grad: 0.8717  LR: 0.00001982  \n",
      "Epoch: [7][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0254(0.0555) Grad: 0.4780  LR: 0.00001979  \n",
      "Epoch: [7][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0353(0.0687) Grad: 1.1923  LR: 0.00001976  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 2.2791(2.2791) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 - avg_train_loss: 0.0687  avg_val_loss: 1.9738  time: 70s\n",
      "INFO:__main__:Epoch 7 - avg_train_loss: 0.0687  avg_val_loss: 1.9738  time: 70s\n",
      "Epoch 7 - Score: 0.4676\n",
      "INFO:__main__:Epoch 7 - Score: 0.4676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 1.3196(1.9738) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [8][0/191] Elapsed 0m 0s (remain 1m 28s) Loss: 0.0680(0.0680) Grad: 2.0532  LR: 0.00001976  \n",
      "Epoch: [8][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0180(0.0536) Grad: 0.3610  LR: 0.00001972  \n",
      "Epoch: [8][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0217(0.0501) Grad: 0.3352  LR: 0.00001969  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 1.0715(1.0715) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 - avg_train_loss: 0.0501  avg_val_loss: 1.8931  time: 70s\n",
      "INFO:__main__:Epoch 8 - avg_train_loss: 0.0501  avg_val_loss: 1.8931  time: 70s\n",
      "Epoch 8 - Score: 0.5178\n",
      "INFO:__main__:Epoch 8 - Score: 0.5178\n",
      "Epoch 8 - Save Best Score: 0.5178 Model\n",
      "INFO:__main__:Epoch 8 - Save Best Score: 0.5178 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 1.3948(1.8931) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [9][0/191] Elapsed 0m 0s (remain 1m 30s) Loss: 0.0273(0.0273) Grad: 0.9649  LR: 0.00001969  \n",
      "Epoch: [9][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0166(0.0556) Grad: 0.4406  LR: 0.00001965  \n",
      "Epoch: [9][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0144(0.0454) Grad: 0.6487  LR: 0.00001960  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 1.6427(1.6427) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 - avg_train_loss: 0.0454  avg_val_loss: 1.8434  time: 70s\n",
      "INFO:__main__:Epoch 9 - avg_train_loss: 0.0454  avg_val_loss: 1.8434  time: 70s\n",
      "Epoch 9 - Score: 0.4882\n",
      "INFO:__main__:Epoch 9 - Score: 0.4882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 1.3384(1.8434) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [10][0/191] Elapsed 0m 0s (remain 1m 30s) Loss: 0.0198(0.0198) Grad: 0.6847  LR: 0.00001960  \n",
      "Epoch: [10][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0223(0.0382) Grad: 0.4056  LR: 0.00001956  \n",
      "Epoch: [10][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0111(0.0295) Grad: 0.2094  LR: 0.00001951  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 2.3388(2.3388) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 - avg_train_loss: 0.0295  avg_val_loss: 2.0786  time: 70s\n",
      "INFO:__main__:Epoch 10 - avg_train_loss: 0.0295  avg_val_loss: 2.0786  time: 70s\n",
      "Epoch 10 - Score: 0.4923\n",
      "INFO:__main__:Epoch 10 - Score: 0.4923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 1.1479(2.0786) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [11][0/191] Elapsed 0m 0s (remain 1m 32s) Loss: 0.0102(0.0102) Grad: 0.2163  LR: 0.00001951  \n",
      "Epoch: [11][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0075(0.0208) Grad: 0.1603  LR: 0.00001946  \n",
      "Epoch: [11][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0170(0.0322) Grad: 0.6203  LR: 0.00001941  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 3.0675(3.0675) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11 - avg_train_loss: 0.0322  avg_val_loss: 2.0899  time: 70s\n",
      "INFO:__main__:Epoch 11 - avg_train_loss: 0.0322  avg_val_loss: 2.0899  time: 70s\n",
      "Epoch 11 - Score: 0.5214\n",
      "INFO:__main__:Epoch 11 - Score: 0.5214\n",
      "Epoch 11 - Save Best Score: 0.5214 Model\n",
      "INFO:__main__:Epoch 11 - Save Best Score: 0.5214 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 1.0782(2.0899) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [12][0/191] Elapsed 0m 0s (remain 1m 31s) Loss: 0.0198(0.0198) Grad: 0.5404  LR: 0.00001941  \n",
      "Epoch: [12][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0414(0.1009) Grad: 1.7561  LR: 0.00001935  \n",
      "Epoch: [12][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0242(0.1622) Grad: 0.8405  LR: 0.00001930  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 1.5136(1.5136) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12 - avg_train_loss: 0.1622  avg_val_loss: 2.3700  time: 70s\n",
      "INFO:__main__:Epoch 12 - avg_train_loss: 0.1622  avg_val_loss: 2.3700  time: 70s\n",
      "Epoch 12 - Score: 0.4460\n",
      "INFO:__main__:Epoch 12 - Score: 0.4460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 1.4793(2.3700) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [13][0/191] Elapsed 0m 0s (remain 1m 31s) Loss: 0.0354(0.0354) Grad: 1.4652  LR: 0.00001930  \n",
      "Epoch: [13][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0142(0.2886) Grad: 0.5370  LR: 0.00001924  \n",
      "Epoch: [13][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 1.0603(0.2525) Grad: 33.9911  LR: 0.00001918  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 2.2979(2.2979) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13 - avg_train_loss: 0.2525  avg_val_loss: 1.8963  time: 70s\n",
      "INFO:__main__:Epoch 13 - avg_train_loss: 0.2525  avg_val_loss: 1.8963  time: 70s\n",
      "Epoch 13 - Score: 0.4770\n",
      "INFO:__main__:Epoch 13 - Score: 0.4770\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 1.2937(1.8963) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [14][0/191] Elapsed 0m 0s (remain 1m 26s) Loss: 0.1402(0.1402) Grad: 24.5238  LR: 0.00001918  \n",
      "Epoch: [14][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.1488(0.1610) Grad: 24.3868  LR: 0.00001911  \n",
      "Epoch: [14][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0343(0.1643) Grad: 1.6380  LR: 0.00001905  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 3.6070(3.6070) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14 - avg_train_loss: 0.1643  avg_val_loss: 2.2110  time: 70s\n",
      "INFO:__main__:Epoch 14 - avg_train_loss: 0.1643  avg_val_loss: 2.2110  time: 70s\n",
      "Epoch 14 - Score: 0.4858\n",
      "INFO:__main__:Epoch 14 - Score: 0.4858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 1.3174(2.2110) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [15][0/191] Elapsed 0m 0s (remain 1m 28s) Loss: 0.0218(0.0218) Grad: 0.5198  LR: 0.00001905  \n",
      "Epoch: [15][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0226(0.0949) Grad: 1.0083  LR: 0.00001898  \n",
      "Epoch: [15][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0216(0.0957) Grad: 2.0277  LR: 0.00001892  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 3.6264(3.6264) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15 - avg_train_loss: 0.0957  avg_val_loss: 2.3331  time: 70s\n",
      "INFO:__main__:Epoch 15 - avg_train_loss: 0.0957  avg_val_loss: 2.3331  time: 70s\n",
      "Epoch 15 - Score: 0.4651\n",
      "INFO:__main__:Epoch 15 - Score: 0.4651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.8204(2.3331) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [16][0/191] Elapsed 0m 0s (remain 1m 28s) Loss: 0.0135(0.0135) Grad: 0.3970  LR: 0.00001891  \n",
      "Epoch: [16][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0082(0.0301) Grad: 0.1913  LR: 0.00001884  \n",
      "Epoch: [16][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0056(0.0268) Grad: 0.0884  LR: 0.00001877  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 3.5216(3.5216) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16 - avg_train_loss: 0.0268  avg_val_loss: 2.3206  time: 70s\n",
      "INFO:__main__:Epoch 16 - avg_train_loss: 0.0268  avg_val_loss: 2.3206  time: 70s\n",
      "Epoch 16 - Score: 0.5199\n",
      "INFO:__main__:Epoch 16 - Score: 0.5199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6047(2.3206) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [17][0/191] Elapsed 0m 0s (remain 1m 28s) Loss: 0.0059(0.0059) Grad: 0.1548  LR: 0.00001877  \n",
      "Epoch: [17][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0150(0.0100) Grad: 0.5007  LR: 0.00001869  \n",
      "Epoch: [17][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0067(0.0087) Grad: 0.2535  LR: 0.00001861  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 3.5009(3.5009) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17 - avg_train_loss: 0.0087  avg_val_loss: 2.4078  time: 70s\n",
      "INFO:__main__:Epoch 17 - avg_train_loss: 0.0087  avg_val_loss: 2.4078  time: 70s\n",
      "Epoch 17 - Score: 0.5017\n",
      "INFO:__main__:Epoch 17 - Score: 0.5017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6077(2.4078) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [18][0/191] Elapsed 0m 0s (remain 1m 29s) Loss: 0.0033(0.0033) Grad: 0.0767  LR: 0.00001861  \n",
      "Epoch: [18][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0040(0.0061) Grad: 0.1266  LR: 0.00001853  \n",
      "Epoch: [18][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0039(0.0057) Grad: 0.0590  LR: 0.00001845  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 3.5255(3.5255) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18 - avg_train_loss: 0.0057  avg_val_loss: 2.4346  time: 70s\n",
      "INFO:__main__:Epoch 18 - avg_train_loss: 0.0057  avg_val_loss: 2.4346  time: 70s\n",
      "Epoch 18 - Score: 0.5091\n",
      "INFO:__main__:Epoch 18 - Score: 0.5091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6022(2.4346) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [19][0/191] Elapsed 0m 0s (remain 1m 29s) Loss: 0.0036(0.0036) Grad: 0.0817  LR: 0.00001845  \n",
      "Epoch: [19][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0024(0.0050) Grad: 0.0408  LR: 0.00001836  \n",
      "Epoch: [19][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0064(0.0049) Grad: 0.1215  LR: 0.00001828  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 3.5496(3.5496) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19 - avg_train_loss: 0.0049  avg_val_loss: 2.4621  time: 69s\n",
      "INFO:__main__:Epoch 19 - avg_train_loss: 0.0049  avg_val_loss: 2.4621  time: 69s\n",
      "Epoch 19 - Score: 0.5048\n",
      "INFO:__main__:Epoch 19 - Score: 0.5048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6068(2.4621) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [20][0/191] Elapsed 0m 0s (remain 1m 27s) Loss: 0.0038(0.0038) Grad: 0.0769  LR: 0.00001828  \n",
      "Epoch: [20][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0044(0.0044) Grad: 0.0986  LR: 0.00001819  \n",
      "Epoch: [20][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0059(0.0043) Grad: 0.1075  LR: 0.00001810  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 3.5862(3.5862) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20 - avg_train_loss: 0.0043  avg_val_loss: 2.4947  time: 69s\n",
      "INFO:__main__:Epoch 20 - avg_train_loss: 0.0043  avg_val_loss: 2.4947  time: 69s\n",
      "Epoch 20 - Score: 0.5048\n",
      "INFO:__main__:Epoch 20 - Score: 0.5048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6068(2.4947) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [21][0/191] Elapsed 0m 0s (remain 1m 28s) Loss: 0.0027(0.0027) Grad: 0.0397  LR: 0.00001810  \n",
      "Epoch: [21][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0027(0.0041) Grad: 0.0440  LR: 0.00001800  \n",
      "Epoch: [21][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0026(0.0039) Grad: 0.0529  LR: 0.00001791  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 3.6335(3.6335) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21 - avg_train_loss: 0.0039  avg_val_loss: 2.5261  time: 69s\n",
      "INFO:__main__:Epoch 21 - avg_train_loss: 0.0039  avg_val_loss: 2.5261  time: 69s\n",
      "Epoch 21 - Score: 0.5128\n",
      "INFO:__main__:Epoch 21 - Score: 0.5128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6123(2.5261) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [22][0/191] Elapsed 0m 0s (remain 1m 28s) Loss: 0.0048(0.0048) Grad: 0.1179  LR: 0.00001791  \n",
      "Epoch: [22][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0048(0.0036) Grad: 0.0837  LR: 0.00001781  \n",
      "Epoch: [22][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0040(0.0035) Grad: 0.0934  LR: 0.00001772  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 3.6640(3.6640) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22 - avg_train_loss: 0.0035  avg_val_loss: 2.5530  time: 70s\n",
      "INFO:__main__:Epoch 22 - avg_train_loss: 0.0035  avg_val_loss: 2.5530  time: 70s\n",
      "Epoch 22 - Score: 0.5124\n",
      "INFO:__main__:Epoch 22 - Score: 0.5124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6111(2.5530) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [23][0/191] Elapsed 0m 0s (remain 1m 29s) Loss: 0.0027(0.0027) Grad: 0.0454  LR: 0.00001772  \n",
      "Epoch: [23][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0031(0.0032) Grad: 0.0552  LR: 0.00001761  \n",
      "Epoch: [23][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0021(0.0031) Grad: 0.0381  LR: 0.00001751  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 3.6929(3.6929) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23 - avg_train_loss: 0.0031  avg_val_loss: 2.5810  time: 70s\n",
      "INFO:__main__:Epoch 23 - avg_train_loss: 0.0031  avg_val_loss: 2.5810  time: 70s\n",
      "Epoch 23 - Score: 0.5118\n",
      "INFO:__main__:Epoch 23 - Score: 0.5118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6072(2.5810) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [24][0/191] Elapsed 0m 0s (remain 1m 25s) Loss: 0.0033(0.0033) Grad: 0.0537  LR: 0.00001751  \n",
      "Epoch: [24][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0028(0.0029) Grad: 0.0461  LR: 0.00001740  \n",
      "Epoch: [24][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0016(0.0028) Grad: 0.0213  LR: 0.00001730  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 3.7070(3.7070) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24 - avg_train_loss: 0.0028  avg_val_loss: 2.6054  time: 69s\n",
      "INFO:__main__:Epoch 24 - avg_train_loss: 0.0028  avg_val_loss: 2.6054  time: 69s\n",
      "Epoch 24 - Score: 0.5171\n",
      "INFO:__main__:Epoch 24 - Score: 0.5171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6093(2.6054) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [25][0/191] Elapsed 0m 0s (remain 1m 29s) Loss: 0.0025(0.0025) Grad: 0.0432  LR: 0.00001730  \n",
      "Epoch: [25][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0027(0.0027) Grad: 0.0515  LR: 0.00001719  \n",
      "Epoch: [25][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0027(0.0026) Grad: 0.0386  LR: 0.00001709  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 3.7357(3.7357) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25 - avg_train_loss: 0.0026  avg_val_loss: 2.6272  time: 70s\n",
      "INFO:__main__:Epoch 25 - avg_train_loss: 0.0026  avg_val_loss: 2.6272  time: 70s\n",
      "Epoch 25 - Score: 0.5138\n",
      "INFO:__main__:Epoch 25 - Score: 0.5138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6128(2.6272) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [26][0/191] Elapsed 0m 0s (remain 1m 28s) Loss: 0.0031(0.0031) Grad: 0.0480  LR: 0.00001708  \n",
      "Epoch: [26][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0022(0.0024) Grad: 0.0598  LR: 0.00001697  \n",
      "Epoch: [26][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0013(0.0024) Grad: 0.0205  LR: 0.00001686  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 3.7662(3.7662) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26 - avg_train_loss: 0.0024  avg_val_loss: 2.6522  time: 69s\n",
      "INFO:__main__:Epoch 26 - avg_train_loss: 0.0024  avg_val_loss: 2.6522  time: 69s\n",
      "Epoch 26 - Score: 0.5138\n",
      "INFO:__main__:Epoch 26 - Score: 0.5138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6148(2.6522) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [27][0/191] Elapsed 0m 0s (remain 1m 29s) Loss: 0.0035(0.0035) Grad: 0.0587  LR: 0.00001686  \n",
      "Epoch: [27][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0009(0.0022) Grad: 0.0171  LR: 0.00001674  \n",
      "Epoch: [27][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0020(0.0022) Grad: 0.0310  LR: 0.00001663  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 3.7869(3.7869) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27 - avg_train_loss: 0.0022  avg_val_loss: 2.6767  time: 70s\n",
      "INFO:__main__:Epoch 27 - avg_train_loss: 0.0022  avg_val_loss: 2.6767  time: 70s\n",
      "Epoch 27 - Score: 0.5138\n",
      "INFO:__main__:Epoch 27 - Score: 0.5138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6135(2.6767) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [28][0/191] Elapsed 0m 0s (remain 1m 31s) Loss: 0.0018(0.0018) Grad: 0.0288  LR: 0.00001663  \n",
      "Epoch: [28][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0025(0.0020) Grad: 0.0483  LR: 0.00001650  \n",
      "Epoch: [28][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0012(0.0020) Grad: 0.0181  LR: 0.00001639  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 3.8074(3.8074) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28 - avg_train_loss: 0.0020  avg_val_loss: 2.7036  time: 70s\n",
      "INFO:__main__:Epoch 28 - avg_train_loss: 0.0020  avg_val_loss: 2.7036  time: 70s\n",
      "Epoch 28 - Score: 0.5171\n",
      "INFO:__main__:Epoch 28 - Score: 0.5171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6167(2.7036) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [29][0/191] Elapsed 0m 0s (remain 1m 29s) Loss: 0.0029(0.0029) Grad: 0.0470  LR: 0.00001639  \n",
      "Epoch: [29][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0021(0.0020) Grad: 0.0431  LR: 0.00001626  \n",
      "Epoch: [29][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0020(0.0019) Grad: 0.0273  LR: 0.00001615  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 3.8450(3.8450) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29 - avg_train_loss: 0.0019  avg_val_loss: 2.7281  time: 70s\n",
      "INFO:__main__:Epoch 29 - avg_train_loss: 0.0019  avg_val_loss: 2.7281  time: 70s\n",
      "Epoch 29 - Score: 0.5206\n",
      "INFO:__main__:Epoch 29 - Score: 0.5206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6215(2.7281) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [30][0/191] Elapsed 0m 0s (remain 1m 29s) Loss: 0.0016(0.0016) Grad: 0.0264  LR: 0.00001615  \n",
      "Epoch: [30][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0022(0.0018) Grad: 0.0423  LR: 0.00001602  \n",
      "Epoch: [30][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0019(0.0018) Grad: 0.0320  LR: 0.00001590  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 3.8886(3.8886) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30 - avg_train_loss: 0.0018  avg_val_loss: 2.7553  time: 70s\n",
      "INFO:__main__:Epoch 30 - avg_train_loss: 0.0018  avg_val_loss: 2.7553  time: 70s\n",
      "Epoch 30 - Score: 0.5206\n",
      "INFO:__main__:Epoch 30 - Score: 0.5206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6222(2.7553) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [31][0/191] Elapsed 0m 0s (remain 1m 28s) Loss: 0.0014(0.0014) Grad: 0.0238  LR: 0.00001590  \n",
      "Epoch: [31][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0020(0.0017) Grad: 0.0279  LR: 0.00001576  \n",
      "Epoch: [31][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0017(0.0017) Grad: 0.0367  LR: 0.00001564  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 3.9120(3.9120) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31 - avg_train_loss: 0.0017  avg_val_loss: 2.7767  time: 70s\n",
      "INFO:__main__:Epoch 31 - avg_train_loss: 0.0017  avg_val_loss: 2.7767  time: 70s\n",
      "Epoch 31 - Score: 0.5206\n",
      "INFO:__main__:Epoch 31 - Score: 0.5206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6225(2.7767) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [32][0/191] Elapsed 0m 0s (remain 1m 29s) Loss: 0.0012(0.0012) Grad: 0.0225  LR: 0.00001564  \n",
      "Epoch: [32][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0011(0.0015) Grad: 0.0186  LR: 0.00001550  \n",
      "Epoch: [32][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0016(0.0015) Grad: 0.0216  LR: 0.00001538  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 3.9498(3.9498) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32 - avg_train_loss: 0.0015  avg_val_loss: 2.8023  time: 70s\n",
      "INFO:__main__:Epoch 32 - avg_train_loss: 0.0015  avg_val_loss: 2.8023  time: 70s\n",
      "Epoch 32 - Score: 0.5206\n",
      "INFO:__main__:Epoch 32 - Score: 0.5206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6186(2.8023) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [33][0/191] Elapsed 0m 0s (remain 1m 27s) Loss: 0.0025(0.0025) Grad: 0.0432  LR: 0.00001538  \n",
      "Epoch: [33][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0013(0.0014) Grad: 0.0215  LR: 0.00001524  \n",
      "Epoch: [33][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0021(0.0014) Grad: 0.0364  LR: 0.00001511  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 3.9708(3.9708) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33 - avg_train_loss: 0.0014  avg_val_loss: 2.8264  time: 70s\n",
      "INFO:__main__:Epoch 33 - avg_train_loss: 0.0014  avg_val_loss: 2.8264  time: 70s\n",
      "Epoch 33 - Score: 0.5216\n",
      "INFO:__main__:Epoch 33 - Score: 0.5216\n",
      "Epoch 33 - Save Best Score: 0.5216 Model\n",
      "INFO:__main__:Epoch 33 - Save Best Score: 0.5216 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6223(2.8264) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [34][0/191] Elapsed 0m 0s (remain 1m 34s) Loss: 0.0011(0.0011) Grad: 0.0173  LR: 0.00001511  \n",
      "Epoch: [34][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0012(0.0013) Grad: 0.0200  LR: 0.00001497  \n",
      "Epoch: [34][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0015(0.0013) Grad: 0.0251  LR: 0.00001484  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 4.0062(4.0062) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34 - avg_train_loss: 0.0013  avg_val_loss: 2.8530  time: 70s\n",
      "INFO:__main__:Epoch 34 - avg_train_loss: 0.0013  avg_val_loss: 2.8530  time: 70s\n",
      "Epoch 34 - Score: 0.5177\n",
      "INFO:__main__:Epoch 34 - Score: 0.5177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6232(2.8530) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [35][0/191] Elapsed 0m 0s (remain 1m 27s) Loss: 0.0011(0.0011) Grad: 0.0194  LR: 0.00001484  \n",
      "Epoch: [35][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0014(0.0013) Grad: 0.0210  LR: 0.00001470  \n",
      "Epoch: [35][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0010(0.0012) Grad: 0.0155  LR: 0.00001457  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 4.0336(4.0336) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35 - avg_train_loss: 0.0012  avg_val_loss: 2.8778  time: 70s\n",
      "INFO:__main__:Epoch 35 - avg_train_loss: 0.0012  avg_val_loss: 2.8778  time: 70s\n",
      "Epoch 35 - Score: 0.5166\n",
      "INFO:__main__:Epoch 35 - Score: 0.5166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6235(2.8778) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [36][0/191] Elapsed 0m 0s (remain 1m 29s) Loss: 0.0008(0.0008) Grad: 0.0120  LR: 0.00001456  \n",
      "Epoch: [36][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0011(0.0011) Grad: 0.0165  LR: 0.00001442  \n",
      "Epoch: [36][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0007(0.0011) Grad: 0.0113  LR: 0.00001428  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 4.0608(4.0608) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36 - avg_train_loss: 0.0011  avg_val_loss: 2.9034  time: 70s\n",
      "INFO:__main__:Epoch 36 - avg_train_loss: 0.0011  avg_val_loss: 2.9034  time: 70s\n",
      "Epoch 36 - Score: 0.5166\n",
      "INFO:__main__:Epoch 36 - Score: 0.5166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6238(2.9034) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [37][0/191] Elapsed 0m 0s (remain 1m 27s) Loss: 0.0018(0.0018) Grad: 0.0311  LR: 0.00001428  \n",
      "Epoch: [37][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0007(0.0011) Grad: 0.0124  LR: 0.00001413  \n",
      "Epoch: [37][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0009(0.0011) Grad: 0.0128  LR: 0.00001400  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 4.0841(4.0841) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37 - avg_train_loss: 0.0011  avg_val_loss: 2.9262  time: 69s\n",
      "INFO:__main__:Epoch 37 - avg_train_loss: 0.0011  avg_val_loss: 2.9262  time: 69s\n",
      "Epoch 37 - Score: 0.5166\n",
      "INFO:__main__:Epoch 37 - Score: 0.5166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6222(2.9262) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [38][0/191] Elapsed 0m 0s (remain 1m 26s) Loss: 0.0013(0.0013) Grad: 0.0198  LR: 0.00001400  \n",
      "Epoch: [38][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0008(0.0010) Grad: 0.0121  LR: 0.00001385  \n",
      "Epoch: [38][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0005(0.0010) Grad: 0.0082  LR: 0.00001371  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 4.1082(4.1082) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38 - avg_train_loss: 0.0010  avg_val_loss: 2.9483  time: 70s\n",
      "INFO:__main__:Epoch 38 - avg_train_loss: 0.0010  avg_val_loss: 2.9483  time: 70s\n",
      "Epoch 38 - Score: 0.5215\n",
      "INFO:__main__:Epoch 38 - Score: 0.5215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6216(2.9483) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [39][0/191] Elapsed 0m 0s (remain 1m 31s) Loss: 0.0013(0.0013) Grad: 0.0213  LR: 0.00001371  \n",
      "Epoch: [39][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0006(0.0009) Grad: 0.0081  LR: 0.00001356  \n",
      "Epoch: [39][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0007(0.0009) Grad: 0.0106  LR: 0.00001342  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 4.1291(4.1291) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39 - avg_train_loss: 0.0009  avg_val_loss: 2.9698  time: 70s\n",
      "INFO:__main__:Epoch 39 - avg_train_loss: 0.0009  avg_val_loss: 2.9698  time: 70s\n",
      "Epoch 39 - Score: 0.5215\n",
      "INFO:__main__:Epoch 39 - Score: 0.5215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6200(2.9698) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [40][0/191] Elapsed 0m 0s (remain 1m 29s) Loss: 0.0007(0.0007) Grad: 0.0119  LR: 0.00001342  \n",
      "Epoch: [40][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0007(0.0009) Grad: 0.0096  LR: 0.00001326  \n",
      "Epoch: [40][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0008(0.0009) Grad: 0.0110  LR: 0.00001312  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 4.1526(4.1526) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40 - avg_train_loss: 0.0009  avg_val_loss: 2.9932  time: 70s\n",
      "INFO:__main__:Epoch 40 - avg_train_loss: 0.0009  avg_val_loss: 2.9932  time: 70s\n",
      "Epoch 40 - Score: 0.5215\n",
      "INFO:__main__:Epoch 40 - Score: 0.5215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6217(2.9932) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [41][0/191] Elapsed 0m 0s (remain 1m 29s) Loss: 0.0009(0.0009) Grad: 0.0162  LR: 0.00001312  \n",
      "Epoch: [41][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0007(0.0008) Grad: 0.0101  LR: 0.00001296  \n",
      "Epoch: [41][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0003(0.0008) Grad: 0.0050  LR: 0.00001282  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 4.1945(4.1945) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41 - avg_train_loss: 0.0008  avg_val_loss: 3.0176  time: 70s\n",
      "INFO:__main__:Epoch 41 - avg_train_loss: 0.0008  avg_val_loss: 3.0176  time: 70s\n",
      "Epoch 41 - Score: 0.5215\n",
      "INFO:__main__:Epoch 41 - Score: 0.5215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6195(3.0176) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [42][0/191] Elapsed 0m 0s (remain 1m 28s) Loss: 0.0009(0.0009) Grad: 0.0184  LR: 0.00001282  \n",
      "Epoch: [42][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0012(0.0008) Grad: 0.0191  LR: 0.00001266  \n",
      "Epoch: [42][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0015(0.0008) Grad: 0.0250  LR: 0.00001252  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 4.2141(4.2141) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42 - avg_train_loss: 0.0008  avg_val_loss: 3.0412  time: 70s\n",
      "INFO:__main__:Epoch 42 - avg_train_loss: 0.0008  avg_val_loss: 3.0412  time: 70s\n",
      "Epoch 42 - Score: 0.5215\n",
      "INFO:__main__:Epoch 42 - Score: 0.5215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6224(3.0412) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [43][0/191] Elapsed 0m 0s (remain 1m 30s) Loss: 0.0005(0.0005) Grad: 0.0077  LR: 0.00001252  \n",
      "Epoch: [43][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0008(0.0007) Grad: 0.0134  LR: 0.00001236  \n",
      "Epoch: [43][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0005(0.0007) Grad: 0.0063  LR: 0.00001222  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 4.2409(4.2409) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43 - avg_train_loss: 0.0007  avg_val_loss: 3.0656  time: 70s\n",
      "INFO:__main__:Epoch 43 - avg_train_loss: 0.0007  avg_val_loss: 3.0656  time: 70s\n",
      "Epoch 43 - Score: 0.5215\n",
      "INFO:__main__:Epoch 43 - Score: 0.5215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6286(3.0656) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [44][0/191] Elapsed 0m 0s (remain 1m 32s) Loss: 0.0011(0.0011) Grad: 0.0196  LR: 0.00001221  \n",
      "Epoch: [44][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0007(0.0007) Grad: 0.0122  LR: 0.00001205  \n",
      "Epoch: [44][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0008(0.0007) Grad: 0.0127  LR: 0.00001191  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 4.2635(4.2635) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44 - avg_train_loss: 0.0007  avg_val_loss: 3.0913  time: 70s\n",
      "INFO:__main__:Epoch 44 - avg_train_loss: 0.0007  avg_val_loss: 3.0913  time: 70s\n",
      "Epoch 44 - Score: 0.5215\n",
      "INFO:__main__:Epoch 44 - Score: 0.5215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6293(3.0913) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [45][0/191] Elapsed 0m 0s (remain 1m 29s) Loss: 0.0007(0.0007) Grad: 0.0099  LR: 0.00001191  \n",
      "Epoch: [45][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0012(0.0006) Grad: 0.0203  LR: 0.00001175  \n",
      "Epoch: [45][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0007(0.0006) Grad: 0.0109  LR: 0.00001160  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 4.2999(4.2999) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45 - avg_train_loss: 0.0006  avg_val_loss: 3.1164  time: 70s\n",
      "INFO:__main__:Epoch 45 - avg_train_loss: 0.0006  avg_val_loss: 3.1164  time: 70s\n",
      "Epoch 45 - Score: 0.5215\n",
      "INFO:__main__:Epoch 45 - Score: 0.5215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6299(3.1164) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [46][0/191] Elapsed 0m 0s (remain 1m 27s) Loss: 0.0007(0.0007) Grad: 0.0128  LR: 0.00001160  \n",
      "Epoch: [46][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0006(0.0006) Grad: 0.0099  LR: 0.00001144  \n",
      "Epoch: [46][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0005(0.0006) Grad: 0.0082  LR: 0.00001129  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 4.3191(4.3191) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46 - avg_train_loss: 0.0006  avg_val_loss: 3.1417  time: 70s\n",
      "INFO:__main__:Epoch 46 - avg_train_loss: 0.0006  avg_val_loss: 3.1417  time: 70s\n",
      "Epoch 46 - Score: 0.5215\n",
      "INFO:__main__:Epoch 46 - Score: 0.5215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6283(3.1417) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [47][0/191] Elapsed 0m 0s (remain 1m 28s) Loss: 0.0004(0.0004) Grad: 0.0063  LR: 0.00001129  \n",
      "Epoch: [47][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0004(0.0005) Grad: 0.0086  LR: 0.00001113  \n",
      "Epoch: [47][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0006(0.0005) Grad: 0.0098  LR: 0.00001098  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 4.3532(4.3532) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47 - avg_train_loss: 0.0005  avg_val_loss: 3.1673  time: 70s\n",
      "INFO:__main__:Epoch 47 - avg_train_loss: 0.0005  avg_val_loss: 3.1673  time: 70s\n",
      "Epoch 47 - Score: 0.5215\n",
      "INFO:__main__:Epoch 47 - Score: 0.5215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6325(3.1673) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [48][0/191] Elapsed 0m 0s (remain 1m 31s) Loss: 0.0006(0.0006) Grad: 0.0080  LR: 0.00001098  \n",
      "Epoch: [48][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0003(0.0005) Grad: 0.0056  LR: 0.00001081  \n",
      "Epoch: [48][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0003(0.0005) Grad: 0.0058  LR: 0.00001067  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 4.3749(4.3749) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48 - avg_train_loss: 0.0005  avg_val_loss: 3.1915  time: 69s\n",
      "INFO:__main__:Epoch 48 - avg_train_loss: 0.0005  avg_val_loss: 3.1915  time: 69s\n",
      "Epoch 48 - Score: 0.5215\n",
      "INFO:__main__:Epoch 48 - Score: 0.5215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6353(3.1915) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [49][0/191] Elapsed 0m 0s (remain 1m 27s) Loss: 0.0003(0.0003) Grad: 0.0045  LR: 0.00001067  \n",
      "Epoch: [49][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0004(0.0005) Grad: 0.0080  LR: 0.00001050  \n",
      "Epoch: [49][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0004(0.0005) Grad: 0.0064  LR: 0.00001035  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 4.4065(4.4065) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49 - avg_train_loss: 0.0005  avg_val_loss: 3.2111  time: 70s\n",
      "INFO:__main__:Epoch 49 - avg_train_loss: 0.0005  avg_val_loss: 3.2111  time: 70s\n",
      "Epoch 49 - Score: 0.5215\n",
      "INFO:__main__:Epoch 49 - Score: 0.5215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6345(3.2111) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [50][0/191] Elapsed 0m 0s (remain 1m 28s) Loss: 0.0004(0.0004) Grad: 0.0077  LR: 0.00001035  \n",
      "Epoch: [50][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0003(0.0004) Grad: 0.0062  LR: 0.00001019  \n",
      "Epoch: [50][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0002(0.0004) Grad: 0.0040  LR: 0.00001004  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 4.4365(4.4365) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50 - avg_train_loss: 0.0004  avg_val_loss: 3.2341  time: 70s\n",
      "INFO:__main__:Epoch 50 - avg_train_loss: 0.0004  avg_val_loss: 3.2341  time: 70s\n",
      "Epoch 50 - Score: 0.5211\n",
      "INFO:__main__:Epoch 50 - Score: 0.5211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6392(3.2341) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [51][0/191] Elapsed 0m 0s (remain 1m 29s) Loss: 0.0005(0.0005) Grad: 0.0082  LR: 0.00001004  \n",
      "Epoch: [51][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0004(0.0004) Grad: 0.0057  LR: 0.00000988  \n",
      "Epoch: [51][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0003(0.0004) Grad: 0.0044  LR: 0.00000973  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 4.4592(4.4592) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51 - avg_train_loss: 0.0004  avg_val_loss: 3.2586  time: 69s\n",
      "INFO:__main__:Epoch 51 - avg_train_loss: 0.0004  avg_val_loss: 3.2586  time: 69s\n",
      "Epoch 51 - Score: 0.5215\n",
      "INFO:__main__:Epoch 51 - Score: 0.5215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6371(3.2586) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [52][0/191] Elapsed 0m 0s (remain 1m 28s) Loss: 0.0003(0.0003) Grad: 0.0042  LR: 0.00000973  \n",
      "Epoch: [52][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0004(0.0004) Grad: 0.0054  LR: 0.00000956  \n",
      "Epoch: [52][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0003(0.0004) Grad: 0.0050  LR: 0.00000941  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 4.4968(4.4968) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52 - avg_train_loss: 0.0004  avg_val_loss: 3.2815  time: 70s\n",
      "INFO:__main__:Epoch 52 - avg_train_loss: 0.0004  avg_val_loss: 3.2815  time: 70s\n",
      "Epoch 52 - Score: 0.5215\n",
      "INFO:__main__:Epoch 52 - Score: 0.5215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6366(3.2815) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [53][0/191] Elapsed 0m 0s (remain 1m 28s) Loss: 0.0003(0.0003) Grad: 0.0052  LR: 0.00000941  \n",
      "Epoch: [53][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0008(0.0004) Grad: 0.0184  LR: 0.00000925  \n",
      "Epoch: [53][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0002(0.0004) Grad: 0.0030  LR: 0.00000910  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 4.5223(4.5223) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53 - avg_train_loss: 0.0004  avg_val_loss: 3.3033  time: 70s\n",
      "INFO:__main__:Epoch 53 - avg_train_loss: 0.0004  avg_val_loss: 3.3033  time: 70s\n",
      "Epoch 53 - Score: 0.5215\n",
      "INFO:__main__:Epoch 53 - Score: 0.5215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6392(3.3033) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [54][0/191] Elapsed 0m 0s (remain 1m 28s) Loss: 0.0011(0.0011) Grad: 0.0173  LR: 0.00000910  \n",
      "Epoch: [54][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0002(0.0003) Grad: 0.0048  LR: 0.00000894  \n",
      "Epoch: [54][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0003(0.0003) Grad: 0.0040  LR: 0.00000879  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 4.5459(4.5459) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54 - avg_train_loss: 0.0003  avg_val_loss: 3.3223  time: 69s\n",
      "INFO:__main__:Epoch 54 - avg_train_loss: 0.0003  avg_val_loss: 3.3223  time: 69s\n",
      "Epoch 54 - Score: 0.5215\n",
      "INFO:__main__:Epoch 54 - Score: 0.5215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6377(3.3223) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [55][0/191] Elapsed 0m 0s (remain 1m 28s) Loss: 0.0004(0.0004) Grad: 0.0060  LR: 0.00000879  \n",
      "Epoch: [55][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0002(0.0003) Grad: 0.0030  LR: 0.00000863  \n",
      "Epoch: [55][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0006(0.0003) Grad: 0.0098  LR: 0.00000848  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 4.5672(4.5672) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55 - avg_train_loss: 0.0003  avg_val_loss: 3.3433  time: 70s\n",
      "INFO:__main__:Epoch 55 - avg_train_loss: 0.0003  avg_val_loss: 3.3433  time: 70s\n",
      "Epoch 55 - Score: 0.5215\n",
      "INFO:__main__:Epoch 55 - Score: 0.5215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6371(3.3433) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [56][0/191] Elapsed 0m 0s (remain 1m 29s) Loss: 0.0002(0.0002) Grad: 0.0034  LR: 0.00000848  \n",
      "Epoch: [56][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0003(0.0003) Grad: 0.0061  LR: 0.00000832  \n",
      "Epoch: [56][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0005(0.0003) Grad: 0.0079  LR: 0.00000817  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 4.5996(4.5996) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56 - avg_train_loss: 0.0003  avg_val_loss: 3.3639  time: 70s\n",
      "INFO:__main__:Epoch 56 - avg_train_loss: 0.0003  avg_val_loss: 3.3639  time: 70s\n",
      "Epoch 56 - Score: 0.5243\n",
      "INFO:__main__:Epoch 56 - Score: 0.5243\n",
      "Epoch 56 - Save Best Score: 0.5243 Model\n",
      "INFO:__main__:Epoch 56 - Save Best Score: 0.5243 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6416(3.3639) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [57][0/191] Elapsed 0m 0s (remain 1m 32s) Loss: 0.0004(0.0004) Grad: 0.0066  LR: 0.00000817  \n",
      "Epoch: [57][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0005(0.0003) Grad: 0.0082  LR: 0.00000801  \n",
      "Epoch: [57][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0001(0.0003) Grad: 0.0019  LR: 0.00000786  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 4.6207(4.6207) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57 - avg_train_loss: 0.0003  avg_val_loss: 3.3919  time: 70s\n",
      "INFO:__main__:Epoch 57 - avg_train_loss: 0.0003  avg_val_loss: 3.3919  time: 70s\n",
      "Epoch 57 - Score: 0.5243\n",
      "INFO:__main__:Epoch 57 - Score: 0.5243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6387(3.3919) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [58][0/191] Elapsed 0m 0s (remain 1m 30s) Loss: 0.0002(0.0002) Grad: 0.0034  LR: 0.00000786  \n",
      "Epoch: [58][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0003(0.0003) Grad: 0.0039  LR: 0.00000770  \n",
      "Epoch: [58][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0002(0.0003) Grad: 0.0028  LR: 0.00000756  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 4.6457(4.6457) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58 - avg_train_loss: 0.0003  avg_val_loss: 3.4106  time: 70s\n",
      "INFO:__main__:Epoch 58 - avg_train_loss: 0.0003  avg_val_loss: 3.4106  time: 70s\n",
      "Epoch 58 - Score: 0.5243\n",
      "INFO:__main__:Epoch 58 - Score: 0.5243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6427(3.4106) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [59][0/191] Elapsed 0m 0s (remain 1m 30s) Loss: 0.0004(0.0004) Grad: 0.0059  LR: 0.00000756  \n",
      "Epoch: [59][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0002(0.0003) Grad: 0.0049  LR: 0.00000740  \n",
      "Epoch: [59][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0003(0.0003) Grad: 0.0036  LR: 0.00000726  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 4.6808(4.6808) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59 - avg_train_loss: 0.0003  avg_val_loss: 3.4344  time: 70s\n",
      "INFO:__main__:Epoch 59 - avg_train_loss: 0.0003  avg_val_loss: 3.4344  time: 70s\n",
      "Epoch 59 - Score: 0.5243\n",
      "INFO:__main__:Epoch 59 - Score: 0.5243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6425(3.4344) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [60][0/191] Elapsed 0m 0s (remain 1m 30s) Loss: 0.0002(0.0002) Grad: 0.0025  LR: 0.00000726  \n",
      "Epoch: [60][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0001(0.0003) Grad: 0.0020  LR: 0.00000710  \n",
      "Epoch: [60][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0002(0.0002) Grad: 0.0026  LR: 0.00000696  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 4.6934(4.6934) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60 - avg_train_loss: 0.0002  avg_val_loss: 3.4506  time: 70s\n",
      "INFO:__main__:Epoch 60 - avg_train_loss: 0.0002  avg_val_loss: 3.4506  time: 70s\n",
      "Epoch 60 - Score: 0.5207\n",
      "INFO:__main__:Epoch 60 - Score: 0.5207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6447(3.4506) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [61][0/191] Elapsed 0m 0s (remain 1m 29s) Loss: 0.0002(0.0002) Grad: 0.0029  LR: 0.00000696  \n",
      "Epoch: [61][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0002(0.0002) Grad: 0.0024  LR: 0.00000680  \n",
      "Epoch: [61][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0003(0.0002) Grad: 0.0054  LR: 0.00000666  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 4.7154(4.7154) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61 - avg_train_loss: 0.0002  avg_val_loss: 3.4719  time: 70s\n",
      "INFO:__main__:Epoch 61 - avg_train_loss: 0.0002  avg_val_loss: 3.4719  time: 70s\n",
      "Epoch 61 - Score: 0.5207\n",
      "INFO:__main__:Epoch 61 - Score: 0.5207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6492(3.4719) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [62][0/191] Elapsed 0m 0s (remain 1m 27s) Loss: 0.0003(0.0003) Grad: 0.0058  LR: 0.00000666  \n",
      "Epoch: [62][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0002(0.0002) Grad: 0.0027  LR: 0.00000650  \n",
      "Epoch: [62][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0002(0.0002) Grad: 0.0038  LR: 0.00000637  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 4.7364(4.7364) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62 - avg_train_loss: 0.0002  avg_val_loss: 3.4937  time: 70s\n",
      "INFO:__main__:Epoch 62 - avg_train_loss: 0.0002  avg_val_loss: 3.4937  time: 70s\n",
      "Epoch 62 - Score: 0.5214\n",
      "INFO:__main__:Epoch 62 - Score: 0.5214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6440(3.4937) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [63][0/191] Elapsed 0m 0s (remain 1m 30s) Loss: 0.0002(0.0002) Grad: 0.0022  LR: 0.00000636  \n",
      "Epoch: [63][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0002(0.0002) Grad: 0.0033  LR: 0.00000621  \n",
      "Epoch: [63][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0001(0.0002) Grad: 0.0017  LR: 0.00000608  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 4.7649(4.7649) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63 - avg_train_loss: 0.0002  avg_val_loss: 3.5146  time: 70s\n",
      "INFO:__main__:Epoch 63 - avg_train_loss: 0.0002  avg_val_loss: 3.5146  time: 70s\n",
      "Epoch 63 - Score: 0.5210\n",
      "INFO:__main__:Epoch 63 - Score: 0.5210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6465(3.5146) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [64][0/191] Elapsed 0m 0s (remain 1m 30s) Loss: 0.0001(0.0001) Grad: 0.0016  LR: 0.00000607  \n",
      "Epoch: [64][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0002(0.0002) Grad: 0.0030  LR: 0.00000592  \n",
      "Epoch: [64][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0001(0.0002) Grad: 0.0021  LR: 0.00000579  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 4.7768(4.7768) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64 - avg_train_loss: 0.0002  avg_val_loss: 3.5326  time: 70s\n",
      "INFO:__main__:Epoch 64 - avg_train_loss: 0.0002  avg_val_loss: 3.5326  time: 70s\n",
      "Epoch 64 - Score: 0.5206\n",
      "INFO:__main__:Epoch 64 - Score: 0.5206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6473(3.5326) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [65][0/191] Elapsed 0m 0s (remain 1m 29s) Loss: 0.0002(0.0002) Grad: 0.0024  LR: 0.00000579  \n",
      "Epoch: [65][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0001(0.0002) Grad: 0.0017  LR: 0.00000564  \n",
      "Epoch: [65][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0002(0.0002) Grad: 0.0027  LR: 0.00000551  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 4.7995(4.7995) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65 - avg_train_loss: 0.0002  avg_val_loss: 3.5526  time: 70s\n",
      "INFO:__main__:Epoch 65 - avg_train_loss: 0.0002  avg_val_loss: 3.5526  time: 70s\n",
      "Epoch 65 - Score: 0.5188\n",
      "INFO:__main__:Epoch 65 - Score: 0.5188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6482(3.5526) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [66][0/191] Elapsed 0m 0s (remain 1m 30s) Loss: 0.0002(0.0002) Grad: 0.0025  LR: 0.00000551  \n",
      "Epoch: [66][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0001(0.0002) Grad: 0.0017  LR: 0.00000536  \n",
      "Epoch: [66][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0001(0.0002) Grad: 0.0015  LR: 0.00000523  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 4.8220(4.8220) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66 - avg_train_loss: 0.0002  avg_val_loss: 3.5683  time: 70s\n",
      "INFO:__main__:Epoch 66 - avg_train_loss: 0.0002  avg_val_loss: 3.5683  time: 70s\n",
      "Epoch 66 - Score: 0.5188\n",
      "INFO:__main__:Epoch 66 - Score: 0.5188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6506(3.5683) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [67][0/191] Elapsed 0m 0s (remain 1m 32s) Loss: 0.0002(0.0002) Grad: 0.0038  LR: 0.00000523  \n",
      "Epoch: [67][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0001(0.0002) Grad: 0.0014  LR: 0.00000509  \n",
      "Epoch: [67][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0002(0.0002) Grad: 0.0025  LR: 0.00000496  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 4.8437(4.8437) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67 - avg_train_loss: 0.0002  avg_val_loss: 3.5845  time: 70s\n",
      "INFO:__main__:Epoch 67 - avg_train_loss: 0.0002  avg_val_loss: 3.5845  time: 70s\n",
      "Epoch 67 - Score: 0.5188\n",
      "INFO:__main__:Epoch 67 - Score: 0.5188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6506(3.5845) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [68][0/191] Elapsed 0m 0s (remain 1m 31s) Loss: 0.0002(0.0002) Grad: 0.0024  LR: 0.00000496  \n",
      "Epoch: [68][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0001(0.0002) Grad: 0.0017  LR: 0.00000481  \n",
      "Epoch: [68][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0001(0.0002) Grad: 0.0018  LR: 0.00000469  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 4.8740(4.8740) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68 - avg_train_loss: 0.0002  avg_val_loss: 3.6085  time: 70s\n",
      "INFO:__main__:Epoch 68 - avg_train_loss: 0.0002  avg_val_loss: 3.6085  time: 70s\n",
      "Epoch 68 - Score: 0.5188\n",
      "INFO:__main__:Epoch 68 - Score: 0.5188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6538(3.6085) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [69][0/191] Elapsed 0m 0s (remain 1m 29s) Loss: 0.0001(0.0001) Grad: 0.0021  LR: 0.00000469  \n",
      "Epoch: [69][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0001(0.0001) Grad: 0.0015  LR: 0.00000455  \n",
      "Epoch: [69][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0002(0.0001) Grad: 0.0031  LR: 0.00000443  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 4.8912(4.8912) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69 - avg_train_loss: 0.0001  avg_val_loss: 3.6276  time: 70s\n",
      "INFO:__main__:Epoch 69 - avg_train_loss: 0.0001  avg_val_loss: 3.6276  time: 70s\n",
      "Epoch 69 - Score: 0.5188\n",
      "INFO:__main__:Epoch 69 - Score: 0.5188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6533(3.6276) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [70][0/191] Elapsed 0m 0s (remain 1m 28s) Loss: 0.0001(0.0001) Grad: 0.0020  LR: 0.00000442  \n",
      "Epoch: [70][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0002(0.0002) Grad: 0.0042  LR: 0.00000429  \n",
      "Epoch: [70][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0001(0.0001) Grad: 0.0022  LR: 0.00000417  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 4.9107(4.9107) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70 - avg_train_loss: 0.0001  avg_val_loss: 3.6432  time: 69s\n",
      "INFO:__main__:Epoch 70 - avg_train_loss: 0.0001  avg_val_loss: 3.6432  time: 69s\n",
      "Epoch 70 - Score: 0.5188\n",
      "INFO:__main__:Epoch 70 - Score: 0.5188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6547(3.6432) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [71][0/191] Elapsed 0m 0s (remain 1m 30s) Loss: 0.0001(0.0001) Grad: 0.0018  LR: 0.00000417  \n",
      "Epoch: [71][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0001(0.0001) Grad: 0.0017  LR: 0.00000403  \n",
      "Epoch: [71][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0002(0.0001) Grad: 0.0043  LR: 0.00000392  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 4.9313(4.9313) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71 - avg_train_loss: 0.0001  avg_val_loss: 3.6593  time: 70s\n",
      "INFO:__main__:Epoch 71 - avg_train_loss: 0.0001  avg_val_loss: 3.6593  time: 70s\n",
      "Epoch 71 - Score: 0.5188\n",
      "INFO:__main__:Epoch 71 - Score: 0.5188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6590(3.6593) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [72][0/191] Elapsed 0m 0s (remain 1m 27s) Loss: 0.0001(0.0001) Grad: 0.0018  LR: 0.00000392  \n",
      "Epoch: [72][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0001(0.0001) Grad: 0.0011  LR: 0.00000379  \n",
      "Epoch: [72][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0001(0.0001) Grad: 0.0022  LR: 0.00000367  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 4.9429(4.9429) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72 - avg_train_loss: 0.0001  avg_val_loss: 3.6749  time: 69s\n",
      "INFO:__main__:Epoch 72 - avg_train_loss: 0.0001  avg_val_loss: 3.6749  time: 69s\n",
      "Epoch 72 - Score: 0.5188\n",
      "INFO:__main__:Epoch 72 - Score: 0.5188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6574(3.6749) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [73][0/191] Elapsed 0m 0s (remain 1m 28s) Loss: 0.0001(0.0001) Grad: 0.0024  LR: 0.00000367  \n",
      "Epoch: [73][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0001(0.0001) Grad: 0.0014  LR: 0.00000354  \n",
      "Epoch: [73][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0001(0.0001) Grad: 0.0017  LR: 0.00000343  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 4.9520(4.9520) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 73 - avg_train_loss: 0.0001  avg_val_loss: 3.6911  time: 69s\n",
      "INFO:__main__:Epoch 73 - avg_train_loss: 0.0001  avg_val_loss: 3.6911  time: 69s\n",
      "Epoch 73 - Score: 0.5188\n",
      "INFO:__main__:Epoch 73 - Score: 0.5188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6578(3.6911) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [74][0/191] Elapsed 0m 0s (remain 1m 28s) Loss: 0.0002(0.0002) Grad: 0.0036  LR: 0.00000343  \n",
      "Epoch: [74][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0001(0.0001) Grad: 0.0013  LR: 0.00000331  \n",
      "Epoch: [74][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0002(0.0001) Grad: 0.0023  LR: 0.00000320  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 4.9724(4.9724) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 74 - avg_train_loss: 0.0001  avg_val_loss: 3.7080  time: 70s\n",
      "INFO:__main__:Epoch 74 - avg_train_loss: 0.0001  avg_val_loss: 3.7080  time: 70s\n",
      "Epoch 74 - Score: 0.5188\n",
      "INFO:__main__:Epoch 74 - Score: 0.5188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6617(3.7080) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [75][0/191] Elapsed 0m 0s (remain 1m 28s) Loss: 0.0001(0.0001) Grad: 0.0021  LR: 0.00000320  \n",
      "Epoch: [75][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0001(0.0001) Grad: 0.0023  LR: 0.00000308  \n",
      "Epoch: [75][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0001(0.0001) Grad: 0.0009  LR: 0.00000297  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 4.9969(4.9969) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 75 - avg_train_loss: 0.0001  avg_val_loss: 3.7234  time: 70s\n",
      "INFO:__main__:Epoch 75 - avg_train_loss: 0.0001  avg_val_loss: 3.7234  time: 70s\n",
      "Epoch 75 - Score: 0.5180\n",
      "INFO:__main__:Epoch 75 - Score: 0.5180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6622(3.7234) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [76][0/191] Elapsed 0m 0s (remain 1m 30s) Loss: 0.0001(0.0001) Grad: 0.0013  LR: 0.00000297  \n",
      "Epoch: [76][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0001(0.0001) Grad: 0.0014  LR: 0.00000286  \n",
      "Epoch: [76][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0000(0.0001) Grad: 0.0008  LR: 0.00000275  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 5.0046(5.0046) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 76 - avg_train_loss: 0.0001  avg_val_loss: 3.7381  time: 69s\n",
      "INFO:__main__:Epoch 76 - avg_train_loss: 0.0001  avg_val_loss: 3.7381  time: 69s\n",
      "Epoch 76 - Score: 0.5188\n",
      "INFO:__main__:Epoch 76 - Score: 0.5188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6625(3.7381) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [77][0/191] Elapsed 0m 0s (remain 1m 28s) Loss: 0.0000(0.0000) Grad: 0.0007  LR: 0.00000275  \n",
      "Epoch: [77][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0001(0.0001) Grad: 0.0010  LR: 0.00000264  \n",
      "Epoch: [77][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0001(0.0001) Grad: 0.0015  LR: 0.00000254  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 5.0272(5.0272) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 77 - avg_train_loss: 0.0001  avg_val_loss: 3.7506  time: 69s\n",
      "INFO:__main__:Epoch 77 - avg_train_loss: 0.0001  avg_val_loss: 3.7506  time: 69s\n",
      "Epoch 77 - Score: 0.5188\n",
      "INFO:__main__:Epoch 77 - Score: 0.5188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6651(3.7506) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [78][0/191] Elapsed 0m 0s (remain 1m 29s) Loss: 0.0001(0.0001) Grad: 0.0008  LR: 0.00000254  \n",
      "Epoch: [78][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0001(0.0001) Grad: 0.0013  LR: 0.00000243  \n",
      "Epoch: [78][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0001(0.0001) Grad: 0.0009  LR: 0.00000234  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 5.0424(5.0424) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 78 - avg_train_loss: 0.0001  avg_val_loss: 3.7627  time: 70s\n",
      "INFO:__main__:Epoch 78 - avg_train_loss: 0.0001  avg_val_loss: 3.7627  time: 70s\n",
      "Epoch 78 - Score: 0.5188\n",
      "INFO:__main__:Epoch 78 - Score: 0.5188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6649(3.7627) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [79][0/191] Elapsed 0m 0s (remain 1m 30s) Loss: 0.0001(0.0001) Grad: 0.0012  LR: 0.00000233  \n",
      "Epoch: [79][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0001(0.0001) Grad: 0.0015  LR: 0.00000223  \n",
      "Epoch: [79][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0000(0.0001) Grad: 0.0005  LR: 0.00000214  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 5.0595(5.0595) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 79 - avg_train_loss: 0.0001  avg_val_loss: 3.7737  time: 70s\n",
      "INFO:__main__:Epoch 79 - avg_train_loss: 0.0001  avg_val_loss: 3.7737  time: 70s\n",
      "Epoch 79 - Score: 0.5188\n",
      "INFO:__main__:Epoch 79 - Score: 0.5188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6667(3.7737) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [80][0/191] Elapsed 0m 0s (remain 1m 30s) Loss: 0.0001(0.0001) Grad: 0.0013  LR: 0.00000214  \n",
      "Epoch: [80][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0001(0.0001) Grad: 0.0014  LR: 0.00000204  \n",
      "Epoch: [80][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0001(0.0001) Grad: 0.0011  LR: 0.00000195  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 5.0728(5.0728) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 80 - avg_train_loss: 0.0001  avg_val_loss: 3.7844  time: 70s\n",
      "INFO:__main__:Epoch 80 - avg_train_loss: 0.0001  avg_val_loss: 3.7844  time: 70s\n",
      "Epoch 80 - Score: 0.5188\n",
      "INFO:__main__:Epoch 80 - Score: 0.5188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6684(3.7844) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [81][0/191] Elapsed 0m 0s (remain 1m 28s) Loss: 0.0001(0.0001) Grad: 0.0009  LR: 0.00000195  \n",
      "Epoch: [81][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0001(0.0001) Grad: 0.0012  LR: 0.00000185  \n",
      "Epoch: [81][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0001(0.0001) Grad: 0.0016  LR: 0.00000177  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 5.0856(5.0856) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 81 - avg_train_loss: 0.0001  avg_val_loss: 3.7977  time: 70s\n",
      "INFO:__main__:Epoch 81 - avg_train_loss: 0.0001  avg_val_loss: 3.7977  time: 70s\n",
      "Epoch 81 - Score: 0.5188\n",
      "INFO:__main__:Epoch 81 - Score: 0.5188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6685(3.7977) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [82][0/191] Elapsed 0m 0s (remain 1m 31s) Loss: 0.0000(0.0000) Grad: 0.0006  LR: 0.00000177  \n",
      "Epoch: [82][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0001(0.0001) Grad: 0.0017  LR: 0.00000167  \n",
      "Epoch: [82][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0001(0.0001) Grad: 0.0018  LR: 0.00000159  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 5.0929(5.0929) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 82 - avg_train_loss: 0.0001  avg_val_loss: 3.8072  time: 70s\n",
      "INFO:__main__:Epoch 82 - avg_train_loss: 0.0001  avg_val_loss: 3.8072  time: 70s\n",
      "Epoch 82 - Score: 0.5178\n",
      "INFO:__main__:Epoch 82 - Score: 0.5178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6691(3.8072) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [83][0/191] Elapsed 0m 0s (remain 1m 28s) Loss: 0.0001(0.0001) Grad: 0.0010  LR: 0.00000159  \n",
      "Epoch: [83][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0001(0.0001) Grad: 0.0013  LR: 0.00000150  \n",
      "Epoch: [83][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0001(0.0001) Grad: 0.0012  LR: 0.00000143  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 5.1031(5.1031) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 83 - avg_train_loss: 0.0001  avg_val_loss: 3.8165  time: 70s\n",
      "INFO:__main__:Epoch 83 - avg_train_loss: 0.0001  avg_val_loss: 3.8165  time: 70s\n",
      "Epoch 83 - Score: 0.5178\n",
      "INFO:__main__:Epoch 83 - Score: 0.5178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6692(3.8165) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [84][0/191] Elapsed 0m 0s (remain 1m 29s) Loss: 0.0000(0.0000) Grad: 0.0006  LR: 0.00000143  \n",
      "Epoch: [84][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0000(0.0001) Grad: 0.0007  LR: 0.00000134  \n",
      "Epoch: [84][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0000(0.0001) Grad: 0.0007  LR: 0.00000127  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 5.1151(5.1151) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 84 - avg_train_loss: 0.0001  avg_val_loss: 3.8259  time: 70s\n",
      "INFO:__main__:Epoch 84 - avg_train_loss: 0.0001  avg_val_loss: 3.8259  time: 70s\n",
      "Epoch 84 - Score: 0.5178\n",
      "INFO:__main__:Epoch 84 - Score: 0.5178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6695(3.8259) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [85][0/191] Elapsed 0m 0s (remain 1m 34s) Loss: 0.0001(0.0001) Grad: 0.0009  LR: 0.00000127  \n",
      "Epoch: [85][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0001(0.0001) Grad: 0.0030  LR: 0.00000119  \n",
      "Epoch: [85][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0001(0.0001) Grad: 0.0011  LR: 0.00000112  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 5.1286(5.1286) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 85 - avg_train_loss: 0.0001  avg_val_loss: 3.8340  time: 70s\n",
      "INFO:__main__:Epoch 85 - avg_train_loss: 0.0001  avg_val_loss: 3.8340  time: 70s\n",
      "Epoch 85 - Score: 0.5178\n",
      "INFO:__main__:Epoch 85 - Score: 0.5178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6712(3.8340) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [86][0/191] Elapsed 0m 0s (remain 1m 30s) Loss: 0.0001(0.0001) Grad: 0.0019  LR: 0.00000112  \n",
      "Epoch: [86][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0001(0.0001) Grad: 0.0009  LR: 0.00000105  \n",
      "Epoch: [86][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0001(0.0001) Grad: 0.0008  LR: 0.00000098  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 5.1321(5.1321) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 86 - avg_train_loss: 0.0001  avg_val_loss: 3.8421  time: 69s\n",
      "INFO:__main__:Epoch 86 - avg_train_loss: 0.0001  avg_val_loss: 3.8421  time: 69s\n",
      "Epoch 86 - Score: 0.5178\n",
      "INFO:__main__:Epoch 86 - Score: 0.5178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6722(3.8421) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [87][0/191] Elapsed 0m 0s (remain 1m 28s) Loss: 0.0001(0.0001) Grad: 0.0021  LR: 0.00000098  \n",
      "Epoch: [87][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0001(0.0001) Grad: 0.0013  LR: 0.00000091  \n",
      "Epoch: [87][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0000(0.0001) Grad: 0.0006  LR: 0.00000085  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 5.1397(5.1397) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 87 - avg_train_loss: 0.0001  avg_val_loss: 3.8492  time: 70s\n",
      "INFO:__main__:Epoch 87 - avg_train_loss: 0.0001  avg_val_loss: 3.8492  time: 70s\n",
      "Epoch 87 - Score: 0.5188\n",
      "INFO:__main__:Epoch 87 - Score: 0.5188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6731(3.8492) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [88][0/191] Elapsed 0m 0s (remain 1m 30s) Loss: 0.0000(0.0000) Grad: 0.0007  LR: 0.00000085  \n",
      "Epoch: [88][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0001(0.0001) Grad: 0.0008  LR: 0.00000079  \n",
      "Epoch: [88][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0001(0.0001) Grad: 0.0008  LR: 0.00000073  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 5.1480(5.1480) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 88 - avg_train_loss: 0.0001  avg_val_loss: 3.8554  time: 70s\n",
      "INFO:__main__:Epoch 88 - avg_train_loss: 0.0001  avg_val_loss: 3.8554  time: 70s\n",
      "Epoch 88 - Score: 0.5188\n",
      "INFO:__main__:Epoch 88 - Score: 0.5188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6747(3.8554) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [89][0/191] Elapsed 0m 0s (remain 1m 27s) Loss: 0.0001(0.0001) Grad: 0.0022  LR: 0.00000073  \n",
      "Epoch: [89][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0001(0.0001) Grad: 0.0012  LR: 0.00000067  \n",
      "Epoch: [89][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0000(0.0001) Grad: 0.0006  LR: 0.00000062  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 5.1531(5.1531) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 89 - avg_train_loss: 0.0001  avg_val_loss: 3.8611  time: 70s\n",
      "INFO:__main__:Epoch 89 - avg_train_loss: 0.0001  avg_val_loss: 3.8611  time: 70s\n",
      "Epoch 89 - Score: 0.5178\n",
      "INFO:__main__:Epoch 89 - Score: 0.5178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6752(3.8611) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [90][0/191] Elapsed 0m 0s (remain 1m 29s) Loss: 0.0001(0.0001) Grad: 0.0017  LR: 0.00000062  \n",
      "Epoch: [90][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0001(0.0001) Grad: 0.0024  LR: 0.00000056  \n",
      "Epoch: [90][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0001(0.0001) Grad: 0.0013  LR: 0.00000051  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 5.1585(5.1585) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 90 - avg_train_loss: 0.0001  avg_val_loss: 3.8666  time: 70s\n",
      "INFO:__main__:Epoch 90 - avg_train_loss: 0.0001  avg_val_loss: 3.8666  time: 70s\n",
      "Epoch 90 - Score: 0.5178\n",
      "INFO:__main__:Epoch 90 - Score: 0.5178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6755(3.8666) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [91][0/191] Elapsed 0m 0s (remain 1m 28s) Loss: 0.0001(0.0001) Grad: 0.0008  LR: 0.00000051  \n",
      "Epoch: [91][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0001(0.0001) Grad: 0.0008  LR: 0.00000046  \n",
      "Epoch: [91][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0001(0.0001) Grad: 0.0014  LR: 0.00000042  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 5.1625(5.1625) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 91 - avg_train_loss: 0.0001  avg_val_loss: 3.8715  time: 70s\n",
      "INFO:__main__:Epoch 91 - avg_train_loss: 0.0001  avg_val_loss: 3.8715  time: 70s\n",
      "Epoch 91 - Score: 0.5178\n",
      "INFO:__main__:Epoch 91 - Score: 0.5178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6751(3.8715) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [92][0/191] Elapsed 0m 0s (remain 1m 29s) Loss: 0.0001(0.0001) Grad: 0.0018  LR: 0.00000042  \n",
      "Epoch: [92][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0001(0.0001) Grad: 0.0014  LR: 0.00000037  \n",
      "Epoch: [92][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0001(0.0001) Grad: 0.0012  LR: 0.00000033  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 5.1663(5.1663) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 92 - avg_train_loss: 0.0001  avg_val_loss: 3.8750  time: 70s\n",
      "INFO:__main__:Epoch 92 - avg_train_loss: 0.0001  avg_val_loss: 3.8750  time: 70s\n",
      "Epoch 92 - Score: 0.5178\n",
      "INFO:__main__:Epoch 92 - Score: 0.5178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6758(3.8750) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [93][0/191] Elapsed 0m 0s (remain 1m 30s) Loss: 0.0001(0.0001) Grad: 0.0014  LR: 0.00000033  \n",
      "Epoch: [93][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0001(0.0001) Grad: 0.0014  LR: 0.00000029  \n",
      "Epoch: [93][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0001(0.0001) Grad: 0.0011  LR: 0.00000026  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 5.1687(5.1687) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 93 - avg_train_loss: 0.0001  avg_val_loss: 3.8778  time: 69s\n",
      "INFO:__main__:Epoch 93 - avg_train_loss: 0.0001  avg_val_loss: 3.8778  time: 69s\n",
      "Epoch 93 - Score: 0.5178\n",
      "INFO:__main__:Epoch 93 - Score: 0.5178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6764(3.8778) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [94][0/191] Elapsed 0m 0s (remain 1m 30s) Loss: 0.0001(0.0001) Grad: 0.0013  LR: 0.00000026  \n",
      "Epoch: [94][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0001(0.0001) Grad: 0.0015  LR: 0.00000022  \n",
      "Epoch: [94][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0001(0.0001) Grad: 0.0008  LR: 0.00000019  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 5.1716(5.1716) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 94 - avg_train_loss: 0.0001  avg_val_loss: 3.8803  time: 69s\n",
      "INFO:__main__:Epoch 94 - avg_train_loss: 0.0001  avg_val_loss: 3.8803  time: 69s\n",
      "Epoch 94 - Score: 0.5178\n",
      "INFO:__main__:Epoch 94 - Score: 0.5178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6767(3.8803) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [95][0/191] Elapsed 0m 0s (remain 1m 30s) Loss: 0.0001(0.0001) Grad: 0.0009  LR: 0.00000019  \n",
      "Epoch: [95][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0001(0.0001) Grad: 0.0008  LR: 0.00000016  \n",
      "Epoch: [95][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0001(0.0001) Grad: 0.0008  LR: 0.00000014  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 5.1750(5.1750) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 95 - avg_train_loss: 0.0001  avg_val_loss: 3.8821  time: 70s\n",
      "INFO:__main__:Epoch 95 - avg_train_loss: 0.0001  avg_val_loss: 3.8821  time: 70s\n",
      "Epoch 95 - Score: 0.5178\n",
      "INFO:__main__:Epoch 95 - Score: 0.5178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6771(3.8821) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [96][0/191] Elapsed 0m 0s (remain 1m 29s) Loss: 0.0000(0.0000) Grad: 0.0007  LR: 0.00000014  \n",
      "Epoch: [96][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0001(0.0001) Grad: 0.0009  LR: 0.00000011  \n",
      "Epoch: [96][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0001(0.0001) Grad: 0.0010  LR: 0.00000009  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 5.1767(5.1767) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 96 - avg_train_loss: 0.0001  avg_val_loss: 3.8833  time: 70s\n",
      "INFO:__main__:Epoch 96 - avg_train_loss: 0.0001  avg_val_loss: 3.8833  time: 70s\n",
      "Epoch 96 - Score: 0.5178\n",
      "INFO:__main__:Epoch 96 - Score: 0.5178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6771(3.8833) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [97][0/191] Elapsed 0m 0s (remain 1m 32s) Loss: 0.0001(0.0001) Grad: 0.0011  LR: 0.00000009  \n",
      "Epoch: [97][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0001(0.0001) Grad: 0.0015  LR: 0.00000007  \n",
      "Epoch: [97][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0001(0.0001) Grad: 0.0028  LR: 0.00000005  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 5.1778(5.1778) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 97 - avg_train_loss: 0.0001  avg_val_loss: 3.8841  time: 70s\n",
      "INFO:__main__:Epoch 97 - avg_train_loss: 0.0001  avg_val_loss: 3.8841  time: 70s\n",
      "Epoch 97 - Score: 0.5178\n",
      "INFO:__main__:Epoch 97 - Score: 0.5178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6772(3.8841) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [98][0/191] Elapsed 0m 0s (remain 1m 32s) Loss: 0.0001(0.0001) Grad: 0.0015  LR: 0.00000005  \n",
      "Epoch: [98][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0001(0.0001) Grad: 0.0011  LR: 0.00000004  \n",
      "Epoch: [98][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0001(0.0001) Grad: 0.0009  LR: 0.00000003  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 5.1785(5.1785) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 98 - avg_train_loss: 0.0001  avg_val_loss: 3.8845  time: 70s\n",
      "INFO:__main__:Epoch 98 - avg_train_loss: 0.0001  avg_val_loss: 3.8845  time: 70s\n",
      "Epoch 98 - Score: 0.5178\n",
      "INFO:__main__:Epoch 98 - Score: 0.5178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6773(3.8845) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [99][0/191] Elapsed 0m 0s (remain 1m 32s) Loss: 0.0001(0.0001) Grad: 0.0017  LR: 0.00000002  \n",
      "Epoch: [99][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0000(0.0001) Grad: 0.0007  LR: 0.00000001  \n",
      "Epoch: [99][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0001(0.0001) Grad: 0.0008  LR: 0.00000001  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 5.1786(5.1786) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 99 - avg_train_loss: 0.0001  avg_val_loss: 3.8847  time: 69s\n",
      "INFO:__main__:Epoch 99 - avg_train_loss: 0.0001  avg_val_loss: 3.8847  time: 69s\n",
      "Epoch 99 - Score: 0.5178\n",
      "INFO:__main__:Epoch 99 - Score: 0.5178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6773(3.8847) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [100][0/191] Elapsed 0m 0s (remain 1m 31s) Loss: 0.0000(0.0000) Grad: 0.0006  LR: 0.00000001  \n",
      "Epoch: [100][100/191] Elapsed 0m 34s (remain 0m 30s) Loss: 0.0001(0.0001) Grad: 0.0011  LR: 0.00000000  \n",
      "Epoch: [100][190/191] Elapsed 1m 4s (remain 0m 0s) Loss: 0.0001(0.0001) Grad: 0.0008  LR: 0.00000000  \n",
      "EVAL: [0/24] Elapsed 0m 0s (remain 0m 7s) Loss: 5.1787(5.1787) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 100 - avg_train_loss: 0.0001  avg_val_loss: 3.8847  time: 70s\n",
      "INFO:__main__:Epoch 100 - avg_train_loss: 0.0001  avg_val_loss: 3.8847  time: 70s\n",
      "Epoch 100 - Score: 0.5178\n",
      "INFO:__main__:Epoch 100 - Score: 0.5178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [23/24] Elapsed 0m 4s (remain 0m 0s) Loss: 0.6773(3.8847) \n"
     ]
    }
   ],
   "source": [
    "fold = 1\n",
    "tr_data = train[train['fold']!=fold].reset_index(drop=True)\n",
    "va_data = train[train['fold']==fold].reset_index(drop=True)\n",
    "tr_dataset = TrainDataset(tr_data,tokenizer)\n",
    "va_dataset =TrainDataset(va_data,tokenizer)\n",
    "val_result = train_loop(fold, model,tr_dataset, va_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YChdW-GfRVdh"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_YQlpOUnhbKL"
   },
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.title = df['title'].values\n",
    "        self.assignee = df['assignee'].values\n",
    "        self.abstract = df['abstract'].values\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sep_token = tokenizer.sep_token\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.title)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        title = self.title[item]\n",
    "        assignee = self.assignee[item]\n",
    "        abstract = self.abstract[item]\n",
    "        input_text = title + self.sep_token + assignee + self.sep_token + abstract\n",
    "        inputs = self.tokenizer(input_text, truncation=True, max_length=400, padding='max_length')\n",
    "        return torch.as_tensor(inputs['input_ids'], dtype=torch.long), \\\n",
    "               torch.as_tensor(inputs['attention_mask'], dtype=torch.long)\n",
    "\n",
    "def infer(test_loader, model, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    probs = []\n",
    "    for step, batch in tqdm(enumerate(test_loader)):\n",
    "        mask = batch[1].to(device)\n",
    "        input_ids = batch[0].to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(input_ids=input_ids, attention_mask=mask)\n",
    "        logits = F.softmax(output.logits, dim=-1)\n",
    "        prob, y_preds = logits.max(dim=-1)\n",
    "        probs.append(prob.to('cpu').numpy())\n",
    "        preds.append(y_preds.to('cpu').numpy())\n",
    "\n",
    "    predictions = np.concatenate(preds)\n",
    "    probs = np.concatenate(probs)\n",
    "    return predictions, probs\n",
    "\n",
    "def infer_5folds(test_loader, model, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    probs = []\n",
    "    for step, batch in tqdm(enumerate(test_loader)):\n",
    "        mask = batch[1].to(device)\n",
    "        input_ids = batch[0].to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(input_ids=input_ids, attention_mask=mask)\n",
    "        logits = F.softmax(output.logits, dim=-1)\n",
    "        # prob, y_preds = logits.max(dim=-1)\n",
    "        # probs.append(prob.to('cpu').numpy())\n",
    "        # preds.append(y_preds.to('cpu').numpy())\n",
    "\n",
    "    predictions = np.concatenate(logits)\n",
    "    #probs = np.concatenate(probs)\n",
    "    return predictions #, probs\n",
    "\n",
    "res = []\n",
    "for fold in range(5):\n",
    "    saved_path = CFG.OUTPUT_DIR + \"{}_best{}.pth\".format(CFG.model_path.replace('/', '_'),fold)\n",
    "    model.load_state_dict(torch.load(saved_path)['model'])\n",
    "    test_dataset = TestDataset(test, tokenizer)\n",
    "    test_dataloader = DataLoader(test_dataset,\n",
    "                                batch_size=CFG.batch_size * 2,\n",
    "                                shuffle=False,\n",
    "                                num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "    result_1fold = infer_5folds(test_dataloader, model, CFG.device)\n",
    "    res.append(result_1fold)\n",
    "res = np.mean(res, axis=1)\n",
    "res = np.argmax(a, axis=-1)\n",
    "test['label'] = res\n",
    "\n",
    "test = test[['id', 'label']]\n",
    "test.to_csv('submit_A.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oeexZ_4u17Kr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
