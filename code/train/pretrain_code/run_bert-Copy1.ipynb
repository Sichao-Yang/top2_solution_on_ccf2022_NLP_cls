{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bef1b8b2-39fa-48eb-b920-5206c843a345",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import pretraining_args as args\n",
    "import time\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "random.seed(args.seed)\n",
    "import sys\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import gc\n",
    "import jieba\n",
    "import collections\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset)\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm, trange\n",
    "import random\n",
    "from random import randrange, randint, shuffle, choice, sample\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.metrics import matthews_corrcoef, f1_score\n",
    "\n",
    "# from NEZHA.configuration_nezha import NeZhaConfig\n",
    "# from NEZHA.modeling_nezha import NeZhaForMaskedLM\n",
    "\n",
    "\n",
    "\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM, BertConfig\n",
    "from transformers.optimization import AdamW\n",
    "import joblib\n",
    "\n",
    "\n",
    "jieba.enable_parallel(10)\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "logger = logging.getLogger(__name__)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0918ca3-da0b-4608-8e0c-71b44f68706b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed_value = 666\n",
    "def seed_all(seed_value):\n",
    "    random.seed(seed_value) # Python\n",
    "    np.random.seed(seed_value) # cpu vars\n",
    "    torch.manual_seed(seed_value) # cpu  vars\n",
    "    \n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value) # gpu vars\n",
    "        torch.backends.cudnn.deterministic = True  #needed\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb91bdcb-cc16-46bb-8bac-513ab2842a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_model = 'train_bert'\n",
    "logger = logging.getLogger('Bert_train')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "timestamp = time.strftime(\"%Y.%m.%d_%H.%M.%S\", time.localtime())\n",
    "fh = logging.FileHandler('log_{}.txt'.format(search_model))\n",
    "fh.setLevel(logging.DEBUG)\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('[%(asctime)s][%(levelname)s] ## %(message)s')\n",
    "fh.setFormatter(formatter)\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(fh)\n",
    "logger.addHandler(ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87c146eb-64cb-498e-a473-db3c30aedc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_signs(tokens):\n",
    "    signs = []\n",
    "    line = ''\n",
    "    for token in tokens:\n",
    "        if len(token) > 1:\n",
    "            line += ' '\n",
    "        else:\n",
    "            line += token\n",
    "    words = jieba.lcut(line)\n",
    "    # 带##的会被当做单独词\n",
    "    sign = 0\n",
    "    # signs中 0 1连续交替，代表词的区分\n",
    "    for word in words:\n",
    "        for i in word:\n",
    "            signs.append(sign)\n",
    "        sign = 1 if sign == 0 else 0\n",
    "    assert len(tokens) == len(signs)\n",
    "    return signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c466713-64c8-4c0d-b100-cefdbbe1f899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./abstracts_new_test.txt\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "args.train_batch_size = args.train_batch_size // args.gradient_accumulation_steps\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(f'{args.pretrained_path}')\n",
    "train_examples = None\n",
    "num_train_optimization_steps = None\n",
    "vocab_words = []\n",
    "with open(args.vocab_file, 'r') as fr:\n",
    "     for line in fr:\n",
    "        vocab_words.append(line.strip(\"\\n\"))\n",
    "print(args.pretrain_train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "353d0f9d-b64e-419f-91da-9b7d51b9ce42",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.pretrain_train_path = './abstracts_new_test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30a3d83d-075f-4ce0-af62-c47122556a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warmup_linear(x, warmup=0.002):\n",
    "    if x < warmup:\n",
    "        return x/warmup\n",
    "    return 1.0 - x\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "\n",
    "\n",
    "\n",
    "def create_masked_lm_predictions(tokens, masked_lm_prob, max_predictions_per_seq, vocab_words, word_ids):\n",
    "    \n",
    "    cand_indexes = []\n",
    "    word_indexes = []\n",
    "    last_word_sign = 0\n",
    "    for (i, token) in enumerate(tokens):\n",
    "        # 特殊符号为 -1\n",
    "        if token == \"[CLS]\" or token == \"[SEP]\" or word_ids[i] == -100:\n",
    "            continue\n",
    "        # (len(cand_indexes) >= 1 )\n",
    "        # 之前的处理中，会把##的去掉\n",
    "        # word_ids中连续0或者1为同一个词\n",
    "        if word_ids[i] == last_word_sign:\n",
    "            word_indexes.append(i)\n",
    "        elif token.startswith(\"##\"):\n",
    "            word_indexes.append(i)\n",
    "            last_word_sign = word_ids[i]\n",
    "        else:\n",
    "            # token不带## 且对应wordid不等则为另外一个词\n",
    "            cand_indexes.append(word_indexes)\n",
    "            word_indexes = []\n",
    "            word_indexes.append(i)\n",
    "            last_word_sign = word_ids[i]\n",
    "\n",
    "    random.shuffle(cand_indexes)\n",
    "    output_tokens = list(tokens)\n",
    "    masked_lm = collections.namedtuple(\"masked_lm\", [\"index\", \"label\"])  \n",
    "    num_to_predict = min(max_predictions_per_seq,\n",
    "                       max(1, int(round(len(tokens) * masked_lm_prob))))\n",
    "    \n",
    "    masked_lms = []\n",
    "    covered_indexes = set()\n",
    "    for word_indexes in cand_indexes:\n",
    "        if str(word_indexes) in covered_indexes:\n",
    "              continue\n",
    "        covered_indexes.add(str(word_indexes))\n",
    "\n",
    "        random1 = random.random()\n",
    "        random2 = random.random()\n",
    "        for index in word_indexes:\n",
    "            if len(masked_lms) >= num_to_predict:\n",
    "                break\n",
    "            masked_token = None\n",
    "            # 80% of the time, replace with [MASK]\n",
    "            if random1 < 0.8:\n",
    "                 masked_token = \"[MASK]\"\n",
    "            else:\n",
    "        # 10% of the time, keep original\n",
    "               if random2 < 0.5:\n",
    "                   masked_token = tokens[index]\n",
    "        # 10% of the time, replace with random word\n",
    "               else:\n",
    "                  masked_token = vocab_words[random.randint(0, len(vocab_words) - 1)]\n",
    "\n",
    "            output_tokens[index] = masked_token\n",
    "            masked_lms.append(masked_lm(index=index, label=tokens[index]))\n",
    "            \n",
    "    masked_lms = sorted(masked_lms, key=lambda x: x.index)\n",
    "\n",
    "    masked_lm_positions = []\n",
    "    masked_lm_labels = []\n",
    "    for p in masked_lms:\n",
    "        masked_lm_positions.append(p.index)\n",
    "        masked_lm_labels.append(p.label)\n",
    "    \n",
    "    \n",
    "    return output_tokens, masked_lm_positions, masked_lm_labels\n",
    "\n",
    "\n",
    "\n",
    "def create_examples(data_path, tokenizer, max_seq_length, masked_lm_prob, max_predictions_per_seq, vocab_words):\n",
    "    \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "#     vocab_check = {w:0 for w in vocab_words}\n",
    "    examples = []\n",
    "    max_num_tokens = max_seq_length - 2\n",
    "    fr = open(data_path, \"r\")\n",
    "    for (i, line) in tqdm(enumerate(fr), desc=\"Creating Example\"):\n",
    "\n",
    "        words_ids = []\n",
    "        line = line.strip().split('---')\n",
    "        title = line[0]\n",
    "        assignee = line[1]\n",
    "        abstract = line[2]\n",
    "        label = line[3]\n",
    "\n",
    "        words_ids = []\n",
    "        tokens_title = tokenizer.tokenize(title)\n",
    "        tokens_assignee= tokenizer.tokenize(assignee)\n",
    "        tokens_abstract = tokenizer.tokenize(abstract)\n",
    "\n",
    "        tokens = [\"[CLS]\"] + tokens_title + [\"[SEP]\"] + tokens_assignee + [\"[SEP]\"] + tokens_abstract + [\"[SEP]\"]\n",
    "        segment_ids = [0] * len(tokens)\n",
    "        \n",
    "        signs_title = get_word_signs(tokens_title)\n",
    "        signs_assignee = get_word_signs(tokens_assignee)\n",
    "        signs_abstract = get_word_signs(tokens_abstract)\n",
    "\n",
    "        for sign in signs_title:\n",
    "            words_ids.append(sign)\n",
    "        words_ids.append(-100)\n",
    "\n",
    "        for sign in signs_assignee:\n",
    "            words_ids.append(sign)\n",
    "        words_ids.append(-100)\n",
    "\n",
    "        for sign in signs_abstract:\n",
    "            words_ids.append(sign)\n",
    "        words_ids.append(-100)\n",
    "\n",
    "        tokens, masked_lm_positions, masked_lm_labels=create_masked_lm_predictions(\n",
    "            tokens, masked_lm_prob, max_predictions_per_seq, vocab_words, words_ids)  \n",
    "        \n",
    "        example = {\n",
    "            \"tokens\": tokens,\n",
    "            \"segment_ids\": segment_ids,\n",
    "            \"masked_lm_positions\": masked_lm_positions,\n",
    "            \"masked_lm_labels\": masked_lm_labels}\n",
    "        examples.append(example)\n",
    "    fr.close()\n",
    "    return examples\n",
    "\n",
    "def convert_examples_to_features(examples, max_seq_length, tokenizer):\n",
    "    features = []\n",
    "    for i, example in enumerate(examples):\n",
    "        if i % 300000 == 0:\n",
    "            print(f'{i} have finished!')\n",
    "        tokens = example[\"tokens\"]\n",
    "        segment_ids = example[\"segment_ids\"]\n",
    "        masked_lm_positions = example[\"masked_lm_positions\"]\n",
    "        masked_lm_labels = example[\"masked_lm_labels\"]\n",
    "#         print(len(tokens), len(segment_ids), max_seq_length)\n",
    "        assert len(tokens) == len(segment_ids) <= max_seq_length  # The preprocessed data should be already truncated\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        masked_label_ids = tokenizer.convert_tokens_to_ids(masked_lm_labels)\n",
    "\n",
    "        input_array = np.zeros(max_seq_length, dtype=np.int)\n",
    "        input_array[:len(input_ids)] = input_ids\n",
    "\n",
    "        mask_array = np.zeros(max_seq_length, dtype=np.bool)\n",
    "        mask_array[:len(input_ids)] = 1\n",
    "\n",
    "        segment_array = np.zeros(max_seq_length, dtype=np.bool)\n",
    "        segment_array[:len(segment_ids)] = segment_ids\n",
    "\n",
    "        lm_label_array = np.full(max_seq_length, dtype=np.int, fill_value=-100)\n",
    "        lm_label_array[masked_lm_positions] = masked_label_ids\n",
    "        \n",
    "        feature = InputFeatures(input_ids=input_array,\n",
    "                         input_mask=mask_array,segment_ids=segment_array, label_id=lm_label_array)\n",
    "        features.append(feature)\n",
    "        # if i < 10:\n",
    "        #     logger.info(\"input_ids: %s\\ninput_mask:%s\\nsegment_ids:%s\\nlabel_id:%s\" %(input_array, mask_array, segment_array, lm_label_array))\n",
    "    return features\n",
    "        \n",
    "# train_features = convert_examples_to_features(train_examples, args.max_seq_length, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98745854-df77-4764-a49a-95ff0fb03d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating Example: 5000it [00:26, 189.90it/s]\n"
     ]
    }
   ],
   "source": [
    "if args.do_train:\n",
    "    train_examples = create_examples(data_path=args.pretrain_train_path,\n",
    "                                     tokenizer=tokenizer,\n",
    "                                     max_seq_length=args.max_seq_length,\n",
    "                                     masked_lm_prob=args.masked_lm_prob,\n",
    "                                     max_predictions_per_seq=args.max_predictions_per_seq,\n",
    "                                     vocab_words=vocab_words)\n",
    "#     with open('./train_examples.pk', 'rb')as f:\n",
    "#         train_examples = joblib.load(f)\n",
    "\n",
    "\n",
    "    num_train_optimization_steps = int(\n",
    "        math.ceil(len(train_examples) / args.train_batch_size) / args.gradient_accumulation_steps) * args.num_train_epochs\n",
    "    num_train_optimization_steps = num_train_optimization_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab6a10c7-c754-477c-823b-94c8c8d38c14",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_138199/3279259686.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_examples_to_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_138199/1319415195.py\u001b[0m in \u001b[0;36mconvert_examples_to_features\u001b[0;34m(examples, max_seq_length, tokenizer)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mconvert_examples_to_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m300000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{i} have finished!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "train_features = convert_examples_to_features(train_examples, args.max_seq_length, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "444144af-468f-4fda-8d83-f114cd661342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import joblib\n",
    "# with open('./train_examples.pk', 'wb')as f:\n",
    "#     joblib.dump(train_examples, f)\n",
    "\n",
    "# with open('./train_examples.pk', 'rb')as f:\n",
    "#     train_examples = joblib.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f075e92c-e653-49ab-9bec-7a98fbc77df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trained_dict = torch.load(f'{args.pretrained_path}pytorch_model.bin')\n",
    "model = BertForMaskedLM(config=BertConfig.from_json_file(args.bert_config_json))\n",
    "# BertForMaskedLM\n",
    "# model = NeZhaForMaskedLM(config=NeZhaConfig.from_json_file(args.bert_config_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f423e6a-2882-457f-b26b-f8b7dde4e0dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_trained_dict = collections.OrderedDict(pre_trained_dict)\n",
    "# pre_trained_dict['cls.seq_relationship.weight']\n",
    "pre_trained_dict.pop('cls.seq_relationship.weight')\n",
    "pre_trained_dict.pop('cls.seq_relationship.bias')\n",
    "n=0\n",
    "model_init = dict(model.state_dict())\n",
    "for k in pre_trained_dict:\n",
    "    if k in model_init:\n",
    "        n+=1\n",
    "        model_init[k] = pre_trained_dict[k]\n",
    "model.load_state_dict(model_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69bc0152-0cc0-4000-8b6b-2d7e0cdb1f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n",
    "warmup_step = int(args.warmup_proportion * num_train_optimization_steps)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=warmup_step, \n",
    "    num_training_steps=int(num_train_optimization_steps)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f46c88b3-5aec-472c-af67-94f89133bfad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19160"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_train_optimization_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fdd5a6-eff8-40f5-8ca1-3f8c2ef36837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 have finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-27 22:25:33,024][INFO] ## training.........................\n",
      "100%|██████████| 3832/3832 [08:54<00:00,  7.17it/s, epoch_loss=1.78]\n",
      "100%|██████████| 3832/3832 [08:54<00:00,  7.17it/s, epoch_loss=1.56]\n",
      " 48%|████▊     | 1855/3832 [04:19<04:32,  7.27it/s, epoch_loss=1.41]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 3832/3832 [08:54<00:00,  7.17it/s, epoch_loss=1.34]\n",
      "100%|██████████| 3832/3832 [08:53<00:00,  7.18it/s, epoch_loss=1.11]\n",
      "100%|██████████| 3832/3832 [08:53<00:00,  7.18it/s, epoch_loss=0.92] \n",
      " 13%|█▎        | 506/3832 [01:11<07:36,  7.29it/s, epoch_loss=0.828]"
     ]
    }
   ],
   "source": [
    "# init epoch1:\n",
    "train_features = convert_examples_to_features(train_examples, args.max_seq_length, tokenizer)\n",
    "all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
    "all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\n",
    "\n",
    "global_step = 0\n",
    "best_loss = 100000\n",
    "tr_loss = 0\n",
    "if args.do_train:\n",
    "    \n",
    "    logger.info('training.........................')\n",
    "    model.train()\n",
    "    #int(args.num_train_epochs)\n",
    "    for epoch in range(int(args.num_train_epochs)):\n",
    "        if epoch % 5 == 0 and epoch != 0:\n",
    "            model_weights = dict(model.state_dict())\n",
    "            model_weights = collections.OrderedDict(model_weights)\n",
    "            torch.save(model_weights, f'./outputs/pytorch_model_{epoch}.bin')            \n",
    "            \n",
    "            \n",
    "        if epoch % 100 == 0 and epoch != 0:\n",
    "            train_examples = create_examples(data_path=args.pretrain_train_path,\n",
    "                                 tokenizer=tokenizer,\n",
    "                                 max_seq_length=args.max_seq_length,\n",
    "                                 masked_lm_prob=args.masked_lm_prob,\n",
    "                                 max_predictions_per_seq=args.max_predictions_per_seq,\n",
    "                                 vocab_words=vocab_words)\n",
    "            \n",
    "        \n",
    "            train_features = convert_examples_to_features(train_examples, args.max_seq_length, tokenizer)\n",
    "            all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
    "            all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
    "            all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
    "            all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\n",
    "\n",
    "        train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "#         train_sampler = RandomSampler(train_data)\n",
    "#         train_dataloader = DataLoader(train_data, sampler=train_sampler,num_workers=4, batch_size=args.train_batch_size)\n",
    "#         train_sampler = torch.utils.data.distributed.DistributedSampler(train_data, num_replicas=2)\n",
    "        train_dataloader = torch.utils.data.DataLoader(train_data,\n",
    "                                         num_workers=4,\n",
    "                                         batch_size=args.train_batch_size)\n",
    "        \n",
    "        train_loss = []\n",
    "        nb_tr_examples, nb_tr_steps = 0, 0\n",
    "        tk0 = tqdm(train_dataloader, total=len(train_dataloader))\n",
    "        for step, batch in enumerate(tk0):\n",
    "#             if nb_tr_steps > 0 and nb_tr_steps % 1000 == 0:\n",
    "#                 logger.info(\"=====-epoch %d -train_step %d -train_loss %.4f\\n\" % (epoch,\n",
    "#                                                                                   nb_tr_steps,\n",
    "#                                                                                   np.mean(train_loss)))\n",
    "                \n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids, input_mask, segment_ids, label_ids = batch\n",
    "            # masked_lm_loss\n",
    "            loss = model(input_ids=input_ids, \n",
    "                         token_type_ids=segment_ids, \n",
    "                         attention_mask=input_mask, \n",
    "                         labels=label_ids)[0]\n",
    "            loss = loss.mean()\n",
    "            if args.gradient_accumulation_steps > 1:\n",
    "                loss = loss / args.gradient_accumulation_steps\n",
    "            loss.backward()\n",
    "\n",
    "            train_loss.append(loss.item()*args.gradient_accumulation_steps)\n",
    "            nb_tr_examples += input_ids.size(0)\n",
    "            nb_tr_steps += 1\n",
    "            if (step + 1) % args.gradient_accumulation_steps==0 or step==len(train_dataloader) - 1:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                scheduler.step()\n",
    "                global_step += 1\n",
    "                \n",
    "            tk0.set_postfix(epoch_loss=np.mean(train_loss))\n",
    "    model_weights = dict(model.state_dict())\n",
    "#         model_weights.pop('cls.seq_relationship.weight')\n",
    "#         model_weights.pop('cls.seq_relationship.bias')\n",
    "    model_weights = collections.OrderedDict(model_weights)\n",
    "    #collections.OrderedDict\n",
    "    torch.save(model_weights, f'./outputs/pytorch_model_{epoch}.bin')\n",
    "#         gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e06f81d-8c12-46e0-a239-fc139c9d0588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class InputFeatures(object):\n",
    "#     \"\"\"A single set of features of data.\"\"\"\n",
    "#     def __init__(self, input_ids, input_mask, segment_ids, label_id):\n",
    "#         self.input_ids = input_ids\n",
    "#         self.input_mask = input_mask\n",
    "#         self.segment_ids = segment_ids\n",
    "#         self.label_id = label_id\n",
    "        \n",
    "# InputFeatures(input_ids=1,input_mask=2,segment_ids=3, label_id=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf82a6b-56d1-4f36-86db-f81cbdfbedc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
