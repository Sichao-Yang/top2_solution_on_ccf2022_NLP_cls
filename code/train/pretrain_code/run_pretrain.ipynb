{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e455d25a-39ad-4904-8a9c-d8b82486bfb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-01 00:47:52.843454: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2022-10-01 00:47:52.843493: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集样本数量： 10000\n"
     ]
    }
   ],
   "source": [
    "# coding:utf-8\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "random.seed(0)\n",
    "np.random.seed(0)#seed应该在main里尽早设置，以防万一\n",
    "os.environ['PYTHONHASHSEED'] =str(0)#消除hash算法的随机性\n",
    "import transformers as _\n",
    "from transformers1 import Trainer, TrainingArguments,BertTokenizer\n",
    "from NLP_Utils import MLM_Data,train_data,blockShuffleDataLoader\n",
    "\n",
    "from NEZHA.configuration_nezha import NeZhaConfig\n",
    "from NEZHA.modeling_nezha import NeZhaForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dd62678-8a79-46e7-96e3-65c3c6e254d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type nezha to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at /root/autodl-tmp/CCF-小样本/Nezha_pytorch/nezha_model/ were not used when initializing NeZhaForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing NeZhaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NeZhaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "maxlen=510\n",
    "batch_size=16\n",
    "file_dir = '/root/autodl-tmp/CCF-小样本/Nezha_pytorch/nezha_model/'\n",
    "tokenizer = BertTokenizer.from_pretrained(file_dir)\n",
    "config = NeZhaConfig.from_pretrained(file_dir)\n",
    "# config = NeZhaConfig(\n",
    "#     vocab_size=len(tokenizer),\n",
    "#     hidden_size=768,\n",
    "#     num_hidden_layers=12,\n",
    "#     num_attention_heads=12,\n",
    "#     max_position_embeddings=512,\n",
    "# )\n",
    "\n",
    "# nezha-cn-base初始权重需要从Github下载https://github.com/lonePatient/NeZha_Chinese_PyTorch\n",
    "model = NeZhaForMaskedLM.from_pretrained(file_dir)\n",
    "\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "# # 只训练word_embedding。能缩短两倍的训练时间\n",
    "# for name, p in model.named_parameters():\n",
    "#     if name != 'bert.embeddings.word_embeddings.weight':\n",
    "#         p.requires_grad = False\n",
    "# print(model)\n",
    "# print('train_data:', train_data)\n",
    "\n",
    "train_MLM_data=MLM_Data(train_data, maxlen, tokenizer)\n",
    "\n",
    "#自己定义dataloader，不要用huggingface的\n",
    "dl=blockShuffleDataLoader(train_MLM_data,None,key=lambda x:len(x[0])+1,shuffle=False\n",
    "                          ,batch_size=batch_size,collate_fn=train_MLM_data.collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4b77f25-399c-442d-9d31-de7c6169349f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(625, 10000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dl), len(train_MLM_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a3c685c-25b9-48bf-8d27-7d14a3b7db72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 3315, 4509,  ...,    0,    0,    0],\n",
      "        [ 101, 3315, 1355,  ...,    0,    0,    0],\n",
      "        [ 101,  103, 1355,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 3315, 2916,  ...,    0,    0,    0],\n",
      "        [ 101,  103, 2141,  ...,    0,    0,    0],\n",
      "        [ 101, 3315,  103,  ...,    0,    0,    0]], device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0'), 'attention_mask': tensor([[ True,  True,  True,  ..., False, False, False],\n",
      "        [ True,  True,  True,  ..., False, False, False],\n",
      "        [ True,  True,  True,  ..., False, False, False],\n",
      "        ...,\n",
      "        [ True,  True,  True,  ..., False, False, False],\n",
      "        [ True,  True,  True,  ..., False, False, False],\n",
      "        [ True,  True,  True,  ..., False, False, False]], device='cuda:0'), 'labels': tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, 3315, -100,  ..., -100, -100, -100],\n",
      "        ...,\n",
      "        [-100, -100, 1355,  ..., -100, -100, -100],\n",
      "        [-100, 3315, -100,  ..., -100, -100, -100],\n",
      "        [-100, 3315, 2141,  ..., -100, -100, -100]], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "for i in dl:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffd3cdb-f6b6-45c0-9017-6678f8dc8a2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
