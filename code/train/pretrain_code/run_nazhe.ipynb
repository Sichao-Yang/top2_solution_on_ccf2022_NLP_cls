{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bef1b8b2-39fa-48eb-b920-5206c843a345",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-01 00:24:17.510312: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-01 00:24:17.749634: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-10-01 00:24:17.803120: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-01 00:24:18.822373: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2022-10-01 00:24:18.822491: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2022-10-01 00:24:18.822503: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.836 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import pretraining_args as args\n",
    "import time\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "random.seed(args.seed)\n",
    "import sys\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import gc\n",
    "import jieba\n",
    "import collections\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset)\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm, trange\n",
    "import random\n",
    "from random import randrange, randint, shuffle, choice, sample\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.metrics import matthews_corrcoef, f1_score\n",
    "\n",
    "from NEZHA.configuration_nezha import NeZhaConfig\n",
    "from NEZHA.modeling_nezha import NeZhaForMaskedLM\n",
    "from transformers import BertTokenizer\n",
    "from transformers.optimization import AdamW\n",
    "import joblib\n",
    "\n",
    "\n",
    "jieba.enable_parallel(10)\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "logger = logging.getLogger(__name__)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b61eb2e9-ff7b-440e-9181-a0dd9f048941",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = '/root/autodl-tmp/CCF-小样本/Nezha_pytorch/nezha_model/'\n",
    "tokenizer = BertTokenizer.from_pretrained(file_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53fbc252-e5cd-41f3-a084-cc3ff43b5ebb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Converts a token string (or a sequence of tokens) in a single integer id (or a sequence of ids), using the\n",
       "vocabulary.\n",
       "\n",
       "Args:\n",
       "    tokens (:obj:`str` or :obj:`List[str]`): One or several token(s) to convert to token id(s).\n",
       "\n",
       "Returns:\n",
       "    :obj:`int` or :obj:`List[int]`: The token id or list of token ids.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/miniconda3/lib/python3.8/site-packages/transformers/tokenization_utils.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77fe95a1-e489-40cf-a483-dbf315beba35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 1506, 1506, 1506, 4263, 1962, 102]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids('哈哈哈爱好')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fa6154b-67ad-4b69-9413-b7e5dca135c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 666\n",
    "def seed_all(seed_value):\n",
    "    random.seed(seed_value) # Python\n",
    "    np.random.seed(seed_value) # cpu vars\n",
    "    torch.manual_seed(seed_value) # cpu  vars\n",
    "    \n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value) # gpu vars\n",
    "        torch.backends.cudnn.deterministic = True  #needed\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "seed_all(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb91bdcb-cc16-46bb-8bac-513ab2842a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_model = 'train_bert'\n",
    "logger = logging.getLogger('Bert_train')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "timestamp = time.strftime(\"%Y.%m.%d_%H.%M.%S\", time.localtime())\n",
    "fh = logging.FileHandler('log_{}.txt'.format(search_model))\n",
    "fh.setLevel(logging.DEBUG)\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('[%(asctime)s][%(levelname)s] ## %(message)s')\n",
    "fh.setFormatter(formatter)\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(fh)\n",
    "logger.addHandler(ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87c146eb-64cb-498e-a473-db3c30aedc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_signs(tokens):\n",
    "    signs = []\n",
    "    line = ''\n",
    "    for token in tokens:\n",
    "        if len(token) > 1:\n",
    "            line += ' '\n",
    "        else:\n",
    "            line += token\n",
    "    words = jieba.lcut(line)\n",
    "    # 带##的会被当做单独词\n",
    "    sign = 0\n",
    "    # signs中 0 1连续交替，代表词的区分\n",
    "    for word in words:\n",
    "        for i in word:\n",
    "            signs.append(sign)\n",
    "        sign = 1 if sign == 0 else 0\n",
    "    assert len(tokens) == len(signs)\n",
    "    return signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "308fb332-cffa-43dc-86d6-2c338664faf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./ori_model/'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'{args.pretrained_path}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c466713-64c8-4c0d-b100-cefdbbe1f899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./abstracts_new.txt\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "args.train_batch_size = args.train_batch_size // args.gradient_accumulation_steps\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(f'{args.pretrained_path}')\n",
    "train_examples = None\n",
    "num_train_optimization_steps = None\n",
    "vocab_words = []\n",
    "with open(args.vocab_file, 'r') as fr:\n",
    "     for line in fr:\n",
    "        vocab_words.append(line.strip(\"\\n\"))\n",
    "print(args.pretrain_train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30a3d83d-075f-4ce0-af62-c47122556a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warmup_linear(x, warmup=0.002):\n",
    "    if x < warmup:\n",
    "        return x/warmup\n",
    "    return 1.0 - x\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "\n",
    "\n",
    "\n",
    "def create_masked_lm_predictions(tokens, masked_lm_prob, max_predictions_per_seq, vocab_words, word_ids):\n",
    "    \n",
    "    cand_indexes = []\n",
    "    word_indexes = []\n",
    "    last_word_sign = 0\n",
    "    for (i, token) in enumerate(tokens):\n",
    "        # 特殊符号为 -1\n",
    "        if token == \"[CLS]\" or token == \"[SEP]\" or word_ids[i] == -100:\n",
    "            continue\n",
    "        # (len(cand_indexes) >= 1 )\n",
    "        # 之前的处理中，会把##的去掉\n",
    "        # word_ids中连续0或者1为同一个词\n",
    "        if word_ids[i] == last_word_sign:\n",
    "            word_indexes.append(i)\n",
    "        elif token.startswith(\"##\"):\n",
    "            word_indexes.append(i)\n",
    "            last_word_sign = word_ids[i]\n",
    "        else:\n",
    "            # token不带## 且对应wordid不等则为另外一个词\n",
    "            cand_indexes.append(word_indexes)\n",
    "            word_indexes = []\n",
    "            word_indexes.append(i)\n",
    "            last_word_sign = word_ids[i]\n",
    "\n",
    "    random.shuffle(cand_indexes)\n",
    "    output_tokens = list(tokens)\n",
    "    masked_lm = collections.namedtuple(\"masked_lm\", [\"index\", \"label\"])  \n",
    "    num_to_predict = min(max_predictions_per_seq,\n",
    "                       max(1, int(round(len(tokens) * masked_lm_prob))))\n",
    "    \n",
    "    masked_lms = []\n",
    "    covered_indexes = set()\n",
    "    for word_indexes in cand_indexes:\n",
    "        if str(word_indexes) in covered_indexes:\n",
    "              continue\n",
    "        covered_indexes.add(str(word_indexes))\n",
    "\n",
    "        random1 = random.random()\n",
    "        random2 = random.random()\n",
    "        for index in word_indexes:\n",
    "            if len(masked_lms) >= num_to_predict:\n",
    "                break\n",
    "            masked_token = None\n",
    "            # 80% of the time, replace with [MASK]\n",
    "            if random1 < 0.8:\n",
    "                 masked_token = \"[MASK]\"\n",
    "            else:\n",
    "        # 10% of the time, keep original\n",
    "               if random2 < 0.5:\n",
    "                   masked_token = tokens[index]\n",
    "        # 10% of the time, replace with random word\n",
    "               else:\n",
    "                  masked_token = vocab_words[random.randint(0, len(vocab_words) - 1)]\n",
    "\n",
    "            output_tokens[index] = masked_token\n",
    "            masked_lms.append(masked_lm(index=index, label=tokens[index]))\n",
    "            \n",
    "    masked_lms = sorted(masked_lms, key=lambda x: x.index)\n",
    "\n",
    "    masked_lm_positions = []\n",
    "    masked_lm_labels = []\n",
    "    for p in masked_lms:\n",
    "        masked_lm_positions.append(p.index)\n",
    "        masked_lm_labels.append(p.label)\n",
    "    \n",
    "    \n",
    "    return output_tokens, masked_lm_positions, masked_lm_labels\n",
    "\n",
    "\n",
    "def create_examples(data_path, tokenizer, max_seq_length, masked_lm_prob, max_predictions_per_seq, vocab_words):\n",
    "    \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "#     vocab_check = {w:0 for w in vocab_words}\n",
    "    examples = []\n",
    "    max_num_tokens = max_seq_length - 2\n",
    "    fr = open(data_path, \"r\")\n",
    "    for (i, line) in tqdm(enumerate(fr), desc=\"Creating Example\"):\n",
    "\n",
    "        words_ids = []\n",
    "        line = line.strip()\n",
    "        \n",
    "        tokens_a = tokenizer.tokenize(line)\n",
    "        tokens_a = tokens_a[: max_num_tokens]\n",
    "        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n",
    "        segment_ids = [0 for _ in range(len(tokens_a) + 2)]\n",
    "        words_ids.append(-100)\n",
    "        signs_a = get_word_signs(tokens_a)\n",
    "        for sign in signs_a:\n",
    "            words_ids.append(sign)\n",
    "        words_ids.append(-100)\n",
    "        tokens, masked_lm_positions, masked_lm_labels=create_masked_lm_predictions(\n",
    "            tokens, masked_lm_prob, max_predictions_per_seq, vocab_words, words_ids)  \n",
    "        \n",
    "        example = {\n",
    "            \"tokens\": tokens,\n",
    "            \"segment_ids\": segment_ids,\n",
    "            \"masked_lm_positions\": masked_lm_positions,\n",
    "            \"masked_lm_labels\": masked_lm_labels}\n",
    "        examples.append(example)\n",
    "    fr.close()\n",
    "    return examples\n",
    "\n",
    "def convert_examples_to_features(examples, max_seq_length, tokenizer):\n",
    "    features = []\n",
    "    for i, example in enumerate(examples):\n",
    "        if i % 300000 == 0:\n",
    "            print(f'{i} have finished!')\n",
    "        tokens = example[\"tokens\"]\n",
    "        segment_ids = example[\"segment_ids\"]\n",
    "        masked_lm_positions = example[\"masked_lm_positions\"]\n",
    "        masked_lm_labels = example[\"masked_lm_labels\"]\n",
    "#         print(len(tokens), len(segment_ids), max_seq_length)\n",
    "        assert len(tokens) == len(segment_ids) <= max_seq_length  # The preprocessed data should be already truncated\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        masked_label_ids = tokenizer.convert_tokens_to_ids(masked_lm_labels)\n",
    "\n",
    "        input_array = np.zeros(max_seq_length, dtype=np.int)\n",
    "        input_array[:len(input_ids)] = input_ids\n",
    "\n",
    "        mask_array = np.zeros(max_seq_length, dtype=np.bool)\n",
    "        mask_array[:len(input_ids)] = 1\n",
    "\n",
    "        segment_array = np.zeros(max_seq_length, dtype=np.bool)\n",
    "        segment_array[:len(segment_ids)] = segment_ids\n",
    "\n",
    "        lm_label_array = np.full(max_seq_length, dtype=np.int, fill_value=-100)\n",
    "        lm_label_array[masked_lm_positions] = masked_label_ids\n",
    "        \n",
    "        feature = InputFeatures(input_ids=input_array,\n",
    "                         input_mask=mask_array,segment_ids=segment_array, label_id=lm_label_array)\n",
    "        features.append(feature)\n",
    "        # if i < 10:\n",
    "        #     logger.info(\"input_ids: %s\\ninput_mask:%s\\nsegment_ids:%s\\nlabel_id:%s\" %(input_array, mask_array, segment_array, lm_label_array))\n",
    "    return features\n",
    "        \n",
    "# train_features = convert_examples_to_features(train_examples, args.max_seq_length, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98745854-df77-4764-a49a-95ff0fb03d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating Example: 30656it [01:52, 271.67it/s]\n"
     ]
    }
   ],
   "source": [
    "if args.do_train:\n",
    "    train_examples = create_examples(data_path=args.pretrain_train_path,\n",
    "                                     tokenizer=tokenizer,\n",
    "                                     max_seq_length=args.max_seq_length,\n",
    "                                     masked_lm_prob=args.masked_lm_prob,\n",
    "                                     max_predictions_per_seq=args.max_predictions_per_seq,\n",
    "                                     vocab_words=vocab_words)\n",
    "#     with open('./train_examples.pk', 'rb')as f:\n",
    "#         train_examples = joblib.load(f)\n",
    "\n",
    "\n",
    "    num_train_optimization_steps = int(\n",
    "        math.ceil(len(train_examples) / args.train_batch_size) / args.gradient_accumulation_steps) * args.num_train_epochs\n",
    "    num_train_optimization_steps = num_train_optimization_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afe5aab8-a462-4347-8006-be84c825a8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/autodl-tmp/CCF-小样本/Nezha_pytorch/pretrain\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "444144af-468f-4fda-8d83-f114cd661342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import joblib\n",
    "with open('./train_examples_nezha.pk', 'wb')as f:\n",
    "    joblib.dump(train_examples, f)\n",
    "\n",
    "# with open('./train_examples.pk', 'rb')as f:\n",
    "#     train_examples = joblib.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f075e92c-e653-49ab-9bec-7a98fbc77df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trained_dict = torch.load(f'{args.pretrained_path}pytorch_model.bin')\n",
    "# model = BertForPreTraining(config=BertConfig.from_json_file(args.bert_config_json))\n",
    "# BertForMaskedLM\n",
    "model = NeZhaForMaskedLM(config=NeZhaConfig.from_json_file(args.bert_config_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f423e6a-2882-457f-b26b-f8b7dde4e0dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_trained_dict = collections.OrderedDict(pre_trained_dict)\n",
    "# pre_trained_dict['cls.seq_relationship.weight']\n",
    "pre_trained_dict.pop('cls.seq_relationship.weight')\n",
    "pre_trained_dict.pop('cls.seq_relationship.bias')\n",
    "n=0\n",
    "model_init = dict(model.state_dict())\n",
    "for k in pre_trained_dict:\n",
    "    if k in model_init:\n",
    "        n+=1\n",
    "        model_init[k] = pre_trained_dict[k]\n",
    "model.load_state_dict(model_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69bc0152-0cc0-4000-8b6b-2d7e0cdb1f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n",
    "warmup_step = int(args.warmup_proportion * num_train_optimization_steps)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=warmup_step, \n",
    "    num_training_steps=int(num_train_optimization_steps)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fdd5a6-eff8-40f5-8ca1-3f8c2ef36837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 have finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-28 00:04:51,362][INFO] ## training.........................\n",
      "100%|██████████| 3832/3832 [10:11<00:00,  6.26it/s, epoch_loss=5.75]\n",
      "100%|██████████| 3832/3832 [10:11<00:00,  6.27it/s, epoch_loss=2.8] \n",
      "100%|██████████| 3832/3832 [10:10<00:00,  6.28it/s, epoch_loss=1.92]\n",
      "100%|██████████| 3832/3832 [10:11<00:00,  6.27it/s, epoch_loss=1.7] \n",
      "100%|██████████| 3832/3832 [10:12<00:00,  6.26it/s, epoch_loss=1.54]\n",
      "100%|██████████| 3832/3832 [10:15<00:00,  6.22it/s, epoch_loss=1.37]\n",
      "100%|██████████| 3832/3832 [10:13<00:00,  6.24it/s, epoch_loss=1.2] \n",
      "100%|██████████| 3832/3832 [10:16<00:00,  6.22it/s, epoch_loss=1.04]\n",
      "100%|██████████| 3832/3832 [10:14<00:00,  6.24it/s, epoch_loss=0.908]\n",
      "100%|██████████| 3832/3832 [10:16<00:00,  6.22it/s, epoch_loss=0.784]\n",
      "100%|██████████| 3832/3832 [10:16<00:00,  6.22it/s, epoch_loss=0.676]\n",
      "100%|██████████| 3832/3832 [10:14<00:00,  6.23it/s, epoch_loss=0.579]\n",
      "100%|██████████| 3832/3832 [10:15<00:00,  6.23it/s, epoch_loss=0.493]\n",
      "100%|██████████| 3832/3832 [10:16<00:00,  6.21it/s, epoch_loss=0.419]\n",
      "100%|██████████| 3832/3832 [10:15<00:00,  6.23it/s, epoch_loss=0.355]\n",
      "Creating Example: 30656it [01:54, 266.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 have finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3832/3832 [10:14<00:00,  6.24it/s, epoch_loss=1.62]\n",
      "100%|██████████| 3832/3832 [10:16<00:00,  6.22it/s, epoch_loss=1.32]\n",
      "100%|██████████| 3832/3832 [10:16<00:00,  6.21it/s, epoch_loss=1.12]\n",
      "100%|██████████| 3832/3832 [10:15<00:00,  6.23it/s, epoch_loss=0.956]\n",
      "100%|██████████| 3832/3832 [10:17<00:00,  6.20it/s, epoch_loss=0.814]\n",
      "100%|██████████| 3832/3832 [10:17<00:00,  6.21it/s, epoch_loss=0.689]\n",
      "100%|██████████| 3832/3832 [10:20<00:00,  6.18it/s, epoch_loss=0.584]\n",
      "100%|██████████| 3832/3832 [10:18<00:00,  6.19it/s, epoch_loss=0.493]\n",
      "100%|██████████| 3832/3832 [10:19<00:00,  6.19it/s, epoch_loss=0.416]\n",
      "100%|██████████| 3832/3832 [10:14<00:00,  6.24it/s, epoch_loss=0.353]\n",
      "100%|██████████| 3832/3832 [10:16<00:00,  6.22it/s, epoch_loss=0.299]\n",
      "100%|██████████| 3832/3832 [10:15<00:00,  6.22it/s, epoch_loss=0.254]\n",
      "100%|██████████| 3832/3832 [10:16<00:00,  6.22it/s, epoch_loss=0.217]\n",
      "100%|██████████| 3832/3832 [10:16<00:00,  6.22it/s, epoch_loss=0.186]\n",
      "100%|██████████| 3832/3832 [10:16<00:00,  6.22it/s, epoch_loss=0.161]\n",
      "Creating Example: 30656it [01:54, 267.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 have finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3832/3832 [10:16<00:00,  6.22it/s, epoch_loss=1.52]\n",
      "100%|██████████| 3832/3832 [10:16<00:00,  6.22it/s, epoch_loss=1.26]\n",
      "100%|██████████| 3832/3832 [10:17<00:00,  6.20it/s, epoch_loss=1.1] \n",
      "100%|██████████| 3832/3832 [10:17<00:00,  6.20it/s, epoch_loss=0.97] \n",
      "100%|██████████| 3832/3832 [10:16<00:00,  6.22it/s, epoch_loss=0.858]\n",
      "100%|██████████| 3832/3832 [10:17<00:00,  6.21it/s, epoch_loss=0.759]\n",
      "100%|██████████| 3832/3832 [10:16<00:00,  6.21it/s, epoch_loss=0.677]\n",
      "100%|██████████| 3832/3832 [10:16<00:00,  6.21it/s, epoch_loss=0.605]\n",
      "100%|██████████| 3832/3832 [10:16<00:00,  6.22it/s, epoch_loss=0.542]\n",
      "100%|██████████| 3832/3832 [10:15<00:00,  6.23it/s, epoch_loss=0.49] \n",
      "100%|██████████| 3832/3832 [10:16<00:00,  6.21it/s, epoch_loss=0.444]\n",
      "100%|██████████| 3832/3832 [10:15<00:00,  6.23it/s, epoch_loss=0.404]\n",
      "100%|██████████| 3832/3832 [10:17<00:00,  6.21it/s, epoch_loss=0.372]\n",
      "100%|██████████| 3832/3832 [10:16<00:00,  6.22it/s, epoch_loss=0.343]\n",
      " 47%|████▋     | 1814/3832 [04:51<05:20,  6.30it/s, epoch_loss=0.31]]"
     ]
    }
   ],
   "source": [
    "# init epoch1:\n",
    "train_features = convert_examples_to_features(train_examples, args.max_seq_length, tokenizer)\n",
    "all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
    "all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\n",
    "\n",
    "global_step = 0\n",
    "best_loss = 100000\n",
    "tr_loss = 0\n",
    "if args.do_train:\n",
    "    \n",
    "    logger.info('training.........................')\n",
    "    model.train()\n",
    "    #int(args.num_train_epochs)\n",
    "    for epoch in range(int(args.num_train_epochs)):\n",
    "        if epoch % 5 == 0 and epoch != 0:\n",
    "            model_weights = dict(model.state_dict())\n",
    "            model_weights = collections.OrderedDict(model_weights)\n",
    "            torch.save(model_weights, f'./nezha_outputs/pytorch_model_{epoch}_40epochs.bin')            \n",
    "            \n",
    "            \n",
    "        if epoch % 15 == 0 and epoch != 0:\n",
    "            train_examples = create_examples(data_path=args.pretrain_train_path,\n",
    "                                 tokenizer=tokenizer,\n",
    "                                 max_seq_length=args.max_seq_length,\n",
    "                                 masked_lm_prob=args.masked_lm_prob,\n",
    "                                 max_predictions_per_seq=args.max_predictions_per_seq,\n",
    "                                 vocab_words=vocab_words)\n",
    "            \n",
    "        \n",
    "            train_features = convert_examples_to_features(train_examples, args.max_seq_length, tokenizer)\n",
    "            all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
    "            all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
    "            all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
    "            all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\n",
    "\n",
    "        train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "#         train_sampler = RandomSampler(train_data)\n",
    "#         train_dataloader = DataLoader(train_data, sampler=train_sampler,num_workers=4, batch_size=args.train_batch_size)\n",
    "#         train_sampler = torch.utils.data.distributed.DistributedSampler(train_data, num_replicas=2)\n",
    "        train_dataloader = torch.utils.data.DataLoader(train_data,\n",
    "                                         num_workers=4,\n",
    "                                         batch_size=args.train_batch_size)\n",
    "        \n",
    "        train_loss = []\n",
    "        nb_tr_examples, nb_tr_steps = 0, 0\n",
    "        tk0 = tqdm(train_dataloader, total=len(train_dataloader))\n",
    "        for step, batch in enumerate(tk0):\n",
    "#             if nb_tr_steps > 0 and nb_tr_steps % 1000 == 0:\n",
    "#                 logger.info(\"=====-epoch %d -train_step %d -train_loss %.4f\\n\" % (epoch,\n",
    "#                                                                                   nb_tr_steps,\n",
    "#                                                                                   np.mean(train_loss)))\n",
    "                \n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids, input_mask, segment_ids, label_ids = batch\n",
    "            # masked_lm_loss\n",
    "            loss = model(input_ids=input_ids, \n",
    "                         token_type_ids=segment_ids, \n",
    "                         attention_mask=input_mask, \n",
    "                         labels=label_ids)[0]\n",
    "            loss = loss.mean()\n",
    "            if args.gradient_accumulation_steps > 1:\n",
    "                loss = loss / args.gradient_accumulation_steps\n",
    "            loss.backward()\n",
    "\n",
    "            train_loss.append(loss.item()*args.gradient_accumulation_steps)\n",
    "            nb_tr_examples += input_ids.size(0)\n",
    "            nb_tr_steps += 1\n",
    "            if (step + 1) % args.gradient_accumulation_steps==0 or step==len(train_dataloader) - 1:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                scheduler.step()\n",
    "                global_step += 1\n",
    "                \n",
    "            tk0.set_postfix(epoch_loss=np.mean(train_loss))\n",
    "    model_weights = dict(model.state_dict())\n",
    "#         model_weights.pop('cls.seq_relationship.weight')\n",
    "#         model_weights.pop('cls.seq_relationship.bias')\n",
    "    model_weights = collections.OrderedDict(model_weights)\n",
    "    #collections.OrderedDict\n",
    "    torch.save(model_weights, f'./nezha_outputs/pytorch_model_{epoch}_40epochs.bin')\n",
    "#         gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac9b193-a359-4771-b8e3-75b36ee7cf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e06f81d-8c12-46e0-a239-fc139c9d0588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class InputFeatures(object):\n",
    "#     \"\"\"A single set of features of data.\"\"\"\n",
    "#     def __init__(self, input_ids, input_mask, segment_ids, label_id):\n",
    "#         self.input_ids = input_ids\n",
    "#         self.input_mask = input_mask\n",
    "#         self.segment_ids = segment_ids\n",
    "#         self.label_id = label_id\n",
    "        \n",
    "# InputFeatures(input_ids=1,input_mask=2,segment_ids=3, label_id=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf82a6b-56d1-4f36-86db-f81cbdfbedc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
